{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1_2\n",
    "## Dataset: Gym\n",
    "\n",
    "## Programmer: Giovanni Vecchione\n",
    "## Date: 4/17/24\n",
    "## Subject: Machine Learning 2 - Project 6\n",
    "Use Reinforced Learning (RL) to build the project. Submit your project as Jupyter notebook.\n",
    "\n",
    "NOTE: Using Q-Learning to the CartPole Enviornment / Not complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mtp\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "#Checks if GPU is being used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0)) \n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "#Using GPU: NVIDIA GeForce GTX 1660 SUPER - Successful\n",
    "#NOTE: This took some time to set up by installing and pathing the cuda toolkit v.12.4 and the right supplemental packages. This drastically improved\n",
    "#training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the environment to simulate the world needed. This includes all the necessary physics, rules, goals, etc.\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The env variable now acts as the way you'll interact with the environment. You'll use functions provided by it to:\n",
    "env.reset(): Get the initial state of the environment.\n",
    "env.step(action): Take an action and see the resulting new state, reward, and whether the episode is finished.\n",
    "env.render(): (Optional) Visualize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)  # What the agent observes\n",
    "print(env.action_space)       # Actions the agent can take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Q-Learning\n",
    "\n",
    "### *Key Points*\n",
    "* Q-value Represents the expected future reward the agent can get by taking a particular action in a given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Setting up a Q-table*\n",
    "In CartPole, the agent's observations (position, velocity, angle, etc.) are continuous values. To use Q-learning effectively, we'll need to discretize these observations into a finite set of states. Use techniques like binning or clustering to do this.\n",
    "\n",
    "1. Understanding Discrete States\n",
    "\n",
    "    * In CartPole, the agent's observations (position, velocity, angle, etc.) are continuous values. To use Q-learning effectively, we'll need to discretize these observations into a finite set of states. You might use techniques like binning or clustering to do this.\n",
    "\n",
    "2. Initializing the Q-table\n",
    "    * We'll use a simple data structure like a NumPy array or a Python dictionary to represent our Q-table\n",
    "\n",
    "3. Interpretation\n",
    "\n",
    "    * Rows: Each row in the Q-table corresponds to a specific discrete state of the environment.\n",
    "    * Columns: Each column represents a possible action the agent can take (in CartPole, move left or move right).\n",
    "    * Values: The value inside a cell Q_table[state, action] represents the estimated future reward the agent expects to get if it takes the action in the state and follows its optimal policy from there onwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Q-table (Starting with 50 as size)\n",
    "num_states = 50\n",
    "num_actions = env.action_space.n\n",
    "Q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "#Creates a Q-table with the number of rows equal to the number of discretized states and \n",
    "#the number of columns equal to the number of actions. We initialize all the Q-values to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## *Agent Initialization and The Learning Loop*\n",
    "Create an agent that will interact with the environment to learn the optimal balancing policy.\n",
    "\n",
    "### *The Agent's Components:*\n",
    "\n",
    "* The Q-table:  This data structure embodies the core knowledge of the agent. It stores the agent's understanding of the best actions to take in different states.\n",
    "\n",
    "* Policy: As the Q-table is updated, the agent implicitly develops a policy. This policy is essentially the way it decides which action to take based on its current state. In Q-learning, the policy is often an epsilon-greedy policy, striking a balance between exploring new actions and exploiting its current knowledge.\n",
    "\n",
    "* Learning Algorithm: The Q-learning update rule itself is the core mechanism by which the agent learns. This algorithm updates the Q-table based on the agent's experiences, progressively improving the agent's knowledge of good and bad actions.\n",
    "\n",
    "### So, where's the agent?\n",
    "\n",
    "The agent in Q-learning is an emergent entity. It's not explicitly programmed, but rather emerges from the combination of the Q-table, its implicit policy, and the learning algorithm.\n",
    "\n",
    "* Discount factor (gamma): Determines how much we value future rewards compared to immediate rewards.\n",
    "* Learning rate (alpha): Controls how much we update our Q-values with each new experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 1.0  # Starting with high exploration (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      2\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mstep(action))\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info = env.reset()\n",
    "action = env.action_space.sample()\n",
    "print(env.step(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q_table[state, :])  \u001b[38;5;66;03m# Exploit learned values\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m new_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action) \u001b[38;5;66;03m# Include 'info' here \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Update Q-table\u001b[39;00m\n\u001b[0;32m     16\u001b[0m old_value \u001b[38;5;241m=\u001b[39m Q_table[state, action]\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < epsilon: \n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state, :])  # Exploit learned values\n",
    "\n",
    "        new_state, reward, done, info = env.step(action) # I believe this needs to be changed - Needs a system for rewards etc.\n",
    "\n",
    "        # Update Q-table\n",
    "        old_value = Q_table[state, action]\n",
    "        next_max = np.max(Q_table[new_state, :])  \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        Q_table[state, action] = new_value\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # Reduce exploration rate over time\n",
    "    if epsilon > 0.01:\n",
    "        epsilon *= 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m----> 7\u001b[0m   action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mQ_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m) \n\u001b[0;32m      8\u001b[0m   state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      9\u001b[0m   env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Function sees how it performs after training\n",
    "for _ in range(5):\n",
    "  state, info = env.reset()\n",
    "  done = False\n",
    "  steps = 0\n",
    "  while not done:\n",
    "    action = np.argmax(Q_table[state, :]) \n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    steps += 1\n",
    "  print(\"Episode lasted\", steps, \"steps\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

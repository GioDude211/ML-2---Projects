{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1_1\n",
    "## Dataset: Gym\n",
    "\n",
    "## Programmer: Giovanni Vecchione\n",
    "## Date: 4/17/24\n",
    "## Subject: Machine Learning 2 - Project 6\n",
    "Use Reinforced Learning (RL) to build the project. Submit your project as Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mtp\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "#Checks if GPU is being used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0)) \n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "#Using GPU: NVIDIA GeForce GTX 1660 SUPER - Successful\n",
    "#NOTE: This took some time to set up by installing and pathing the cuda toolkit v.12.4 and the right supplemental packages. This drastically improved\n",
    "#training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "#render_mode=\"rgb_array\" dosn't work for some reason when using the render call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)    # See what kind of data the environment provides\n",
    "print(env.action_space)         # See the agent's possible actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render()\n",
    "img.shape #only works when using hte rgb call from earlier, however it does not seem to function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1  # accelerate right\n",
    "obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The step() method executes the desired action and returns five values:\n",
    "\n",
    "### *1. obs*\n",
    "    This is the new observation. The cart is now moving toward the right (obs[1] > 0). The pole is still tilted toward the right (obs[2] > 0), but its angular velocity is now negative (obs[3] < 0), so it will likely be tilted toward the left after the next step.\n",
    "\n",
    "### *2. reward*\n",
    "    In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep the episode running for as long as possible.\n",
    "\n",
    "### *3. done*\n",
    "    This value will be True when the episode is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, you have won). After that, the environment must be reset before it can be used again.\n",
    "\n",
    "### *4. truncated*\n",
    "    This value will be True when an episode is interrupted early, for example by an environment wrapper that imposes a maximum number of steps per episode (see Gym's documentation for more details on environment wrappers). Some RL algorithms treat truncated episodes differently from episodes finished normally (i.e., when done is True), but in this chapter we will treat them identically.\n",
    "\n",
    "### *5. info*\n",
    "    This environment-specific dictionary may provide extra information, just like the one returned by the reset() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 50\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Policies\n",
    "\n",
    "Basically using a neural net instead of a basic policy function\n",
    "\n",
    "*Typical Work Flow:* \n",
    "\n",
    "Define Neural Network (model1_1) and loss function (loss_fn)\n",
    "\n",
    "For each iteration:\n",
    "\n",
    "1. Run multiple episodes using play_multiple_episodes\n",
    "2. Discount and Normalize the rewards using the helper functions\n",
    "3. Update the model1_1 parameters based on the collected gradients (this would need an optimizer and a gradient application step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the code to build a basic neural network policy using Keras:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model1_1 = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu'), \n",
    "    keras.layers.Dense(32, activation='relu'),  # Another hidden layer\n",
    "    keras.layers.Dense(1, activation='sigmoid')   # Output layer: probabilities for 2 actions\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients:\n",
    "Neural Nets cannot train on their own and must have a policy to follow. In this case we're using a Policy Gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core Action: This function takes a single step within the environment.\n",
    "\n",
    "Neural Network Interaction: It receives the current observation (obs), \n",
    "passes it through the model (model1_1) to get the probability of moving left (left_proba).\n",
    "\n",
    "Action Selection: An action is sampled based on that probability.\n",
    "\n",
    "Environment Update: The action is executed, and the function gets the next observation, reward, and 'done' flag from the environment.\n",
    "\n",
    "Loss Calculation: It prepares data for calculating the policy gradient loss, using a loss_fn (due to cross entropy stated earlier).\n",
    "\n",
    "Gradient Calculation: Uses a tf.GradientTape to record the operations, enabling the calculation of the policy gradient.\n",
    "\n",
    "NOTE: Returning actions to track\n",
    "\"\"\"\n",
    "\n",
    "def play_one_step(env, obs, model1_1, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model1_1(obs[np.newaxis])\n",
    "        # Calculate action probabilities from your neural network\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        # Calculate a loss based on the returns and action probabilities\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "\n",
    "# Calculate gradients and update the model\n",
    "    grads = tape.gradient(loss, model1_1.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Episode Loop: This function is responsible for running n_episodes.\n",
    "\n",
    "Data Collection: It collects the rewards (all_rewards) and gradients (all_grads) produced by play_one_step during each episode.\n",
    "*New* imporvements: added an observation collection also.\n",
    "    Added an action collection also.\n",
    "\"\"\"\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model1_1, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    all_observations = []  # Added for storing observations\n",
    "    all_actions = [] #Added for storing actions\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        current_observations = []  # Store observations for each episode\n",
    "        current_actions = []  # Actions for each episode\n",
    "        obs, info = env.reset()\n",
    "        current_observations.append(obs)  # Store the initial observation\n",
    "\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads, action = play_one_step(\n",
    "                env, obs, model1_1, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            current_actions.append(action)  # Store the action \n",
    "            current_observations.append(obs) # Store each subsequent observation \n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "        all_observations.append(current_observations) \n",
    "        all_actions.append(current_actions)\n",
    "\n",
    "    return all_rewards, all_grads, all_observations, all_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Discounted Returns: This straightforward function takes a list of rewards from a single episode and calculates the \n",
    "discounted cumulative rewards, with future rewards being weighted less by the discount_factor.\n",
    "\"\"\"\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "\"\"\" \n",
    "Normalization: This function applies the discount_rewards to each episode's rewards and then normalizes them \n",
    "(subtracting the mean and dividing by the standard deviation). Normalization can often improve stability during learning.\n",
    "\n",
    "Updated this--------------------------- \n",
    "\n",
    "Normalization Scope: Now, we calculate the mean and standard deviation for each individual episode's discounted rewards \n",
    "and normalize accordingly.\n",
    "\n",
    "Preserving Episode Structure: The output will maintain the structure of nested lists, where each inner list represents a \n",
    "single episode's normalized rewards.\n",
    "\"\"\"\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = []\n",
    "\n",
    "    #Normalize\n",
    "    for rewards in all_rewards:\n",
    "        discounted_rewards = discount_rewards(rewards, discount_factor)\n",
    "        reward_mean = discounted_rewards.mean()\n",
    "        reward_std = discounted_rewards.std()\n",
    "        normalized_rewards = (discounted_rewards - reward_mean) / reward_std  # Normalize here\n",
    "        all_discounted_rewards.append(normalized_rewards) \n",
    "    return all_discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensor Conversion: Ensure all_final_rewards is a TensorFlow tensor for efficient calculation.\n",
    "\n",
    "GradientTape:  Essential for tracking the operations needed to calculate gradients later.\n",
    "\n",
    "Resimulating Episodes:  Since the action probabilities from the model are needed, we re-run the episodes, \n",
    "this time storing the log probabilities of the actions that were taken .\n",
    "(NOTE: need to replace \"initial_obs\" and \"actions\" with ways to access these from the code above).\n",
    "\n",
    "Log Probabilities:  We calculate the log probabilities of the chosen actions.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\"\"\"\n",
    "def calculate_policy_gradient_loss(model, all_final_rewards, all_grads, all_observations, all_actions):\n",
    "    print(all_final_rewards)  # Inspect the structure thoroughly\n",
    "    print([len(episode_rewards) for episode_rewards in all_final_rewards])  # Check lengths\n",
    "    \n",
    "    all_final_rewards = tf.convert_to_tensor(all_final_rewards, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:  \n",
    "        # Simulate the episodes again to get action probabilities \n",
    "        all_log_probs = [] \n",
    "        for episode_index, final_rewards in enumerate(all_final_rewards):\n",
    "            obs = all_observations[episode_index][0]  # Access initial observation\n",
    "            episode_log_probs = []\n",
    "            \n",
    "            for step, reward in enumerate(final_rewards):\n",
    "                action_probs = model(obs[tf.newaxis])  \n",
    "                selected_action = all_actions[episode_index][step]  # Access stored action\n",
    "                log_prob = tf.math.log(tf.gather_nd(action_probs, tf.stack([tf.range(1), selected_action], axis=1)))\n",
    "                episode_log_probs.append(log_prob)\n",
    "                obs = all_observations[episode_index][step + 1]  # Update obs for the next step\n",
    "                \n",
    "                if done:\n",
    "                    break \n",
    "\n",
    "            all_log_probs.append(episode_log_probs) \n",
    "            \n",
    "        all_log_probs = tf.stack(all_log_probs, axis=0) \n",
    "        \n",
    "        # Calculate the losses, weigh them, and take the mean across episodes  \n",
    "        losses = -all_log_probs * all_final_rewards \n",
    "        loss = tf.reduce_mean(losses)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* Since we are sampling a single action based on the probability, the y_target in the play_one_step function represents which action was actually taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)  #Nadam is an extension of adam and is common when using RL\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.28468712,  1.23873344,  1.19036115,  1.13944295,  1.08584484,\n",
      "        1.02942578,  0.9700373 ,  0.90752311,  0.84171869,  0.77245089,\n",
      "        0.69953741,  0.62278638,  0.54199582,  0.45695313,  0.36743451,\n",
      "        0.27320438,  0.17401476,  0.06960465, -0.04030074, -0.15599062,\n",
      "       -0.27776944, -0.40595767, -0.54089265, -0.68292947, -0.83244192,\n",
      "       -0.98982344, -1.1554882 , -1.32987216, -1.51343422, -1.70665744,\n",
      "       -1.9100503 , -2.12414805]), array([ 1.43765107,  1.28945721,  1.13346366,  0.96925994,  0.79641391,\n",
      "        0.61447072,  0.42295158,  0.22135248,  0.0091429 , -0.21423561,\n",
      "       -0.44937087, -0.69688168, -0.95741937, -1.23166958, -1.520354  ,\n",
      "       -1.82423234]), array([ 1.45052841,  1.27120908,  1.0824519 ,  0.88376012,  0.67461089,\n",
      "        0.45445379,  0.22270949, -0.02123189, -0.27801228, -0.54830743,\n",
      "       -0.83282864, -1.13232466, -1.44758362, -1.77943516]), array([ 1.33595584,  1.27245261,  1.20560709,  1.1352434 ,  1.06117635,\n",
      "        0.98321103,  0.90114228,  0.81475412,  0.72381922,  0.62809826,\n",
      "        0.52733936,  0.42127737,  0.30963316,  0.19211294,  0.06840744,\n",
      "       -0.06180886, -0.19887866, -0.34316266, -0.49504056, -0.65491203,\n",
      "       -0.82319778, -1.00034068, -1.18680689, -1.38308711, -1.58969787,\n",
      "       -1.80718288, -2.03611447]), array([ 1.38572292,  1.29520764,  1.19992839,  1.09963444,  0.99406186,\n",
      "        0.88293283,  0.76595491,  0.64282025,  0.51320483,  0.37676753,\n",
      "        0.23314933,  0.08197227, -0.07716147, -0.24467067, -0.42099615,\n",
      "       -0.60660191, -0.8019764 , -1.00763376, -1.22411519, -1.45199038,\n",
      "       -1.691859  , -1.94435228]), array([ 1.44451807,  1.28185627,  1.11063332,  0.93039864,  0.74067793,\n",
      "        0.54097191,  0.33075506,  0.10947415, -0.12345311, -0.36863971,\n",
      "       -0.62673087, -0.89840577, -1.18437934, -1.48540416, -1.80227239]), array([ 1.43765107,  1.28945721,  1.13346366,  0.96925994,  0.79641391,\n",
      "        0.61447072,  0.42295158,  0.22135248,  0.0091429 , -0.21423561,\n",
      "       -0.44937087, -0.69688168, -0.95741937, -1.23166958, -1.520354  ,\n",
      "       -1.82423234]), array([ 1.34611223,  1.27814188,  1.20659414,  1.13128074,  1.05200347,\n",
      "        0.96855372,  0.88071187,  0.78824677,  0.69091508,  0.58846067,\n",
      "        0.48061393,  0.36709104,  0.24759326,  0.12180612, -0.01060139,\n",
      "       -0.14997772, -0.29668964, -0.45112324, -0.61368493, -0.7848025 ,\n",
      "       -0.96492625, -1.1545302 , -1.35411331, -1.56420079, -1.78534551,\n",
      "       -2.01812942]), array([ 1.36617528,  1.28798671,  1.20568296,  1.11904743,  1.02785213,\n",
      "        0.93185708,  0.83080966,  0.72444396,  0.61248006,  0.49462332,\n",
      "        0.3705636 ,  0.23997442,  0.10251212, -0.04218503, -0.19449782,\n",
      "       -0.35482708, -0.52359472, -0.70124486, -0.88824502, -1.08508728,\n",
      "       -1.29228967, -1.51039745, -1.73998458, -1.98165524]), array([ 1.33595584,  1.27245261,  1.20560709,  1.1352434 ,  1.06117635,\n",
      "        0.98321103,  0.90114228,  0.81475412,  0.72381922,  0.62809826,\n",
      "        0.52733936,  0.42127737,  0.30963316,  0.19211294,  0.06840744,\n",
      "       -0.06180886, -0.19887866, -0.34316266, -0.49504056, -0.65491203,\n",
      "       -0.82319778, -1.00034068, -1.18680689, -1.38308711, -1.58969787,\n",
      "       -1.80718288, -2.03611447])]\n",
      "[32, 16, 14, 27, 22, 15, 16, 26, 24, 27]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[439], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m all_final_rewards \u001b[38;5;241m=\u001b[39m discount_and_normalize_rewards(all_rewards,\n\u001b[0;32m      5\u001b[0m                                                    discount_factor)\n\u001b[0;32m      6\u001b[0m all_mean_grads \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_policy_gradient_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_final_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_actions\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model1_1\u001b[38;5;241m.\u001b[39mtrainable_variables)):\n\u001b[0;32m     11\u001b[0m     mean_grads \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\n\u001b[0;32m     12\u001b[0m         [final_reward \u001b[38;5;241m*\u001b[39m all_grads[episode_index][step][var_index]\n\u001b[0;32m     13\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m episode_index, final_rewards \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_final_rewards)\n\u001b[0;32m     14\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m step, final_reward \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(final_rewards)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[437], line 20\u001b[0m, in \u001b[0;36mcalculate_policy_gradient_loss\u001b[1;34m(model, all_final_rewards, all_grads, all_observations, all_actions)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_final_rewards)  \u001b[38;5;66;03m# Inspect the structure thoroughly\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m([\u001b[38;5;28mlen\u001b[39m(episode_rewards) \u001b[38;5;28;01mfor\u001b[39;00m episode_rewards \u001b[38;5;129;01min\u001b[39;00m all_final_rewards])  \u001b[38;5;66;03m# Check lengths\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m all_final_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_final_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:  \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Simulate the episodes again to get action probabilities \u001b[39;00m\n\u001b[0;32m     24\u001b[0m     all_log_probs \u001b[38;5;241m=\u001b[39m [] \n",
      "File \u001b[1;32mc:\\Users\\GioDude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\GioDude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads, all_observations, all_actions = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model1_1, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_factor)\n",
    "    all_mean_grads = []\n",
    "\n",
    "    loss = calculate_policy_gradient_loss(model1_1, all_final_rewards, all_grads, all_observations, all_actions) \n",
    "\n",
    "    for var_index in range(len(model1_1.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model1_1.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degugging\n",
    "\n",
    "### Version 1: \n",
    "model1_1 = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu'), \n",
    "    keras.layers.Dense(32, activation='relu'),  # Another hidden layer\n",
    "    keras.layers.Dense(1, activation='sigmoid')   # Output layer: probabilities for 2 actions\n",
    "])\n",
    "\n",
    "    So the issue with this isn't the code or the model just need to make a choice here. When using sequential models you need to ensure that the data is consistent in shape to feed into training. However in my case I wanted to add a Policy Gradient Loss Calculation as well. This means the shapes of the \"all_final_rewards\" need to be uniformed either by enforcing; a max size through early termination, padding shorter sequences to a certain length, OR just switching to an RNN since it can handle inconsistent lengths of data (does add more complexity tho).\n",
    "\n",
    "Tried to use early termination but that didn't seem to work.\n",
    "\n",
    "Okay so the issue is that we are trying to convert inconsistent data lengths (which I assume is because it takes different steps to succeed or fail the goal), into a tensorFlow format. This is the issue when implementing a calc_gradient_loss function. Although this is not explicitly in this section, it appears to be good practice to integrate it.\n",
    "1. Early Termination\n",
    "2. Padding\n",
    "3. RNN\n",
    "\n",
    "Even when adding a padding function to help with shape it still does not accept the shape. Probably due to the 0s being interpretted as T/F or something.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1_1\n",
    "## Dataset: Gym\n",
    "\n",
    "## Programmer: Giovanni Vecchione\n",
    "## Date: 4/17/24\n",
    "## Subject: Machine Learning 2 - Project 6\n",
    "Use Reinforced Learning (RL) to build the project. Submit your project as Jupyter notebook.\n",
    "\n",
    "## Policy: \n",
    "Cusom Reward function w/ RNN\n",
    "## Status: \n",
    "Incomplete, was unable to figure out the shape mismatch issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mtp\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "#Checks if GPU is being used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0)) \n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "#Using GPU: NVIDIA GeForce GTX 1660 SUPER - Successful\n",
    "#NOTE: This took some time to set up by installing and pathing the cuda toolkit v.12.4 and the right supplemental packages. This drastically improved\n",
    "#training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "#render_mode=\"rgb_array\" dosn't work for some reason when using the render call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole-v1 is a classic control problem where the goal is to balance a pole on a cart by applying forces left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)    # See what kind of data the environment provides\n",
    "print(env.action_space)         # See the agent's possible actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render()\n",
    "img.shape #only works when using hte rgb call from earlier, however it does not seem to function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GioDude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "action = 1  # accelerate right\n",
    "obs, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The step() method executes the desired action and returns five values:\n",
    "\n",
    "### *1. obs*\n",
    "    This is the new observation. The cart is now moving toward the right (obs[1] > 0). The pole is still tilted toward the right (obs[2] > 0), but its angular velocity is now negative (obs[3] < 0), so it will likely be tilted toward the left after the next step.\n",
    "\n",
    "### *2. reward*\n",
    "    In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep the episode running for as long as possible.\n",
    "\n",
    "### *3. done*\n",
    "    This value will be True when the episode is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, you have won). After that, the environment must be reset before it can be used again.\n",
    "\n",
    "### *4. truncated*\n",
    "    This value will be True when an episode is interrupted early, for example by an environment wrapper that imposes a maximum number of steps per episode (see Gym's documentation for more details on environment wrappers). Some RL algorithms treat truncated episodes differently from episodes finished normally (i.e., when done is True), but in this chapter we will treat them identically.\n",
    "\n",
    "### *5. info*\n",
    "    This environment-specific dictionary may provide extra information, just like the one returned by the reset() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 50\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Policies\n",
    "\n",
    "Basically using a neural net instead of a basic policy function\n",
    "\n",
    "*Typical Work Flow:* \n",
    "\n",
    "Define Neural Network (model1_1) and loss function (loss_fn)\n",
    "\n",
    "For each iteration:\n",
    "\n",
    "1. Run multiple episodes using play_multiple_episodes\n",
    "2. Discount and Normalize the rewards using the helper functions\n",
    "3. Update the model1_1 parameters based on the collected gradients (this would need an optimizer and a gradient application step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the code to build a basic neural network policy using Keras:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model1_1 = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu'), \n",
    "    keras.layers.Dense(32, activation='relu'),  # Another hidden layer\n",
    "    keras.layers.Dense(1, activation='sigmoid')   # Output layer: probabilities for 2 actions\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients:\n",
    "Neural Nets cannot train on their own and must have a policy to follow. In this case we're using a Policy Gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core Action: This function takes a single step within the environment.\n",
    "\n",
    "Neural Network Interaction: It receives the current observation (obs), \n",
    "passes it through the model (model1_1) to get the probability of moving left (left_proba).\n",
    "\n",
    "Action Selection: An action is sampled based on that probability.\n",
    "\n",
    "Environment Update: The action is executed, and the function gets the next observation, reward, and 'done' flag from the environment.\n",
    "\n",
    "Loss Calculation: It prepares data for calculating the policy gradient loss, using a loss_fn (due to cross entropy stated earlier).\n",
    "\n",
    "Gradient Calculation: Uses a tf.GradientTape to record the operations, enabling the calculation of the policy gradient.\n",
    "\n",
    "NOTE: Returning actions to track\n",
    "\"\"\"\n",
    "\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Episode Loop: This function is responsible for running n_episodes.\n",
    "\n",
    "Data Collection: It collects the rewards (all_rewards) and gradients (all_grads) produced by play_one_step during each episode.\n",
    "*New* imporvements: added an observation collection also.\n",
    "    Added an action collection also.\n",
    "\"\"\"\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(\n",
    "                env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef calculate_advantage(all_final_rewards, baseline=None):\\n    if baseline is None:\\n        baseline = np.mean(all_final_rewards)  # Simple average baseline\\n\\n    advantages = np.array([reward - baseline for reward in all_final_rewards])\\n    return advantages\\n'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "advantage-based update*\n",
    "\n",
    "Advantage function attempts to answer the question: \"Was this action better or worse than expected in the current situation?\" \n",
    "It provides a more refined learning signal compared to just using the raw rewards.\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def calculate_advantage(all_final_rewards, baseline=None):\n",
    "    if baseline is None:\n",
    "        baseline = np.mean(all_final_rewards)  # Simple average baseline\n",
    "\n",
    "    advantages = np.array([reward - baseline for reward in all_final_rewards])\n",
    "    return advantages\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Discounted Returns: This straightforward function takes a list of rewards from a single episode and calculates the \n",
    "discounted cumulative rewards, with future rewards being weighted less by the discount_factor.\n",
    "\"\"\"\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "\"\"\" \n",
    "Normalization: This function applies the discount_rewards to each episode's rewards and then normalizes them \n",
    "(subtracting the mean and dividing by the standard deviation). Normalization can often improve stability during learning.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* Since we are sampling a single action based on the probability, the y_target in the play_one_step function represents which action was actually taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)  #Nadam is an extension of adam and is common when using RL\n",
    "loss_fn = tf.keras.losses.binary_crossentropy #You can replace this with a custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model1_1, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_factor)\n",
    "    # Advantage Calculation \n",
    "    #all_advantages = calculate_advantage(all_final_rewards)  \n",
    "\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model1_1.trainable_variables)):\n",
    "\n",
    "        # Get gradients from 'play_one_step' for the variable \n",
    "        #grads_from_play_one_step = [grads[var_index] for episode_rewards, grads in zip(all_rewards, all_grads)]\n",
    "\n",
    "    \n",
    "        # Calculate the mean\n",
    "        #mean_grad = tf.reduce_mean(grads_from_play_one_step, axis=0)\n",
    "                                   \n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        \n",
    "        #Implement the advanatage-based factor to the mean_grads\n",
    "        #modified_grad = mean_grad * all_advantages \n",
    "\n",
    "        all_mean_grads.append(mean_grads)\n",
    "\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model1_1.trainable_variables))\n",
    "\n",
    "    \"\"\" \n",
    "    NOTE: The commented out code lines above are meant to implement a custom advantage-based function.\n",
    "    However sequential models are very limited it seems in being able to customize with variable length of data.\n",
    "    A solution to this could be to implement an RNN however this would take some more time.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, n_test_episodes=100):\n",
    "    total_rewards = []\n",
    "    for _ in range(n_test_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = np.argmax(model.predict(obs[np.newaxis])[0])  # Get action from the model\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[223], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the trained model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m avg_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage reward after training:\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_reward)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mThis in no means is the standard in evaluating how a model performs in game.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mCouldn't find a specific way to do this so went with a general avg_reward evaluation.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mWe could add this to the training loop to evaluate the average overtime.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[222], line 8\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, n_test_episodes)\u001b[0m\n\u001b[0;32m      6\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m----> 8\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m)[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Get action from the model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     10\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model\n",
    "avg_reward = evaluate_model(model1_1)\n",
    "print(\"Average reward after training:\", avg_reward)\n",
    "\n",
    "\"\"\" \n",
    "This in no means is the standard in evaluating how a model performs in game.\n",
    "Couldn't find a specific way to do this so went with a general avg_reward evaluation.\n",
    "We could add this to the training loop to evaluate the average overtime.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Degugging\n",
    "\n",
    "### ISSUE 1: Custom Loss Function not working\n",
    "### 1: SOLVED\n",
    "model1_1 = keras.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu'), \n",
    "    keras.layers.Dense(32, activation='relu'),  # Another hidden layer\n",
    "    keras.layers.Dense(1, activation='sigmoid')   # Output layer: probabilities for 2 actions\n",
    "])\n",
    "\n",
    "    So the issue with this isn't the code or the model just need to make a choice here. When using sequential models you need to ensure that the data is consistent in shape to feed into training. However in my case I wanted to add a Policy Gradient Loss Calculation as well. This means the shapes of the \"all_final_rewards\" need to be uniformed either by enforcing; a max size through early termination, padding shorter sequences to a certain length, OR just switching to an RNN since it can handle inconsistent lengths of data (does add more complexity tho).\n",
    "\n",
    "Tried to use early termination but that didn't seem to work.\n",
    "\n",
    "Okay so the issue is that we are trying to convert inconsistent data lengths (which I assume is because it takes different steps to succeed or fail the goal), into a tensorFlow format. This is the issue when implementing a calc_gradient_loss function. Although this is not explicitly in this section, it appears to be good practice to integrate it.\n",
    "1. Early Termination\n",
    "2. Padding\n",
    "3. RNN\n",
    "\n",
    "Even when adding a padding function to help with shape it still does not accept the shape. Probably due to the 0s being interpretted as T/F or something.\n",
    "\n",
    "\n",
    "*FOUND IT*, I tried to implement muliple loss functions at once, I removed my custom one and instead am using an advantage-based function to act as a helper.\n",
    "\n",
    "### ISSUE 2: Advantage-Based Function not working\n",
    "### 2: SOLVED\n",
    "\n",
    "Sequential models are very limited it seems in being able to customize with variable length of data.\n",
    "A solution to this could be to implement an RNN however this would take some more time.\n",
    "\n",
    "### MODEL DECISION FOR TESTING : Focus on the parameters of the actual model. We'll implement custom functions another time.\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "* RNN Exploration: While you're focusing on sequential model optimization now, keep the RNN option in mind.  If you hit limitations with the sequential approach, the time investment to migrate to an RNN model could pay off in terms of performance.\n",
    "\n",
    "* Advanced Advantage:  If your current advantage calculation is working well, consider investigating Generalized Advantage Estimation (GAE)  later on for potentially even better learning stability.\n",
    "\n",
    "### ISSUE 3: How do we even evaluate a Neural Net Policy? \n",
    "\n",
    "Can't tell how I can see how it's performing.\n",
    "Attempting a avg_reward function?\n",
    "\n",
    "It's best practice to use a separate CartPole environment for evaluation. This ensures your model generalizes well and isn't memorizing specific training episodes. Here's how to create a separate environment:\n",
    "import gym\n",
    "\n",
    "#### Environment for training\n",
    "train_env = gym.make('CartPole-v1')\n",
    "\n",
    "##### Environment for evaluation (keep separate)\n",
    "eval_env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING:\n",
    "\n",
    "### TEST #1: \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

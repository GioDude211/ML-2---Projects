{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1_2\n",
    "## Dataset: MNIST FASHION from KERAS\n",
    "\n",
    "## Programmer: Giovanni Vecchione\n",
    "## Date: 4/10/24\n",
    "## Subject: Machine Learning 2 - Project 5\n",
    "Use Generative Adversarial Networks (GAN)  to build the project. Submit your project as Jupyter notebook\n",
    "\n",
    "Goal: Create a fashion image generator using a GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What is a Generative Adversarial Network (GAN)?**\n",
    "\n",
    "#### **The Art of Imitation:** GANs are a type of deep learning architecture where two neural networks are locked in an elaborate game of cat-and-mouse.  Think of them as a master art forger and an expert art critic:\n",
    "   **The Generator:** The 'art forger' responsible for creating realistic fakes that could pass as authentic pieces (e.g., images, music, text etc.).\n",
    "     \n",
    "   **The Discriminator:**  The 'art critic'  trained to tell the difference between a genuine masterpiece and the generator's forgeries.\n",
    "\n",
    "#### **A Competitive Game:**  Both networks get better through this competition.  The generator keeps refining its fakes to fool the discriminator, and the discriminator gets better at spotting the subtle flaws in those fakes. Over time, the generator gets so good that its creations become indistinguishable from real data.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "1. **Unsupervised Learning:** GANs don't need labeled data. They learn from a dataset of examples (e.g., a collection of real photographs), extracting the patterns and underlying structure without explicit labels like \"dog\" or \"cat.\"\n",
    "\n",
    "2. **Generator Network:**\n",
    "   * Takes random noise as input.\n",
    "   * Tries to transform that noise into data that resembles the real examples it has seen.\n",
    "   * Its goal is to make the discriminator believe its outputs are real.\n",
    "\n",
    "3. **Discriminator Network:**\n",
    "   * Takes as input both real data samples and the generator's outputs.\n",
    "   * Tries to classify whether an input is \"real\" (from the training dataset) or \"fake\" (created by the generator).\n",
    "   * Gives feedback to the generator to help it improve its fakes.\n",
    "\n",
    "4. **Adversarial Training:**\n",
    "   * The generator and discriminator are trained simultaneously.\n",
    "   * The generator improves by making the discriminator's job harder.\n",
    "   * The discriminator improves by becoming a better judge of authenticity.\n",
    "   * It's this constant push-and-pull that makes GANs so powerful.\n",
    "\n",
    "**Why use GANs?**\n",
    "\n",
    "* **Generating New Data:** Create realistic images, music, videos, or other data forms that weren't in your original dataset.\n",
    "* **Data Augmentation:** Increase the size and diversity of a dataset, useful in areas where real data is scarce.\n",
    "* **Image-to-Image Translation:** Change images from one style to another (e.g., turning sketches into photos).\n",
    "* **Super Resolution:** Enhancing image or video quality.\n",
    "\n",
    "**Ready to Build?**\n",
    "\n",
    "Building a GAN involves:\n",
    "\n",
    "* **Choosing Your Domain:**  Images, text, audio, etc.\n",
    "* **Data Preparation:**  Gathering or creating a suitable dataset.\n",
    "* **Network Architecture:**  Designing your generator and discriminator networks (often using convolutional neural networks for images).\n",
    "* **Training Setup:**  Defining the loss functions that guide the networks and selecting an optimization algorithm. \n",
    "* **Implementation:**  Coding your GAN using a deep learning framework like TensorFlow or PyTorch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "#Checks if GPU is being used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0)) \n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "#Using GPU: NVIDIA GeForce GTX 1660 SUPER - Successful\n",
    "#NOTE: This took some time to set up by installing and pathing the cuda toolkit v.12.4 and the right supplemental packages. This drastically improved\n",
    "#training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "#split dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Limit Dataset Sizes\n",
    "train_images = train_images[:6000]\n",
    "train_labels = train_labels[:6000]\n",
    "test_images = test_images[:3000]\n",
    "test_labels = test_labels[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing:\n",
    "\n",
    "# Data Conversion and Normalization\n",
    "train_images = tf.cast(train_images, tf.float32)  # Convert to float32 / this type is needed, MNIST is unit8 type\n",
    "train_images /= 255.0  # Normalize to [0, 1] range\n",
    "\n",
    "# Do the same for the test dataset if needed:\n",
    "test_images = tf.cast(test_images, tf.float32)\n",
    "test_images /= 255.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at the first image and its label\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.title(train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_size = 100\n",
    "\n",
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(7 * 7 * 128),\n",
    "    tf.keras.layers.Reshape([7, 7, 128]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"tanh\"),\n",
    "])\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
    "                           activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
    "                           activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan = tf.keras.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size=1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training loop is unusual, we cannot use the regular fit() method. Instead, we will write a custom training loop. For this, we first need to create a Dataset to iterate through the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
    "    generator, discriminator = gan.layers\n",
    "\n",
    "    display_interval = 1  # Display images every 5 epochs\n",
    "    num_images_to_display = 2 \n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch in dataset:\n",
    "\n",
    "            # phase 1 - training the discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, tf.expand_dims(X_batch, axis=-1)], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator_loss = discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            print(f\"Discriminator Loss: {discriminator_loss}\") \n",
    "\n",
    "            # phase 2 - training the generator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            generator_loss = gan.train_on_batch(noise, y2)\n",
    "            print(f\"Generator Loss: {generator_loss}\")\n",
    "\n",
    "        # Image Visualization\n",
    "        if (epoch + 1) % display_interval == 0: \n",
    "            \n",
    "            noise = tf.random.normal([num_images_to_display, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            for i in range(generated_images.shape[0]):\n",
    "                plt.subplot(4, 4, i+1)\n",
    "                img = generated_images[i, :, :, :]\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Debugging \n",
    "            print(\"About to save image...\")  \n",
    "            print(f\"Shape of generated_images: {generated_images.shape}\")\n",
    "\n",
    "            plt.savefig(f\"generated_images_GAN/generated_images_epoch{epoch + 1}.png\") #Have to create a folder named \"generated_images_GAN\" in the same project file.\n",
    "            plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: 1.0839431285858154\n",
      "Generator Loss: 0.2545773386955261\n",
      "Discriminator Loss: 1.0947057008743286\n",
      "Generator Loss: 0.26304519176483154\n",
      "Discriminator Loss: 1.1077325344085693\n",
      "Generator Loss: 0.2662310004234314\n",
      "Discriminator Loss: 1.1167733669281006\n",
      "Generator Loss: 0.2748752236366272\n",
      "Discriminator Loss: 1.1305452585220337\n",
      "Generator Loss: 0.2777939736843109\n",
      "Discriminator Loss: 1.1394826173782349\n",
      "Generator Loss: 0.28356581926345825\n",
      "Discriminator Loss: 1.1493325233459473\n",
      "Generator Loss: 0.2908141016960144\n",
      "Discriminator Loss: 1.1618577241897583\n",
      "Generator Loss: 0.29459601640701294\n",
      "Discriminator Loss: 1.1714273691177368\n",
      "Generator Loss: 0.3011634051799774\n",
      "Discriminator Loss: 1.1812937259674072\n",
      "Generator Loss: 0.3055483400821686\n",
      "Discriminator Loss: 1.1909265518188477\n",
      "Generator Loss: 0.3114650547504425\n",
      "Discriminator Loss: 1.2011443376541138\n",
      "Generator Loss: 0.3149050176143646\n",
      "Discriminator Loss: 1.2101919651031494\n",
      "Generator Loss: 0.3190280795097351\n",
      "Discriminator Loss: 1.2192394733428955\n",
      "Generator Loss: 0.3243328630924225\n",
      "Discriminator Loss: 1.2280423641204834\n",
      "Generator Loss: 0.3289443254470825\n",
      "Discriminator Loss: 1.2378665208816528\n",
      "Generator Loss: 0.3329967260360718\n",
      "Discriminator Loss: 1.2451626062393188\n",
      "Generator Loss: 0.33932268619537354\n",
      "Discriminator Loss: 1.2531441450119019\n",
      "Generator Loss: 0.344475656747818\n",
      "Discriminator Loss: 1.2607108354568481\n",
      "Generator Loss: 0.3488636314868927\n",
      "Discriminator Loss: 1.2677141427993774\n",
      "Generator Loss: 0.3541053235530853\n",
      "Discriminator Loss: 1.2764811515808105\n",
      "Generator Loss: 0.35658758878707886\n",
      "Discriminator Loss: 1.2812387943267822\n",
      "Generator Loss: 0.3636453449726105\n",
      "Discriminator Loss: 1.2902954816818237\n",
      "Generator Loss: 0.3672117590904236\n",
      "Discriminator Loss: 1.2980389595031738\n",
      "Generator Loss: 0.3714524805545807\n",
      "Discriminator Loss: 1.3054280281066895\n",
      "Generator Loss: 0.37490299344062805\n",
      "Discriminator Loss: 1.3122451305389404\n",
      "Generator Loss: 0.37987738847732544\n",
      "Discriminator Loss: 1.3195174932479858\n",
      "Generator Loss: 0.38300710916519165\n",
      "Discriminator Loss: 1.3247162103652954\n",
      "Generator Loss: 0.38669833540916443\n",
      "Discriminator Loss: 1.3305052518844604\n",
      "Generator Loss: 0.39131686091423035\n",
      "Discriminator Loss: 1.3379157781600952\n",
      "Generator Loss: 0.39395827054977417\n",
      "Discriminator Loss: 1.3426909446716309\n",
      "Generator Loss: 0.3969452679157257\n",
      "Discriminator Loss: 1.349615216255188\n",
      "Generator Loss: 0.40259790420532227\n",
      "Discriminator Loss: 1.3580422401428223\n",
      "Generator Loss: 0.4050087630748749\n",
      "Discriminator Loss: 1.3631272315979004\n",
      "Generator Loss: 0.4096585810184479\n",
      "Discriminator Loss: 1.3699085712432861\n",
      "Generator Loss: 0.4126195013523102\n",
      "Discriminator Loss: 1.3753416538238525\n",
      "Generator Loss: 0.41509294509887695\n",
      "Discriminator Loss: 1.3809936046600342\n",
      "Generator Loss: 0.41963618993759155\n",
      "Discriminator Loss: 1.3851280212402344\n",
      "Generator Loss: 0.4246077239513397\n",
      "Discriminator Loss: 1.3909095525741577\n",
      "Generator Loss: 0.4273007810115814\n",
      "Discriminator Loss: 1.3954206705093384\n",
      "Generator Loss: 0.4322957992553711\n",
      "Discriminator Loss: 1.400822401046753\n",
      "Generator Loss: 0.4340812563896179\n",
      "Discriminator Loss: 1.403283953666687\n",
      "Generator Loss: 0.4375162124633789\n",
      "Discriminator Loss: 1.4067543745040894\n",
      "Generator Loss: 0.4421229660511017\n",
      "Discriminator Loss: 1.412200689315796\n",
      "Generator Loss: 0.44487109780311584\n",
      "Discriminator Loss: 1.4177987575531006\n",
      "Generator Loss: 0.4478686451911926\n",
      "Discriminator Loss: 1.421268343925476\n",
      "Generator Loss: 0.45075371861457825\n",
      "Discriminator Loss: 1.4266706705093384\n",
      "Generator Loss: 0.453169584274292\n",
      "Discriminator Loss: 1.4296928644180298\n",
      "Generator Loss: 0.45476290583610535\n",
      "Discriminator Loss: 1.4324183464050293\n",
      "Generator Loss: 0.4593222737312317\n",
      "Discriminator Loss: 1.436982274055481\n",
      "Generator Loss: 0.46206551790237427\n",
      "Discriminator Loss: 1.4407469034194946\n",
      "Generator Loss: 0.46435222029685974\n",
      "Discriminator Loss: 1.443961501121521\n",
      "Generator Loss: 0.4687305986881256\n",
      "Discriminator Loss: 1.4484540224075317\n",
      "Generator Loss: 0.47050929069519043\n",
      "Discriminator Loss: 1.4507288932800293\n",
      "Generator Loss: 0.4724053144454956\n",
      "Discriminator Loss: 1.4533438682556152\n",
      "Generator Loss: 0.4753016531467438\n",
      "Discriminator Loss: 1.4560686349868774\n",
      "Generator Loss: 0.4781002700328827\n",
      "Discriminator Loss: 1.4587366580963135\n",
      "Generator Loss: 0.48004308342933655\n",
      "Discriminator Loss: 1.4620990753173828\n",
      "Generator Loss: 0.4817526936531067\n",
      "Discriminator Loss: 1.4646766185760498\n",
      "Generator Loss: 0.4852728247642517\n",
      "Discriminator Loss: 1.467790126800537\n",
      "Generator Loss: 0.4865871071815491\n",
      "Discriminator Loss: 1.4698009490966797\n",
      "Generator Loss: 0.49018535017967224\n",
      "Discriminator Loss: 1.4724414348602295\n",
      "Generator Loss: 0.4911405146121979\n",
      "Discriminator Loss: 1.473821759223938\n",
      "Generator Loss: 0.49493134021759033\n",
      "Discriminator Loss: 1.4767862558364868\n",
      "Generator Loss: 0.4969317317008972\n",
      "Discriminator Loss: 1.4789851903915405\n",
      "Generator Loss: 0.4984169006347656\n",
      "Discriminator Loss: 1.4804632663726807\n",
      "Generator Loss: 0.5007901191711426\n",
      "Discriminator Loss: 1.4826842546463013\n",
      "Generator Loss: 0.502244234085083\n",
      "Discriminator Loss: 1.4840095043182373\n",
      "Generator Loss: 0.5062230825424194\n",
      "Discriminator Loss: 1.4861314296722412\n",
      "Generator Loss: 0.5072068572044373\n",
      "Discriminator Loss: 1.4869943857192993\n",
      "Generator Loss: 0.5109251737594604\n",
      "Discriminator Loss: 1.4891749620437622\n",
      "Generator Loss: 0.5130069255828857\n",
      "Discriminator Loss: 1.4907437562942505\n",
      "Generator Loss: 0.514660656452179\n",
      "Discriminator Loss: 1.4920008182525635\n",
      "Generator Loss: 0.5165703892707825\n",
      "Discriminator Loss: 1.4935499429702759\n",
      "Generator Loss: 0.5187698006629944\n",
      "Discriminator Loss: 1.4947340488433838\n",
      "Generator Loss: 0.5207855701446533\n",
      "Discriminator Loss: 1.4960765838623047\n",
      "Generator Loss: 0.5226708650588989\n",
      "Discriminator Loss: 1.497252106666565\n",
      "Generator Loss: 0.5246691703796387\n",
      "Discriminator Loss: 1.4985426664352417\n",
      "Generator Loss: 0.5258046388626099\n",
      "Discriminator Loss: 1.4996132850646973\n",
      "Generator Loss: 0.5280036926269531\n",
      "Discriminator Loss: 1.501013159751892\n",
      "Generator Loss: 0.52913498878479\n",
      "Discriminator Loss: 1.5023201704025269\n",
      "Generator Loss: 0.5305081605911255\n",
      "Discriminator Loss: 1.5033372640609741\n",
      "Generator Loss: 0.5320426225662231\n",
      "Discriminator Loss: 1.5053783655166626\n",
      "Generator Loss: 0.5343948006629944\n",
      "Discriminator Loss: 1.5070117712020874\n",
      "Generator Loss: 0.5357994437217712\n",
      "Discriminator Loss: 1.5079740285873413\n",
      "Generator Loss: 0.5373618602752686\n",
      "Discriminator Loss: 1.5086417198181152\n",
      "Generator Loss: 0.5387513041496277\n",
      "Discriminator Loss: 1.5091922283172607\n",
      "Generator Loss: 0.5403602123260498\n",
      "Discriminator Loss: 1.510969638824463\n",
      "Generator Loss: 0.5417510271072388\n",
      "Discriminator Loss: 1.512040615081787\n",
      "Generator Loss: 0.5429403781890869\n",
      "Discriminator Loss: 1.5132802724838257\n",
      "Generator Loss: 0.544379472732544\n",
      "Discriminator Loss: 1.5139347314834595\n",
      "Generator Loss: 0.5457925200462341\n",
      "Discriminator Loss: 1.5147638320922852\n",
      "Generator Loss: 0.5473889112472534\n",
      "Discriminator Loss: 1.5152738094329834\n",
      "Generator Loss: 0.5483147501945496\n",
      "Discriminator Loss: 1.51579749584198\n",
      "Generator Loss: 0.5491867661476135\n",
      "Discriminator Loss: 1.5163321495056152\n",
      "Generator Loss: 0.5502867698669434\n",
      "Discriminator Loss: 1.516093134880066\n",
      "Generator Loss: 0.5510343909263611\n",
      "Discriminator Loss: 1.516387701034546\n",
      "Generator Loss: 0.5527341961860657\n",
      "Discriminator Loss: 1.5168511867523193\n",
      "Generator Loss: 0.5537592172622681\n",
      "Discriminator Loss: 1.5174976587295532\n",
      "Generator Loss: 0.5548783540725708\n",
      "Discriminator Loss: 1.5175483226776123\n",
      "Generator Loss: 0.5557902455329895\n",
      "Discriminator Loss: 1.5178189277648926\n",
      "Generator Loss: 0.556678056716919\n",
      "Discriminator Loss: 1.5180537700653076\n",
      "Generator Loss: 0.5573819875717163\n",
      "Discriminator Loss: 1.5182571411132812\n",
      "Generator Loss: 0.5588183403015137\n",
      "Discriminator Loss: 1.5183302164077759\n",
      "Generator Loss: 0.5592524409294128\n",
      "Discriminator Loss: 1.5180370807647705\n",
      "Generator Loss: 0.5600622296333313\n",
      "Discriminator Loss: 1.517496943473816\n",
      "Generator Loss: 0.5607832074165344\n",
      "Discriminator Loss: 1.517219066619873\n",
      "Generator Loss: 0.5615366697311401\n",
      "Discriminator Loss: 1.516829252243042\n",
      "Generator Loss: 0.5621876120567322\n",
      "Discriminator Loss: 1.5164679288864136\n",
      "Generator Loss: 0.5628131031990051\n",
      "Discriminator Loss: 1.5159906148910522\n",
      "Generator Loss: 0.5631772875785828\n",
      "Discriminator Loss: 1.5156276226043701\n",
      "Generator Loss: 0.563853919506073\n",
      "Discriminator Loss: 1.5150612592697144\n",
      "Generator Loss: 0.5642368197441101\n",
      "Discriminator Loss: 1.5140659809112549\n",
      "Generator Loss: 0.5646203756332397\n",
      "Discriminator Loss: 1.512567400932312\n",
      "Generator Loss: 0.5650300979614258\n",
      "Discriminator Loss: 1.5112347602844238\n",
      "Generator Loss: 0.5657610297203064\n",
      "Discriminator Loss: 1.5100353956222534\n",
      "Generator Loss: 0.5658400654792786\n",
      "Discriminator Loss: 1.5088016986846924\n",
      "Generator Loss: 0.5660876631736755\n",
      "Discriminator Loss: 1.507894515991211\n",
      "Generator Loss: 0.5664443969726562\n",
      "Discriminator Loss: 1.5067578554153442\n",
      "Generator Loss: 0.5665152668952942\n",
      "Discriminator Loss: 1.5055407285690308\n",
      "Generator Loss: 0.566757321357727\n",
      "Discriminator Loss: 1.5042160749435425\n",
      "Generator Loss: 0.566757321357727\n",
      "Discriminator Loss: 1.502949595451355\n",
      "Generator Loss: 0.5667060613632202\n",
      "Discriminator Loss: 1.5015963315963745\n",
      "Generator Loss: 0.5667221546173096\n",
      "Discriminator Loss: 1.50013267993927\n",
      "Generator Loss: 0.5665789842605591\n",
      "Discriminator Loss: 1.498769998550415\n",
      "Generator Loss: 0.5666090250015259\n",
      "Discriminator Loss: 1.497424602508545\n",
      "Generator Loss: 0.5664876103401184\n",
      "Discriminator Loss: 1.4959373474121094\n",
      "Generator Loss: 0.5665881633758545\n",
      "Discriminator Loss: 1.4942998886108398\n",
      "Generator Loss: 0.5665785074234009\n",
      "Discriminator Loss: 1.4928117990493774\n",
      "Generator Loss: 0.5665234923362732\n",
      "Discriminator Loss: 1.4913641214370728\n",
      "Generator Loss: 0.5665109157562256\n",
      "Discriminator Loss: 1.4897596836090088\n",
      "Generator Loss: 0.5662522315979004\n",
      "Discriminator Loss: 1.4881694316864014\n",
      "Generator Loss: 0.5661009550094604\n",
      "Discriminator Loss: 1.4866600036621094\n",
      "Generator Loss: 0.5657764673233032\n",
      "Discriminator Loss: 1.4850763082504272\n",
      "Generator Loss: 0.5655111074447632\n",
      "Discriminator Loss: 1.483543872833252\n",
      "Generator Loss: 0.565296471118927\n",
      "Discriminator Loss: 1.4820438623428345\n",
      "Generator Loss: 0.565105676651001\n",
      "Discriminator Loss: 1.4804353713989258\n",
      "Generator Loss: 0.5648903846740723\n",
      "Discriminator Loss: 1.4787808656692505\n",
      "Generator Loss: 0.5647573471069336\n",
      "Discriminator Loss: 1.477090835571289\n",
      "Generator Loss: 0.5644546151161194\n",
      "Discriminator Loss: 1.4755330085754395\n",
      "Generator Loss: 0.5642213225364685\n",
      "Discriminator Loss: 1.473909854888916\n",
      "Generator Loss: 0.5639744400978088\n",
      "Discriminator Loss: 1.4722745418548584\n",
      "Generator Loss: 0.5640342831611633\n",
      "Discriminator Loss: 1.4708800315856934\n",
      "Generator Loss: 0.5635669827461243\n",
      "Discriminator Loss: 1.4694275856018066\n",
      "Generator Loss: 0.5632419586181641\n",
      "Discriminator Loss: 1.467921495437622\n",
      "Generator Loss: 0.5629225969314575\n",
      "Discriminator Loss: 1.466411828994751\n",
      "Generator Loss: 0.5630778670310974\n",
      "Discriminator Loss: 1.4645763635635376\n",
      "Generator Loss: 0.5627793073654175\n",
      "Discriminator Loss: 1.4634140729904175\n",
      "Generator Loss: 0.562512218952179\n",
      "Discriminator Loss: 1.46214759349823\n",
      "Generator Loss: 0.5621671676635742\n",
      "Discriminator Loss: 1.4608893394470215\n",
      "Generator Loss: 0.5617086887359619\n",
      "Discriminator Loss: 1.4595274925231934\n",
      "Generator Loss: 0.5614021420478821\n",
      "Discriminator Loss: 1.4580261707305908\n",
      "Generator Loss: 0.5611796379089355\n",
      "Discriminator Loss: 1.4565575122833252\n",
      "Generator Loss: 0.560859739780426\n",
      "Discriminator Loss: 1.4551273584365845\n",
      "Generator Loss: 0.5604639649391174\n",
      "Discriminator Loss: 1.4537101984024048\n",
      "Generator Loss: 0.5600628852844238\n",
      "Discriminator Loss: 1.452237606048584\n",
      "Generator Loss: 0.5596902966499329\n",
      "Discriminator Loss: 1.4508060216903687\n",
      "Generator Loss: 0.5594215989112854\n",
      "Discriminator Loss: 1.4493821859359741\n",
      "Generator Loss: 0.5589866042137146\n",
      "Discriminator Loss: 1.4479844570159912\n",
      "Generator Loss: 0.558611273765564\n",
      "Discriminator Loss: 1.446613073348999\n",
      "Generator Loss: 0.5584876537322998\n",
      "Discriminator Loss: 1.4451491832733154\n",
      "Generator Loss: 0.5582402944564819\n",
      "Discriminator Loss: 1.443724513053894\n",
      "Generator Loss: 0.5580254197120667\n",
      "Discriminator Loss: 1.442421793937683\n",
      "Generator Loss: 0.5576918721199036\n",
      "Discriminator Loss: 1.4411534070968628\n",
      "Generator Loss: 0.5572201609611511\n",
      "Discriminator Loss: 1.4399093389511108\n",
      "Generator Loss: 0.556795060634613\n",
      "Discriminator Loss: 1.4385517835617065\n",
      "Generator Loss: 0.5563775300979614\n",
      "Discriminator Loss: 1.4372215270996094\n",
      "Generator Loss: 0.5558655261993408\n",
      "Discriminator Loss: 1.4358928203582764\n",
      "Generator Loss: 0.555420994758606\n",
      "Discriminator Loss: 1.4345428943634033\n",
      "Generator Loss: 0.5549682378768921\n",
      "Discriminator Loss: 1.4332784414291382\n",
      "Generator Loss: 0.5546053647994995\n",
      "Discriminator Loss: 1.431909203529358\n",
      "Generator Loss: 0.5541858077049255\n",
      "Discriminator Loss: 1.4306167364120483\n",
      "Generator Loss: 0.5537521839141846\n",
      "Discriminator Loss: 1.429275631904602\n",
      "Generator Loss: 0.5533077120780945\n",
      "Discriminator Loss: 1.4278795719146729\n",
      "Generator Loss: 0.552991509437561\n",
      "Discriminator Loss: 1.4264414310455322\n",
      "Generator Loss: 0.5528132319450378\n",
      "Discriminator Loss: 1.4251073598861694\n",
      "Generator Loss: 0.5524563193321228\n",
      "Discriminator Loss: 1.4239906072616577\n",
      "Generator Loss: 0.5519347190856934\n",
      "Discriminator Loss: 1.422813057899475\n",
      "Generator Loss: 0.5514254570007324\n",
      "Discriminator Loss: 1.4214935302734375\n",
      "Generator Loss: 0.551011860370636\n",
      "Discriminator Loss: 1.4203077554702759\n",
      "Generator Loss: 0.5505638122558594\n",
      "Discriminator Loss: 1.4190062284469604\n",
      "Generator Loss: 0.5500718951225281\n",
      "Discriminator Loss: 1.417723536491394\n",
      "Generator Loss: 0.549571692943573\n",
      "Discriminator Loss: 1.4164968729019165\n",
      "Generator Loss: 0.5491470694541931\n",
      "Discriminator Loss: 1.4152170419692993\n",
      "Generator Loss: 0.5486783385276794\n",
      "Discriminator Loss: 1.4140316247940063\n",
      "Generator Loss: 0.5483002066612244\n",
      "Discriminator Loss: 1.4126967191696167\n",
      "Generator Loss: 0.5481128692626953\n",
      "Discriminator Loss: 1.4115376472473145\n",
      "Generator Loss: 0.5477851033210754\n",
      "About to save image...\n",
      "Shape of generated_images: (2, 28, 28, 1)\n",
      "Discriminator Loss: 1.4105377197265625\n",
      "Generator Loss: 0.5472478866577148\n",
      "Discriminator Loss: 1.4094321727752686\n",
      "Generator Loss: 0.5466957688331604\n",
      "Discriminator Loss: 1.408466100692749\n",
      "Generator Loss: 0.546190083026886\n",
      "Discriminator Loss: 1.407310962677002\n",
      "Generator Loss: 0.5456625819206238\n",
      "Discriminator Loss: 1.406151533126831\n",
      "Generator Loss: 0.5451763868331909\n",
      "Discriminator Loss: 1.4050489664077759\n",
      "Generator Loss: 0.5446603298187256\n",
      "Discriminator Loss: 1.4039697647094727\n",
      "Generator Loss: 0.5441261529922485\n",
      "Discriminator Loss: 1.402947187423706\n",
      "Generator Loss: 0.5436105728149414\n",
      "Discriminator Loss: 1.401777744293213\n",
      "Generator Loss: 0.5430984497070312\n",
      "Discriminator Loss: 1.4006690979003906\n",
      "Generator Loss: 0.5425863862037659\n",
      "Discriminator Loss: 1.399702787399292\n",
      "Generator Loss: 0.5420519709587097\n",
      "Discriminator Loss: 1.3987354040145874\n",
      "Generator Loss: 0.5415306687355042\n",
      "Discriminator Loss: 1.3977214097976685\n",
      "Generator Loss: 0.540982723236084\n",
      "Discriminator Loss: 1.3967719078063965\n",
      "Generator Loss: 0.5404824614524841\n",
      "Discriminator Loss: 1.3957358598709106\n",
      "Generator Loss: 0.5399587750434875\n",
      "Discriminator Loss: 1.3947774171829224\n",
      "Generator Loss: 0.5394546389579773\n",
      "Discriminator Loss: 1.393748164176941\n",
      "Generator Loss: 0.5390155911445618\n",
      "Discriminator Loss: 1.3926035165786743\n",
      "Generator Loss: 0.5387599468231201\n",
      "Discriminator Loss: 1.391427993774414\n",
      "Generator Loss: 0.5384494066238403\n",
      "Discriminator Loss: 1.3907973766326904\n",
      "Generator Loss: 0.5378915071487427\n",
      "Discriminator Loss: 1.3899664878845215\n",
      "Generator Loss: 0.5372742414474487\n",
      "Discriminator Loss: 1.3891538381576538\n",
      "Generator Loss: 0.5366383790969849\n",
      "Discriminator Loss: 1.388473391532898\n",
      "Generator Loss: 0.536100447177887\n",
      "Discriminator Loss: 1.3874742984771729\n",
      "Generator Loss: 0.5355647206306458\n",
      "Discriminator Loss: 1.3865362405776978\n",
      "Generator Loss: 0.5350463390350342\n",
      "Discriminator Loss: 1.3855278491973877\n",
      "Generator Loss: 0.5344882607460022\n",
      "Discriminator Loss: 1.3846526145935059\n",
      "Generator Loss: 0.5339457988739014\n",
      "Discriminator Loss: 1.383763313293457\n",
      "Generator Loss: 0.5333566069602966\n",
      "Discriminator Loss: 1.3829447031021118\n",
      "Generator Loss: 0.5328372716903687\n",
      "Discriminator Loss: 1.3820593357086182\n",
      "Generator Loss: 0.5322537422180176\n",
      "Discriminator Loss: 1.3811819553375244\n",
      "Generator Loss: 0.5317313075065613\n",
      "Discriminator Loss: 1.3803009986877441\n",
      "Generator Loss: 0.5312070846557617\n",
      "Discriminator Loss: 1.3794447183609009\n",
      "Generator Loss: 0.5306839942932129\n",
      "Discriminator Loss: 1.378494381904602\n",
      "Generator Loss: 0.5302011370658875\n",
      "Discriminator Loss: 1.3776229619979858\n",
      "Generator Loss: 0.529708981513977\n",
      "Discriminator Loss: 1.3767802715301514\n",
      "Generator Loss: 0.5291866064071655\n",
      "Discriminator Loss: 1.3761628866195679\n",
      "Generator Loss: 0.5287159085273743\n",
      "Discriminator Loss: 1.3752802610397339\n",
      "Generator Loss: 0.528217077255249\n",
      "Discriminator Loss: 1.3744961023330688\n",
      "Generator Loss: 0.5276347994804382\n",
      "Discriminator Loss: 1.3736743927001953\n",
      "Generator Loss: 0.5269988179206848\n",
      "Discriminator Loss: 1.372883915901184\n",
      "Generator Loss: 0.5264067053794861\n",
      "Discriminator Loss: 1.3720879554748535\n",
      "Generator Loss: 0.5258427858352661\n",
      "Discriminator Loss: 1.3713477849960327\n",
      "Generator Loss: 0.5252728462219238\n",
      "Discriminator Loss: 1.3706178665161133\n",
      "Generator Loss: 0.5247570872306824\n",
      "Discriminator Loss: 1.369861364364624\n",
      "Generator Loss: 0.5241727232933044\n",
      "Discriminator Loss: 1.3692731857299805\n",
      "Generator Loss: 0.5236259698867798\n",
      "Discriminator Loss: 1.3685237169265747\n",
      "Generator Loss: 0.5230560898780823\n",
      "Discriminator Loss: 1.367911696434021\n",
      "Generator Loss: 0.5224828720092773\n",
      "Discriminator Loss: 1.367111325263977\n",
      "Generator Loss: 0.5219086408615112\n",
      "Discriminator Loss: 1.366300106048584\n",
      "Generator Loss: 0.5213826894760132\n",
      "Discriminator Loss: 1.3655036687850952\n",
      "Generator Loss: 0.5208753347396851\n",
      "Discriminator Loss: 1.3647260665893555\n",
      "Generator Loss: 0.5204369425773621\n",
      "Discriminator Loss: 1.364043951034546\n",
      "Generator Loss: 0.5201581716537476\n",
      "Discriminator Loss: 1.3633455038070679\n",
      "Generator Loss: 0.5197358131408691\n",
      "Discriminator Loss: 1.362791657447815\n",
      "Generator Loss: 0.5190706849098206\n",
      "Discriminator Loss: 1.3621790409088135\n",
      "Generator Loss: 0.518466055393219\n",
      "Discriminator Loss: 1.3615516424179077\n",
      "Generator Loss: 0.517881453037262\n",
      "Discriminator Loss: 1.36090886592865\n",
      "Generator Loss: 0.5173081159591675\n",
      "Discriminator Loss: 1.3602406978607178\n",
      "Generator Loss: 0.5167295336723328\n",
      "Discriminator Loss: 1.3595863580703735\n",
      "Generator Loss: 0.516201913356781\n",
      "Discriminator Loss: 1.3589863777160645\n",
      "Generator Loss: 0.5156999230384827\n",
      "Discriminator Loss: 1.3583873510360718\n",
      "Generator Loss: 0.5151454210281372\n",
      "Discriminator Loss: 1.3577609062194824\n",
      "Generator Loss: 0.5145865678787231\n",
      "Discriminator Loss: 1.357115387916565\n",
      "Generator Loss: 0.5140013694763184\n",
      "Discriminator Loss: 1.3564527034759521\n",
      "Generator Loss: 0.5134532451629639\n",
      "Discriminator Loss: 1.3558707237243652\n",
      "Generator Loss: 0.5128865242004395\n",
      "Discriminator Loss: 1.3551759719848633\n",
      "Generator Loss: 0.5123341083526611\n",
      "Discriminator Loss: 1.354555606842041\n",
      "Generator Loss: 0.511804461479187\n",
      "Discriminator Loss: 1.3540016412734985\n",
      "Generator Loss: 0.511271059513092\n",
      "Discriminator Loss: 1.3534669876098633\n",
      "Generator Loss: 0.5107107162475586\n",
      "Discriminator Loss: 1.3528164625167847\n",
      "Generator Loss: 0.5101880431175232\n",
      "Discriminator Loss: 1.3521955013275146\n",
      "Generator Loss: 0.5096445679664612\n",
      "Discriminator Loss: 1.3515360355377197\n",
      "Generator Loss: 0.5090978741645813\n",
      "Discriminator Loss: 1.3509365320205688\n",
      "Generator Loss: 0.5086301565170288\n",
      "Discriminator Loss: 1.3502532243728638\n",
      "Generator Loss: 0.5081024169921875\n",
      "Discriminator Loss: 1.3498200178146362\n",
      "Generator Loss: 0.5075305700302124\n",
      "Discriminator Loss: 1.3492735624313354\n",
      "Generator Loss: 0.506988525390625\n",
      "Discriminator Loss: 1.3487253189086914\n",
      "Generator Loss: 0.506435751914978\n",
      "Discriminator Loss: 1.3481392860412598\n",
      "Generator Loss: 0.5058943629264832\n",
      "Discriminator Loss: 1.3477575778961182\n",
      "Generator Loss: 0.5054453015327454\n",
      "Discriminator Loss: 1.347326397895813\n",
      "Generator Loss: 0.504931628704071\n",
      "Discriminator Loss: 1.3469197750091553\n",
      "Generator Loss: 0.5043947100639343\n",
      "Discriminator Loss: 1.346408486366272\n",
      "Generator Loss: 0.50384521484375\n",
      "Discriminator Loss: 1.3458926677703857\n",
      "Generator Loss: 0.5033292174339294\n",
      "Discriminator Loss: 1.3454320430755615\n",
      "Generator Loss: 0.5027939677238464\n",
      "Discriminator Loss: 1.3449293375015259\n",
      "Generator Loss: 0.5022639036178589\n",
      "Discriminator Loss: 1.3443877696990967\n",
      "Generator Loss: 0.5017179250717163\n",
      "Discriminator Loss: 1.343808889389038\n",
      "Generator Loss: 0.5012422204017639\n",
      "Discriminator Loss: 1.3432117700576782\n",
      "Generator Loss: 0.5007120370864868\n",
      "Discriminator Loss: 1.3427528142929077\n",
      "Generator Loss: 0.5002236366271973\n",
      "Discriminator Loss: 1.342153549194336\n",
      "Generator Loss: 0.49976691603660583\n",
      "Discriminator Loss: 1.3417127132415771\n",
      "Generator Loss: 0.49932873249053955\n",
      "Discriminator Loss: 1.3411338329315186\n",
      "Generator Loss: 0.4988527297973633\n",
      "Discriminator Loss: 1.3406685590744019\n",
      "Generator Loss: 0.4983483850955963\n",
      "Discriminator Loss: 1.3401986360549927\n",
      "Generator Loss: 0.49782854318618774\n",
      "Discriminator Loss: 1.3397382497787476\n",
      "Generator Loss: 0.497305303812027\n",
      "Discriminator Loss: 1.3391473293304443\n",
      "Generator Loss: 0.4967966079711914\n",
      "Discriminator Loss: 1.3386114835739136\n",
      "Generator Loss: 0.49628081917762756\n",
      "Discriminator Loss: 1.3380728960037231\n",
      "Generator Loss: 0.49576708674430847\n",
      "Discriminator Loss: 1.3374652862548828\n",
      "Generator Loss: 0.49526074528694153\n",
      "Discriminator Loss: 1.336952805519104\n",
      "Generator Loss: 0.4947478473186493\n",
      "Discriminator Loss: 1.3364005088806152\n",
      "Generator Loss: 0.4942111372947693\n",
      "Discriminator Loss: 1.3358960151672363\n",
      "Generator Loss: 0.49371734261512756\n",
      "Discriminator Loss: 1.3353002071380615\n",
      "Generator Loss: 0.49320346117019653\n",
      "Discriminator Loss: 1.3348530530929565\n",
      "Generator Loss: 0.4927026033401489\n",
      "Discriminator Loss: 1.3343065977096558\n",
      "Generator Loss: 0.49224162101745605\n",
      "Discriminator Loss: 1.333858609199524\n",
      "Generator Loss: 0.49180060625076294\n",
      "Discriminator Loss: 1.3332340717315674\n",
      "Generator Loss: 0.49131500720977783\n",
      "Discriminator Loss: 1.3327940702438354\n",
      "Generator Loss: 0.4909038543701172\n",
      "Discriminator Loss: 1.3322100639343262\n",
      "Generator Loss: 0.49043703079223633\n",
      "Discriminator Loss: 1.3318425416946411\n",
      "Generator Loss: 0.48996269702911377\n",
      "Discriminator Loss: 1.3313446044921875\n",
      "Generator Loss: 0.48947373032569885\n",
      "Discriminator Loss: 1.3309741020202637\n",
      "Generator Loss: 0.4889478385448456\n",
      "Discriminator Loss: 1.3305107355117798\n",
      "Generator Loss: 0.488465815782547\n",
      "Discriminator Loss: 1.330082654953003\n",
      "Generator Loss: 0.48797160387039185\n",
      "Discriminator Loss: 1.3296185731887817\n",
      "Generator Loss: 0.48747843503952026\n",
      "Discriminator Loss: 1.3291902542114258\n",
      "Generator Loss: 0.48699405789375305\n",
      "Discriminator Loss: 1.3286356925964355\n",
      "Generator Loss: 0.48652029037475586\n",
      "Discriminator Loss: 1.3281834125518799\n",
      "Generator Loss: 0.48606231808662415\n",
      "Discriminator Loss: 1.327694296836853\n",
      "Generator Loss: 0.485577255487442\n",
      "Discriminator Loss: 1.3272801637649536\n",
      "Generator Loss: 0.4851081967353821\n",
      "Discriminator Loss: 1.3268101215362549\n",
      "Generator Loss: 0.4846249222755432\n",
      "Discriminator Loss: 1.3263765573501587\n",
      "Generator Loss: 0.48420125246047974\n",
      "Discriminator Loss: 1.3258382081985474\n",
      "Generator Loss: 0.4836864769458771\n",
      "Discriminator Loss: 1.3254400491714478\n",
      "Generator Loss: 0.48318585753440857\n",
      "Discriminator Loss: 1.3249568939208984\n",
      "Generator Loss: 0.48271334171295166\n",
      "Discriminator Loss: 1.324512004852295\n",
      "Generator Loss: 0.48221710324287415\n",
      "Discriminator Loss: 1.324109673500061\n",
      "Generator Loss: 0.48175325989723206\n",
      "Discriminator Loss: 1.3236719369888306\n",
      "Generator Loss: 0.48128339648246765\n",
      "Discriminator Loss: 1.3232150077819824\n",
      "Generator Loss: 0.48086199164390564\n",
      "Discriminator Loss: 1.3227602243423462\n",
      "Generator Loss: 0.4804483652114868\n",
      "Discriminator Loss: 1.3222395181655884\n",
      "Generator Loss: 0.48000195622444153\n",
      "Discriminator Loss: 1.321988582611084\n",
      "Generator Loss: 0.4795013964176178\n",
      "Discriminator Loss: 1.321563482284546\n",
      "Generator Loss: 0.4790113568305969\n",
      "Discriminator Loss: 1.321192979812622\n",
      "Generator Loss: 0.4785514771938324\n",
      "Discriminator Loss: 1.3207905292510986\n",
      "Generator Loss: 0.47807231545448303\n",
      "Discriminator Loss: 1.3203703165054321\n",
      "Generator Loss: 0.477618545293808\n",
      "Discriminator Loss: 1.3199706077575684\n",
      "Generator Loss: 0.47716182470321655\n",
      "Discriminator Loss: 1.319548487663269\n",
      "Generator Loss: 0.47670501470565796\n",
      "Discriminator Loss: 1.3191368579864502\n",
      "Generator Loss: 0.4762590527534485\n",
      "Discriminator Loss: 1.3187549114227295\n",
      "Generator Loss: 0.4758045971393585\n",
      "Discriminator Loss: 1.3183461427688599\n",
      "Generator Loss: 0.4753418564796448\n",
      "Discriminator Loss: 1.3179596662521362\n",
      "Generator Loss: 0.47484856843948364\n",
      "Discriminator Loss: 1.3176573514938354\n",
      "Generator Loss: 0.474365234375\n",
      "Discriminator Loss: 1.3172413110733032\n",
      "Generator Loss: 0.473875492811203\n",
      "Discriminator Loss: 1.3168482780456543\n",
      "Generator Loss: 0.47341448068618774\n",
      "Discriminator Loss: 1.3165007829666138\n",
      "Generator Loss: 0.47293543815612793\n",
      "Discriminator Loss: 1.3161870241165161\n",
      "Generator Loss: 0.47246253490448\n",
      "Discriminator Loss: 1.3157463073730469\n",
      "Generator Loss: 0.4719897508621216\n",
      "Discriminator Loss: 1.315578579902649\n",
      "Generator Loss: 0.4715578854084015\n",
      "Discriminator Loss: 1.3153290748596191\n",
      "Generator Loss: 0.4710959494113922\n",
      "Discriminator Loss: 1.3149813413619995\n",
      "Generator Loss: 0.4706210196018219\n",
      "Discriminator Loss: 1.3146445751190186\n",
      "Generator Loss: 0.4701925814151764\n",
      "Discriminator Loss: 1.3143870830535889\n",
      "Generator Loss: 0.46973302960395813\n",
      "Discriminator Loss: 1.3141708374023438\n",
      "Generator Loss: 0.4692593514919281\n",
      "Discriminator Loss: 1.3138539791107178\n",
      "Generator Loss: 0.4687691330909729\n",
      "Discriminator Loss: 1.313613772392273\n",
      "Generator Loss: 0.468313604593277\n",
      "Discriminator Loss: 1.31330406665802\n",
      "Generator Loss: 0.46781617403030396\n",
      "Discriminator Loss: 1.3132236003875732\n",
      "Generator Loss: 0.46733614802360535\n",
      "Discriminator Loss: 1.313006043434143\n",
      "Generator Loss: 0.4668300747871399\n",
      "Discriminator Loss: 1.312705159187317\n",
      "Generator Loss: 0.46638259291648865\n",
      "Discriminator Loss: 1.3126909732818604\n",
      "Generator Loss: 0.4658907651901245\n",
      "Discriminator Loss: 1.312471628189087\n",
      "Generator Loss: 0.46537816524505615\n",
      "Discriminator Loss: 1.3124319314956665\n",
      "Generator Loss: 0.4649513065814972\n",
      "Discriminator Loss: 1.3124024868011475\n",
      "Generator Loss: 0.4644494950771332\n",
      "Discriminator Loss: 1.3122706413269043\n",
      "Generator Loss: 0.46401435136795044\n",
      "Discriminator Loss: 1.3121634721755981\n",
      "Generator Loss: 0.463538259267807\n",
      "Discriminator Loss: 1.3121188879013062\n",
      "Generator Loss: 0.46304890513420105\n",
      "Discriminator Loss: 1.3119416236877441\n",
      "Generator Loss: 0.462539941072464\n",
      "Discriminator Loss: 1.3117451667785645\n",
      "Generator Loss: 0.4620664119720459\n",
      "Discriminator Loss: 1.311643123626709\n",
      "Generator Loss: 0.46158266067504883\n",
      "Discriminator Loss: 1.311498999595642\n",
      "Generator Loss: 0.46111664175987244\n",
      "Discriminator Loss: 1.3113253116607666\n",
      "Generator Loss: 0.46067559719085693\n",
      "Discriminator Loss: 1.3111612796783447\n",
      "Generator Loss: 0.46021801233291626\n",
      "Discriminator Loss: 1.3112140893936157\n",
      "Generator Loss: 0.4597546458244324\n",
      "Discriminator Loss: 1.3110731840133667\n",
      "Generator Loss: 0.45926323533058167\n",
      "Discriminator Loss: 1.3108928203582764\n",
      "Generator Loss: 0.4588300287723541\n",
      "Discriminator Loss: 1.310859203338623\n",
      "Generator Loss: 0.45837679505348206\n",
      "Discriminator Loss: 1.310733437538147\n",
      "Generator Loss: 0.45793306827545166\n",
      "Discriminator Loss: 1.3106105327606201\n",
      "Generator Loss: 0.4575132727622986\n",
      "Discriminator Loss: 1.3105171918869019\n",
      "Generator Loss: 0.4570753276348114\n",
      "Discriminator Loss: 1.3105498552322388\n",
      "Generator Loss: 0.4566361904144287\n",
      "Discriminator Loss: 1.310426115989685\n",
      "Generator Loss: 0.45617833733558655\n",
      "Discriminator Loss: 1.3102229833602905\n",
      "Generator Loss: 0.45574912428855896\n",
      "Discriminator Loss: 1.310075283050537\n",
      "Generator Loss: 0.4552898705005646\n",
      "Discriminator Loss: 1.3100521564483643\n",
      "Generator Loss: 0.4548397958278656\n",
      "Discriminator Loss: 1.3099559545516968\n",
      "Generator Loss: 0.45440855622291565\n",
      "About to save image...\n",
      "Shape of generated_images: (2, 28, 28, 1)\n",
      "Discriminator Loss: 1.309924602508545\n",
      "Generator Loss: 0.4539572596549988\n",
      "Discriminator Loss: 1.309815526008606\n",
      "Generator Loss: 0.45352599024772644\n",
      "Discriminator Loss: 1.309794306755066\n",
      "Generator Loss: 0.4530720114707947\n",
      "Discriminator Loss: 1.3096832036972046\n",
      "Generator Loss: 0.4526240825653076\n",
      "Discriminator Loss: 1.3096367120742798\n",
      "Generator Loss: 0.45217305421829224\n",
      "Discriminator Loss: 1.3095581531524658\n",
      "Generator Loss: 0.45174410939216614\n",
      "Discriminator Loss: 1.3095314502716064\n",
      "Generator Loss: 0.4513115882873535\n",
      "Discriminator Loss: 1.3094350099563599\n",
      "Generator Loss: 0.4508976340293884\n",
      "Discriminator Loss: 1.3092896938323975\n",
      "Generator Loss: 0.45043808221817017\n",
      "Discriminator Loss: 1.3091883659362793\n",
      "Generator Loss: 0.44997552037239075\n",
      "Discriminator Loss: 1.309086799621582\n",
      "Generator Loss: 0.44954460859298706\n",
      "Discriminator Loss: 1.3089481592178345\n",
      "Generator Loss: 0.4491216838359833\n",
      "Discriminator Loss: 1.3088017702102661\n",
      "Generator Loss: 0.4486820101737976\n",
      "Discriminator Loss: 1.3086789846420288\n",
      "Generator Loss: 0.44825711846351624\n",
      "Discriminator Loss: 1.3086196184158325\n",
      "Generator Loss: 0.44780057668685913\n",
      "Discriminator Loss: 1.3085911273956299\n",
      "Generator Loss: 0.44737809896469116\n",
      "Discriminator Loss: 1.3084473609924316\n",
      "Generator Loss: 0.4469267427921295\n",
      "Discriminator Loss: 1.3083391189575195\n",
      "Generator Loss: 0.44647446274757385\n",
      "Discriminator Loss: 1.3082298040390015\n",
      "Generator Loss: 0.4460148215293884\n",
      "Discriminator Loss: 1.3081958293914795\n",
      "Generator Loss: 0.4455716609954834\n",
      "Discriminator Loss: 1.3082109689712524\n",
      "Generator Loss: 0.4451954960823059\n",
      "Discriminator Loss: 1.3081148862838745\n",
      "Generator Loss: 0.44479766488075256\n",
      "Discriminator Loss: 1.3080804347991943\n",
      "Generator Loss: 0.4443952739238739\n",
      "Discriminator Loss: 1.308080792427063\n",
      "Generator Loss: 0.44394969940185547\n",
      "Discriminator Loss: 1.308069109916687\n",
      "Generator Loss: 0.4434932470321655\n",
      "Discriminator Loss: 1.3080543279647827\n",
      "Generator Loss: 0.44307681918144226\n",
      "Discriminator Loss: 1.308106541633606\n",
      "Generator Loss: 0.44267380237579346\n",
      "Discriminator Loss: 1.3080817461013794\n",
      "Generator Loss: 0.44226503372192383\n",
      "Discriminator Loss: 1.308109998703003\n",
      "Generator Loss: 0.4418486952781677\n",
      "Discriminator Loss: 1.3081058263778687\n",
      "Generator Loss: 0.4414423704147339\n",
      "Discriminator Loss: 1.3081538677215576\n",
      "Generator Loss: 0.4410075843334198\n",
      "Discriminator Loss: 1.3081351518630981\n",
      "Generator Loss: 0.44060003757476807\n",
      "Discriminator Loss: 1.3080263137817383\n",
      "Generator Loss: 0.440210223197937\n",
      "Discriminator Loss: 1.3079802989959717\n",
      "Generator Loss: 0.4398331642150879\n",
      "Discriminator Loss: 1.3079649209976196\n",
      "Generator Loss: 0.4394172430038452\n",
      "Discriminator Loss: 1.3078925609588623\n",
      "Generator Loss: 0.4390143156051636\n",
      "Discriminator Loss: 1.3080037832260132\n",
      "Generator Loss: 0.4385777413845062\n",
      "Discriminator Loss: 1.3080238103866577\n",
      "Generator Loss: 0.4381837546825409\n",
      "Discriminator Loss: 1.3081127405166626\n",
      "Generator Loss: 0.4377909302711487\n",
      "Discriminator Loss: 1.3080623149871826\n",
      "Generator Loss: 0.43735992908477783\n",
      "Discriminator Loss: 1.308010220527649\n",
      "Generator Loss: 0.43696504831314087\n",
      "Discriminator Loss: 1.3080363273620605\n",
      "Generator Loss: 0.43655624985694885\n",
      "Discriminator Loss: 1.3080272674560547\n",
      "Generator Loss: 0.43617403507232666\n",
      "Discriminator Loss: 1.3079973459243774\n",
      "Generator Loss: 0.4357792139053345\n",
      "Discriminator Loss: 1.3080607652664185\n",
      "Generator Loss: 0.43539318442344666\n",
      "Discriminator Loss: 1.3081023693084717\n",
      "Generator Loss: 0.4349961280822754\n",
      "Discriminator Loss: 1.3081077337265015\n",
      "Generator Loss: 0.43457844853401184\n",
      "Discriminator Loss: 1.3081504106521606\n",
      "Generator Loss: 0.4341728985309601\n",
      "Discriminator Loss: 1.3081783056259155\n",
      "Generator Loss: 0.43378010392189026\n",
      "Discriminator Loss: 1.308215618133545\n",
      "Generator Loss: 0.4333726763725281\n",
      "Discriminator Loss: 1.3083646297454834\n",
      "Generator Loss: 0.43297508358955383\n",
      "Discriminator Loss: 1.3083726167678833\n",
      "Generator Loss: 0.4325941503047943\n",
      "Discriminator Loss: 1.3084088563919067\n",
      "Generator Loss: 0.43222108483314514\n",
      "Discriminator Loss: 1.3085325956344604\n",
      "Generator Loss: 0.4318341612815857\n",
      "Discriminator Loss: 1.3086649179458618\n",
      "Generator Loss: 0.4314349591732025\n",
      "Discriminator Loss: 1.3087605237960815\n",
      "Generator Loss: 0.4310566484928131\n",
      "Discriminator Loss: 1.3089157342910767\n",
      "Generator Loss: 0.4307146668434143\n",
      "Discriminator Loss: 1.308923363685608\n",
      "Generator Loss: 0.4303101599216461\n",
      "Discriminator Loss: 1.3090224266052246\n",
      "Generator Loss: 0.4299240708351135\n",
      "Discriminator Loss: 1.3090895414352417\n",
      "Generator Loss: 0.429547518491745\n",
      "Discriminator Loss: 1.3092986345291138\n",
      "Generator Loss: 0.4291784465312958\n",
      "Discriminator Loss: 1.3094556331634521\n",
      "Generator Loss: 0.42878517508506775\n",
      "Discriminator Loss: 1.309602975845337\n",
      "Generator Loss: 0.42841392755508423\n",
      "Discriminator Loss: 1.3097925186157227\n",
      "Generator Loss: 0.4280405044555664\n",
      "Discriminator Loss: 1.3099030256271362\n",
      "Generator Loss: 0.4276745021343231\n",
      "Discriminator Loss: 1.3100868463516235\n",
      "Generator Loss: 0.42728757858276367\n",
      "Discriminator Loss: 1.3102484941482544\n",
      "Generator Loss: 0.4269348382949829\n",
      "Discriminator Loss: 1.3104338645935059\n",
      "Generator Loss: 0.4265522360801697\n",
      "Discriminator Loss: 1.310633659362793\n",
      "Generator Loss: 0.4261815547943115\n",
      "Discriminator Loss: 1.310713529586792\n",
      "Generator Loss: 0.42580053210258484\n",
      "Discriminator Loss: 1.310872197151184\n",
      "Generator Loss: 0.4254135489463806\n",
      "Discriminator Loss: 1.3109818696975708\n",
      "Generator Loss: 0.4250427484512329\n",
      "Discriminator Loss: 1.3111599683761597\n",
      "Generator Loss: 0.4246981739997864\n",
      "Discriminator Loss: 1.3113064765930176\n",
      "Generator Loss: 0.4243050217628479\n",
      "Discriminator Loss: 1.3113713264465332\n",
      "Generator Loss: 0.4239298105239868\n",
      "Discriminator Loss: 1.3114994764328003\n",
      "Generator Loss: 0.423574298620224\n",
      "Discriminator Loss: 1.3116850852966309\n",
      "Generator Loss: 0.42322131991386414\n",
      "Discriminator Loss: 1.3118650913238525\n",
      "Generator Loss: 0.4228581190109253\n",
      "Discriminator Loss: 1.31199049949646\n",
      "Generator Loss: 0.42249491810798645\n",
      "Discriminator Loss: 1.3121663331985474\n",
      "Generator Loss: 0.4221487045288086\n",
      "Discriminator Loss: 1.31229567527771\n",
      "Generator Loss: 0.4218309223651886\n",
      "Discriminator Loss: 1.3126060962677002\n",
      "Generator Loss: 0.42148441076278687\n",
      "Discriminator Loss: 1.312717318534851\n",
      "Generator Loss: 0.42111822962760925\n",
      "Discriminator Loss: 1.3128547668457031\n",
      "Generator Loss: 0.4207497835159302\n",
      "Discriminator Loss: 1.3130437135696411\n",
      "Generator Loss: 0.42039430141448975\n",
      "Discriminator Loss: 1.3132463693618774\n",
      "Generator Loss: 0.4200402498245239\n",
      "Discriminator Loss: 1.3133975267410278\n",
      "Generator Loss: 0.41966474056243896\n",
      "Discriminator Loss: 1.3135331869125366\n",
      "Generator Loss: 0.4193311631679535\n",
      "Discriminator Loss: 1.3137117624282837\n",
      "Generator Loss: 0.419010192155838\n",
      "Discriminator Loss: 1.3138941526412964\n",
      "Generator Loss: 0.41862624883651733\n",
      "Discriminator Loss: 1.3140308856964111\n",
      "Generator Loss: 0.41829150915145874\n",
      "Discriminator Loss: 1.3142026662826538\n",
      "Generator Loss: 0.4179753065109253\n",
      "Discriminator Loss: 1.3144060373306274\n",
      "Generator Loss: 0.4176422655582428\n",
      "Discriminator Loss: 1.3145984411239624\n",
      "Generator Loss: 0.4173334836959839\n",
      "Discriminator Loss: 1.3147307634353638\n",
      "Generator Loss: 0.41697877645492554\n",
      "Discriminator Loss: 1.3149023056030273\n",
      "Generator Loss: 0.4166153371334076\n",
      "Discriminator Loss: 1.315011739730835\n",
      "Generator Loss: 0.4162585139274597\n",
      "Discriminator Loss: 1.315087914466858\n",
      "Generator Loss: 0.41591131687164307\n",
      "Discriminator Loss: 1.3150795698165894\n",
      "Generator Loss: 0.415565550327301\n",
      "Discriminator Loss: 1.3151370286941528\n",
      "Generator Loss: 0.41523969173431396\n",
      "Discriminator Loss: 1.3152966499328613\n",
      "Generator Loss: 0.4148976802825928\n",
      "Discriminator Loss: 1.315482258796692\n",
      "Generator Loss: 0.4145616590976715\n",
      "Discriminator Loss: 1.3155694007873535\n",
      "Generator Loss: 0.4142356514930725\n",
      "Discriminator Loss: 1.3157448768615723\n",
      "Generator Loss: 0.41389232873916626\n",
      "Discriminator Loss: 1.3157472610473633\n",
      "Generator Loss: 0.41357919573783875\n",
      "Discriminator Loss: 1.3159152269363403\n",
      "Generator Loss: 0.41324159502983093\n",
      "Discriminator Loss: 1.3159540891647339\n",
      "Generator Loss: 0.4129149317741394\n",
      "Discriminator Loss: 1.3161218166351318\n",
      "Generator Loss: 0.41257724165916443\n",
      "Discriminator Loss: 1.3162471055984497\n",
      "Generator Loss: 0.41228240728378296\n",
      "Discriminator Loss: 1.3163466453552246\n",
      "Generator Loss: 0.4119604229927063\n",
      "Discriminator Loss: 1.316555380821228\n",
      "Generator Loss: 0.4116232991218567\n",
      "Discriminator Loss: 1.3166298866271973\n",
      "Generator Loss: 0.4113038182258606\n",
      "Discriminator Loss: 1.31681489944458\n",
      "Generator Loss: 0.41098448634147644\n",
      "Discriminator Loss: 1.3169410228729248\n",
      "Generator Loss: 0.4106769263744354\n",
      "Discriminator Loss: 1.317084789276123\n",
      "Generator Loss: 0.4103424847126007\n",
      "Discriminator Loss: 1.317223310470581\n",
      "Generator Loss: 0.4100213944911957\n",
      "Discriminator Loss: 1.317272424697876\n",
      "Generator Loss: 0.40967467427253723\n",
      "Discriminator Loss: 1.3173527717590332\n",
      "Generator Loss: 0.4093523621559143\n",
      "Discriminator Loss: 1.3173381090164185\n",
      "Generator Loss: 0.4090433418750763\n",
      "Discriminator Loss: 1.3173853158950806\n",
      "Generator Loss: 0.4086872935295105\n",
      "Discriminator Loss: 1.31755793094635\n",
      "Generator Loss: 0.40838924050331116\n",
      "Discriminator Loss: 1.3176875114440918\n",
      "Generator Loss: 0.4080657362937927\n",
      "Discriminator Loss: 1.3178385496139526\n",
      "Generator Loss: 0.4077511727809906\n",
      "Discriminator Loss: 1.3180111646652222\n",
      "Generator Loss: 0.4074139893054962\n",
      "Discriminator Loss: 1.318153738975525\n",
      "Generator Loss: 0.4071139097213745\n",
      "Discriminator Loss: 1.3183513879776\n",
      "Generator Loss: 0.4067609906196594\n",
      "Discriminator Loss: 1.3184969425201416\n",
      "Generator Loss: 0.4064537584781647\n",
      "Discriminator Loss: 1.3185685873031616\n",
      "Generator Loss: 0.4061437249183655\n",
      "Discriminator Loss: 1.3187400102615356\n",
      "Generator Loss: 0.4058181643486023\n",
      "Discriminator Loss: 1.318933367729187\n",
      "Generator Loss: 0.4054942727088928\n",
      "Discriminator Loss: 1.319137692451477\n",
      "Generator Loss: 0.4051781892776489\n",
      "Discriminator Loss: 1.3191877603530884\n",
      "Generator Loss: 0.40485450625419617\n",
      "Discriminator Loss: 1.3192857503890991\n",
      "Generator Loss: 0.4045560657978058\n",
      "Discriminator Loss: 1.3194091320037842\n",
      "Generator Loss: 0.404209703207016\n",
      "Discriminator Loss: 1.319441795349121\n",
      "Generator Loss: 0.4038808047771454\n",
      "Discriminator Loss: 1.3195257186889648\n",
      "Generator Loss: 0.40356993675231934\n",
      "Discriminator Loss: 1.319650650024414\n",
      "Generator Loss: 0.40325015783309937\n",
      "Discriminator Loss: 1.3198140859603882\n",
      "Generator Loss: 0.4029427468776703\n",
      "Discriminator Loss: 1.3200074434280396\n",
      "Generator Loss: 0.40265652537345886\n",
      "Discriminator Loss: 1.3202791213989258\n",
      "Generator Loss: 0.402330219745636\n",
      "Discriminator Loss: 1.3204445838928223\n",
      "Generator Loss: 0.40201398730278015\n",
      "Discriminator Loss: 1.320615530014038\n",
      "Generator Loss: 0.40171018242836\n",
      "Discriminator Loss: 1.3207587003707886\n",
      "Generator Loss: 0.4013901650905609\n",
      "Discriminator Loss: 1.3208568096160889\n",
      "Generator Loss: 0.4010781943798065\n",
      "Discriminator Loss: 1.3210564851760864\n",
      "Generator Loss: 0.40077176690101624\n",
      "Discriminator Loss: 1.321222186088562\n",
      "Generator Loss: 0.40047213435173035\n",
      "Discriminator Loss: 1.3214280605316162\n",
      "Generator Loss: 0.40015867352485657\n",
      "Discriminator Loss: 1.3216426372528076\n",
      "Generator Loss: 0.39987438917160034\n",
      "Discriminator Loss: 1.3218008279800415\n",
      "Generator Loss: 0.39956793189048767\n",
      "Discriminator Loss: 1.3220069408416748\n",
      "Generator Loss: 0.39931267499923706\n",
      "Discriminator Loss: 1.322217345237732\n",
      "Generator Loss: 0.39899083971977234\n",
      "Discriminator Loss: 1.3223832845687866\n",
      "Generator Loss: 0.3986877501010895\n",
      "Discriminator Loss: 1.3226451873779297\n",
      "Generator Loss: 0.39840760827064514\n",
      "Discriminator Loss: 1.3228847980499268\n",
      "Generator Loss: 0.3981302082538605\n",
      "Discriminator Loss: 1.3230714797973633\n",
      "Generator Loss: 0.397827684879303\n",
      "Discriminator Loss: 1.3231786489486694\n",
      "Generator Loss: 0.39750683307647705\n",
      "Discriminator Loss: 1.3232991695404053\n",
      "Generator Loss: 0.39719679951667786\n",
      "Discriminator Loss: 1.3234728574752808\n",
      "Generator Loss: 0.39690449833869934\n",
      "Discriminator Loss: 1.3237051963806152\n",
      "Generator Loss: 0.3966139853000641\n",
      "Discriminator Loss: 1.3238792419433594\n",
      "Generator Loss: 0.3963143825531006\n",
      "Discriminator Loss: 1.3241138458251953\n",
      "Generator Loss: 0.39601871371269226\n",
      "Discriminator Loss: 1.3243272304534912\n",
      "Generator Loss: 0.3957556188106537\n",
      "Discriminator Loss: 1.3245575428009033\n",
      "Generator Loss: 0.39546746015548706\n",
      "Discriminator Loss: 1.324703574180603\n",
      "Generator Loss: 0.39517512917518616\n",
      "Discriminator Loss: 1.32478666305542\n",
      "Generator Loss: 0.3948903977870941\n",
      "Discriminator Loss: 1.3250163793563843\n",
      "Generator Loss: 0.3945864737033844\n",
      "Discriminator Loss: 1.325234293937683\n",
      "Generator Loss: 0.39432400465011597\n",
      "Discriminator Loss: 1.3254470825195312\n",
      "Generator Loss: 0.39403456449508667\n",
      "Discriminator Loss: 1.3255438804626465\n",
      "Generator Loss: 0.3937382996082306\n",
      "Discriminator Loss: 1.325688362121582\n",
      "Generator Loss: 0.39345139265060425\n",
      "Discriminator Loss: 1.325893521308899\n",
      "Generator Loss: 0.393146276473999\n",
      "Discriminator Loss: 1.3260332345962524\n",
      "Generator Loss: 0.3928714096546173\n",
      "Discriminator Loss: 1.3262759447097778\n",
      "Generator Loss: 0.39260226488113403\n",
      "Discriminator Loss: 1.3264669179916382\n",
      "Generator Loss: 0.3923119008541107\n",
      "Discriminator Loss: 1.326648235321045\n",
      "Generator Loss: 0.3920280635356903\n",
      "Discriminator Loss: 1.3268636465072632\n",
      "Generator Loss: 0.39174941182136536\n",
      "Discriminator Loss: 1.327219009399414\n",
      "Generator Loss: 0.3914680480957031\n",
      "Discriminator Loss: 1.3274121284484863\n",
      "Generator Loss: 0.39115941524505615\n",
      "Discriminator Loss: 1.327546238899231\n",
      "Generator Loss: 0.3908827006816864\n",
      "Discriminator Loss: 1.327595829963684\n",
      "Generator Loss: 0.39060500264167786\n",
      "Discriminator Loss: 1.3277889490127563\n",
      "Generator Loss: 0.3903270363807678\n",
      "Discriminator Loss: 1.3279649019241333\n",
      "Generator Loss: 0.39003923535346985\n",
      "Discriminator Loss: 1.3280537128448486\n",
      "Generator Loss: 0.38972797989845276\n",
      "Discriminator Loss: 1.3280880451202393\n",
      "Generator Loss: 0.3894489109516144\n",
      "Discriminator Loss: 1.32821524143219\n",
      "Generator Loss: 0.3891713619232178\n",
      "Discriminator Loss: 1.3284683227539062\n",
      "Generator Loss: 0.38888019323349\n",
      "Discriminator Loss: 1.328673243522644\n",
      "Generator Loss: 0.38864925503730774\n",
      "About to save image...\n",
      "Shape of generated_images: (2, 28, 28, 1)\n",
      "Discriminator Loss: 1.3288341760635376\n",
      "Generator Loss: 0.3883410692214966\n",
      "Discriminator Loss: 1.3290714025497437\n",
      "Generator Loss: 0.388097882270813\n",
      "Discriminator Loss: 1.3291761875152588\n",
      "Generator Loss: 0.38779979944229126\n",
      "Discriminator Loss: 1.329317331314087\n",
      "Generator Loss: 0.38752835988998413\n",
      "Discriminator Loss: 1.3294851779937744\n",
      "Generator Loss: 0.38724416494369507\n",
      "Discriminator Loss: 1.3296210765838623\n",
      "Generator Loss: 0.38698503375053406\n",
      "Discriminator Loss: 1.3298155069351196\n",
      "Generator Loss: 0.38671839237213135\n",
      "Discriminator Loss: 1.329970359802246\n",
      "Generator Loss: 0.38645032048225403\n",
      "Discriminator Loss: 1.330194115638733\n",
      "Generator Loss: 0.3861725926399231\n",
      "Discriminator Loss: 1.330349326133728\n",
      "Generator Loss: 0.38590967655181885\n",
      "Discriminator Loss: 1.330588459968567\n",
      "Generator Loss: 0.385618656873703\n",
      "Discriminator Loss: 1.3307921886444092\n",
      "Generator Loss: 0.38536369800567627\n",
      "Discriminator Loss: 1.3309379816055298\n",
      "Generator Loss: 0.38510817289352417\n",
      "Discriminator Loss: 1.3310736417770386\n",
      "Generator Loss: 0.3848475515842438\n",
      "Discriminator Loss: 1.3311805725097656\n",
      "Generator Loss: 0.3845827877521515\n",
      "Discriminator Loss: 1.3314239978790283\n",
      "Generator Loss: 0.3843168616294861\n",
      "Discriminator Loss: 1.331702709197998\n",
      "Generator Loss: 0.38405686616897583\n",
      "Discriminator Loss: 1.331908106803894\n",
      "Generator Loss: 0.3837912976741791\n",
      "Discriminator Loss: 1.3321377038955688\n",
      "Generator Loss: 0.38351649045944214\n",
      "Discriminator Loss: 1.3323326110839844\n",
      "Generator Loss: 0.3832755982875824\n",
      "Discriminator Loss: 1.3324998617172241\n",
      "Generator Loss: 0.38302019238471985\n",
      "Discriminator Loss: 1.3326694965362549\n",
      "Generator Loss: 0.3827384412288666\n",
      "Discriminator Loss: 1.3328685760498047\n",
      "Generator Loss: 0.38248905539512634\n",
      "Discriminator Loss: 1.3330762386322021\n",
      "Generator Loss: 0.3822135031223297\n",
      "Discriminator Loss: 1.3332489728927612\n",
      "Generator Loss: 0.3819660544395447\n",
      "Discriminator Loss: 1.3334085941314697\n",
      "Generator Loss: 0.3817012906074524\n",
      "Discriminator Loss: 1.333524227142334\n",
      "Generator Loss: 0.381435364484787\n",
      "Discriminator Loss: 1.333754539489746\n",
      "Generator Loss: 0.3811771869659424\n",
      "Discriminator Loss: 1.3339067697525024\n",
      "Generator Loss: 0.3809114992618561\n",
      "Discriminator Loss: 1.3340463638305664\n",
      "Generator Loss: 0.38064906001091003\n",
      "Discriminator Loss: 1.3342255353927612\n",
      "Generator Loss: 0.3803728520870209\n",
      "Discriminator Loss: 1.3343299627304077\n",
      "Generator Loss: 0.38012659549713135\n",
      "Discriminator Loss: 1.3344855308532715\n",
      "Generator Loss: 0.3798872232437134\n",
      "Discriminator Loss: 1.3347587585449219\n",
      "Generator Loss: 0.37962159514427185\n",
      "Discriminator Loss: 1.33489990234375\n",
      "Generator Loss: 0.3793642818927765\n",
      "Discriminator Loss: 1.3350476026535034\n",
      "Generator Loss: 0.3791322708129883\n",
      "Discriminator Loss: 1.3352391719818115\n",
      "Generator Loss: 0.3788840174674988\n",
      "Discriminator Loss: 1.3353644609451294\n",
      "Generator Loss: 0.3785976469516754\n",
      "Discriminator Loss: 1.3355170488357544\n",
      "Generator Loss: 0.3783365488052368\n",
      "Discriminator Loss: 1.3356373310089111\n",
      "Generator Loss: 0.37808144092559814\n",
      "Discriminator Loss: 1.335790991783142\n",
      "Generator Loss: 0.37784135341644287\n",
      "Discriminator Loss: 1.3360477685928345\n",
      "Generator Loss: 0.377608984708786\n",
      "Discriminator Loss: 1.336187481880188\n",
      "Generator Loss: 0.3773435652256012\n",
      "Discriminator Loss: 1.336341142654419\n",
      "Generator Loss: 0.37710392475128174\n",
      "Discriminator Loss: 1.336506724357605\n",
      "Generator Loss: 0.37684738636016846\n",
      "Discriminator Loss: 1.3366644382476807\n",
      "Generator Loss: 0.3766081929206848\n",
      "Discriminator Loss: 1.336866855621338\n",
      "Generator Loss: 0.3763439953327179\n",
      "Discriminator Loss: 1.3370301723480225\n",
      "Generator Loss: 0.37611642479896545\n",
      "Discriminator Loss: 1.3371816873550415\n",
      "Generator Loss: 0.3758681118488312\n",
      "Discriminator Loss: 1.3372727632522583\n",
      "Generator Loss: 0.37562867999076843\n",
      "Discriminator Loss: 1.3374203443527222\n",
      "Generator Loss: 0.37537944316864014\n",
      "Discriminator Loss: 1.3375080823898315\n",
      "Generator Loss: 0.37512120604515076\n",
      "Discriminator Loss: 1.3376106023788452\n",
      "Generator Loss: 0.3748597204685211\n",
      "Discriminator Loss: 1.337785243988037\n",
      "Generator Loss: 0.3746223747730255\n",
      "Discriminator Loss: 1.337868094444275\n",
      "Generator Loss: 0.37437957525253296\n",
      "Discriminator Loss: 1.3380227088928223\n",
      "Generator Loss: 0.3741239011287689\n",
      "Discriminator Loss: 1.338289499282837\n",
      "Generator Loss: 0.3738638162612915\n",
      "Discriminator Loss: 1.3384525775909424\n",
      "Generator Loss: 0.3736158013343811\n",
      "Discriminator Loss: 1.3385742902755737\n",
      "Generator Loss: 0.37336182594299316\n",
      "Discriminator Loss: 1.3387422561645508\n",
      "Generator Loss: 0.37311530113220215\n",
      "Discriminator Loss: 1.33888840675354\n",
      "Generator Loss: 0.372869610786438\n",
      "Discriminator Loss: 1.3390579223632812\n",
      "Generator Loss: 0.37261962890625\n",
      "Discriminator Loss: 1.3391683101654053\n",
      "Generator Loss: 0.372367262840271\n",
      "Discriminator Loss: 1.3392887115478516\n",
      "Generator Loss: 0.37212061882019043\n",
      "Discriminator Loss: 1.3393718004226685\n",
      "Generator Loss: 0.37186405062675476\n",
      "Discriminator Loss: 1.3395674228668213\n",
      "Generator Loss: 0.37163451313972473\n",
      "Discriminator Loss: 1.3397135734558105\n",
      "Generator Loss: 0.3713969588279724\n",
      "Discriminator Loss: 1.3398663997650146\n",
      "Generator Loss: 0.3711499273777008\n",
      "Discriminator Loss: 1.3400182723999023\n",
      "Generator Loss: 0.37088873982429504\n",
      "Discriminator Loss: 1.3401652574539185\n",
      "Generator Loss: 0.3706592917442322\n",
      "Discriminator Loss: 1.34031081199646\n",
      "Generator Loss: 0.3704110085964203\n",
      "Discriminator Loss: 1.3404701948165894\n",
      "Generator Loss: 0.3701644241809845\n",
      "Discriminator Loss: 1.3405799865722656\n",
      "Generator Loss: 0.3699221909046173\n",
      "Discriminator Loss: 1.3406965732574463\n",
      "Generator Loss: 0.3696857988834381\n",
      "Discriminator Loss: 1.3408865928649902\n",
      "Generator Loss: 0.369437575340271\n",
      "Discriminator Loss: 1.3409922122955322\n",
      "Generator Loss: 0.36921846866607666\n",
      "Discriminator Loss: 1.341207504272461\n",
      "Generator Loss: 0.3689756989479065\n",
      "Discriminator Loss: 1.3413058519363403\n",
      "Generator Loss: 0.368743896484375\n",
      "Discriminator Loss: 1.341424584388733\n",
      "Generator Loss: 0.36848992109298706\n",
      "Discriminator Loss: 1.3415449857711792\n",
      "Generator Loss: 0.36826062202453613\n",
      "Discriminator Loss: 1.3417038917541504\n",
      "Generator Loss: 0.36802032589912415\n",
      "Discriminator Loss: 1.3417803049087524\n",
      "Generator Loss: 0.3677786886692047\n",
      "Discriminator Loss: 1.3418766260147095\n",
      "Generator Loss: 0.36754748225212097\n",
      "Discriminator Loss: 1.3420127630233765\n",
      "Generator Loss: 0.3672868311405182\n",
      "Discriminator Loss: 1.3420727252960205\n",
      "Generator Loss: 0.36705338954925537\n",
      "Discriminator Loss: 1.342146873474121\n",
      "Generator Loss: 0.36682772636413574\n",
      "Discriminator Loss: 1.3423352241516113\n",
      "Generator Loss: 0.3665851056575775\n",
      "Discriminator Loss: 1.3425397872924805\n",
      "Generator Loss: 0.36632663011550903\n",
      "Discriminator Loss: 1.3426711559295654\n",
      "Generator Loss: 0.3661120533943176\n",
      "Discriminator Loss: 1.3428407907485962\n",
      "Generator Loss: 0.36589139699935913\n",
      "Discriminator Loss: 1.3429744243621826\n",
      "Generator Loss: 0.36566269397735596\n",
      "Discriminator Loss: 1.343085765838623\n",
      "Generator Loss: 0.36542487144470215\n",
      "Discriminator Loss: 1.3431906700134277\n",
      "Generator Loss: 0.3651862144470215\n",
      "Discriminator Loss: 1.3432869911193848\n",
      "Generator Loss: 0.36497047543525696\n",
      "Discriminator Loss: 1.3434460163116455\n",
      "Generator Loss: 0.36474210023880005\n",
      "Discriminator Loss: 1.3435571193695068\n",
      "Generator Loss: 0.36450493335723877\n",
      "Discriminator Loss: 1.3436897993087769\n",
      "Generator Loss: 0.364274263381958\n",
      "Discriminator Loss: 1.3438143730163574\n",
      "Generator Loss: 0.36405205726623535\n",
      "Discriminator Loss: 1.3439502716064453\n",
      "Generator Loss: 0.36381396651268005\n",
      "Discriminator Loss: 1.3440015316009521\n",
      "Generator Loss: 0.3635783791542053\n",
      "Discriminator Loss: 1.3441097736358643\n",
      "Generator Loss: 0.3633432388305664\n",
      "Discriminator Loss: 1.3441613912582397\n",
      "Generator Loss: 0.363107293844223\n",
      "Discriminator Loss: 1.3442476987838745\n",
      "Generator Loss: 0.3628827929496765\n",
      "Discriminator Loss: 1.3443490266799927\n",
      "Generator Loss: 0.3626357316970825\n",
      "Discriminator Loss: 1.3444339036941528\n",
      "Generator Loss: 0.3624027967453003\n",
      "Discriminator Loss: 1.344520926475525\n",
      "Generator Loss: 0.36217620968818665\n",
      "Discriminator Loss: 1.3445861339569092\n",
      "Generator Loss: 0.3619441092014313\n",
      "Discriminator Loss: 1.3446590900421143\n",
      "Generator Loss: 0.3617173433303833\n",
      "Discriminator Loss: 1.344794511795044\n",
      "Generator Loss: 0.36148911714553833\n",
      "Discriminator Loss: 1.3448561429977417\n",
      "Generator Loss: 0.3612578511238098\n",
      "Discriminator Loss: 1.3449342250823975\n",
      "Generator Loss: 0.361063152551651\n",
      "Discriminator Loss: 1.345067024230957\n",
      "Generator Loss: 0.36084920167922974\n",
      "Discriminator Loss: 1.3452292680740356\n",
      "Generator Loss: 0.36060383915901184\n",
      "Discriminator Loss: 1.3453567028045654\n",
      "Generator Loss: 0.3603758215904236\n",
      "Discriminator Loss: 1.345439076423645\n",
      "Generator Loss: 0.36015555262565613\n",
      "Discriminator Loss: 1.3455076217651367\n",
      "Generator Loss: 0.3599250912666321\n",
      "Discriminator Loss: 1.3456507921218872\n",
      "Generator Loss: 0.35973143577575684\n",
      "Discriminator Loss: 1.3457880020141602\n",
      "Generator Loss: 0.3595012128353119\n",
      "Discriminator Loss: 1.345838189125061\n",
      "Generator Loss: 0.35927993059158325\n",
      "Discriminator Loss: 1.345921277999878\n",
      "Generator Loss: 0.3590450882911682\n",
      "Discriminator Loss: 1.3459845781326294\n",
      "Generator Loss: 0.35882335901260376\n",
      "Discriminator Loss: 1.346144199371338\n",
      "Generator Loss: 0.3586007356643677\n",
      "Discriminator Loss: 1.346210241317749\n",
      "Generator Loss: 0.3583749532699585\n",
      "Discriminator Loss: 1.3462483882904053\n",
      "Generator Loss: 0.35815519094467163\n",
      "Discriminator Loss: 1.3463879823684692\n",
      "Generator Loss: 0.3579253554344177\n",
      "Discriminator Loss: 1.346516728401184\n",
      "Generator Loss: 0.35771316289901733\n",
      "Discriminator Loss: 1.3466293811798096\n",
      "Generator Loss: 0.35748979449272156\n",
      "Discriminator Loss: 1.3466780185699463\n",
      "Generator Loss: 0.3572809100151062\n",
      "Discriminator Loss: 1.3467724323272705\n",
      "Generator Loss: 0.3570706844329834\n",
      "Discriminator Loss: 1.3468598127365112\n",
      "Generator Loss: 0.3568442165851593\n",
      "Discriminator Loss: 1.3469096422195435\n",
      "Generator Loss: 0.35662806034088135\n",
      "Discriminator Loss: 1.3470180034637451\n",
      "Generator Loss: 0.3564060628414154\n",
      "Discriminator Loss: 1.3471496105194092\n",
      "Generator Loss: 0.3561955988407135\n",
      "Discriminator Loss: 1.3472944498062134\n",
      "Generator Loss: 0.35597747564315796\n",
      "Discriminator Loss: 1.3474119901657104\n",
      "Generator Loss: 0.35575973987579346\n",
      "Discriminator Loss: 1.3475706577301025\n",
      "Generator Loss: 0.3555375933647156\n",
      "Discriminator Loss: 1.3476766347885132\n",
      "Generator Loss: 0.35533407330513\n",
      "Discriminator Loss: 1.3478955030441284\n",
      "Generator Loss: 0.3550969064235687\n",
      "Discriminator Loss: 1.3480385541915894\n",
      "Generator Loss: 0.354892760515213\n",
      "Discriminator Loss: 1.3481734991073608\n",
      "Generator Loss: 0.3546750545501709\n",
      "Discriminator Loss: 1.3482824563980103\n",
      "Generator Loss: 0.35445719957351685\n",
      "Discriminator Loss: 1.3483554124832153\n",
      "Generator Loss: 0.3542359173297882\n",
      "Discriminator Loss: 1.3484808206558228\n",
      "Generator Loss: 0.354009211063385\n",
      "Discriminator Loss: 1.3486357927322388\n",
      "Generator Loss: 0.35379859805107117\n",
      "Discriminator Loss: 1.348778486251831\n",
      "Generator Loss: 0.3535875976085663\n",
      "Discriminator Loss: 1.348991870880127\n",
      "Generator Loss: 0.3533722162246704\n",
      "Discriminator Loss: 1.3491321802139282\n",
      "Generator Loss: 0.35316982865333557\n",
      "Discriminator Loss: 1.349325180053711\n",
      "Generator Loss: 0.3529536724090576\n",
      "Discriminator Loss: 1.349472165107727\n",
      "Generator Loss: 0.35273313522338867\n",
      "Discriminator Loss: 1.3496114015579224\n",
      "Generator Loss: 0.3525157868862152\n",
      "Discriminator Loss: 1.3496850728988647\n",
      "Generator Loss: 0.3523023724555969\n",
      "Discriminator Loss: 1.3498461246490479\n",
      "Generator Loss: 0.352083683013916\n",
      "Discriminator Loss: 1.3499319553375244\n",
      "Generator Loss: 0.3518882691860199\n",
      "Discriminator Loss: 1.3501077890396118\n",
      "Generator Loss: 0.35168421268463135\n",
      "Discriminator Loss: 1.350324034690857\n",
      "Generator Loss: 0.3514671325683594\n",
      "Discriminator Loss: 1.3504526615142822\n",
      "Generator Loss: 0.35123953223228455\n",
      "Discriminator Loss: 1.3505290746688843\n",
      "Generator Loss: 0.35103219747543335\n",
      "Discriminator Loss: 1.3507137298583984\n",
      "Generator Loss: 0.35082221031188965\n",
      "Discriminator Loss: 1.3508363962173462\n",
      "Generator Loss: 0.3506166338920593\n",
      "Discriminator Loss: 1.350985050201416\n",
      "Generator Loss: 0.35041046142578125\n",
      "Discriminator Loss: 1.3510918617248535\n",
      "Generator Loss: 0.35018599033355713\n",
      "Discriminator Loss: 1.351248860359192\n",
      "Generator Loss: 0.3499836027622223\n",
      "Discriminator Loss: 1.3513020277023315\n",
      "Generator Loss: 0.34977203607559204\n",
      "Discriminator Loss: 1.351392149925232\n",
      "Generator Loss: 0.349559485912323\n",
      "Discriminator Loss: 1.3515043258666992\n",
      "Generator Loss: 0.3493509590625763\n",
      "Discriminator Loss: 1.351693868637085\n",
      "Generator Loss: 0.34914740920066833\n",
      "Discriminator Loss: 1.3518366813659668\n",
      "Generator Loss: 0.3489409387111664\n",
      "Discriminator Loss: 1.35187566280365\n",
      "Generator Loss: 0.34873709082603455\n",
      "Discriminator Loss: 1.3520371913909912\n",
      "Generator Loss: 0.3485250174999237\n",
      "Discriminator Loss: 1.3521385192871094\n",
      "Generator Loss: 0.34833022952079773\n",
      "Discriminator Loss: 1.3523542881011963\n",
      "Generator Loss: 0.3481084704399109\n",
      "Discriminator Loss: 1.3525018692016602\n",
      "Generator Loss: 0.347923219203949\n",
      "Discriminator Loss: 1.352630853652954\n",
      "Generator Loss: 0.34771692752838135\n",
      "Discriminator Loss: 1.352770447731018\n",
      "Generator Loss: 0.34750622510910034\n",
      "Discriminator Loss: 1.3528962135314941\n",
      "Generator Loss: 0.3473089933395386\n",
      "Discriminator Loss: 1.3530330657958984\n",
      "Generator Loss: 0.34710243344306946\n",
      "Discriminator Loss: 1.353102445602417\n",
      "Generator Loss: 0.3468826413154602\n",
      "Discriminator Loss: 1.3531856536865234\n",
      "Generator Loss: 0.3466956615447998\n",
      "Discriminator Loss: 1.3532956838607788\n",
      "Generator Loss: 0.34649181365966797\n",
      "Discriminator Loss: 1.3535175323486328\n",
      "Generator Loss: 0.34629034996032715\n",
      "Discriminator Loss: 1.3536361455917358\n",
      "Generator Loss: 0.3461002707481384\n",
      "Discriminator Loss: 1.3537966012954712\n",
      "Generator Loss: 0.3458927571773529\n",
      "Discriminator Loss: 1.353914737701416\n",
      "Generator Loss: 0.34567680954933167\n",
      "Discriminator Loss: 1.3540124893188477\n",
      "Generator Loss: 0.3454899489879608\n",
      "Discriminator Loss: 1.3541086912155151\n",
      "Generator Loss: 0.34527918696403503\n",
      "Discriminator Loss: 1.354209065437317\n",
      "Generator Loss: 0.34507814049720764\n",
      "Discriminator Loss: 1.354295015335083\n",
      "Generator Loss: 0.3448666036128998\n",
      "About to save image...\n",
      "Shape of generated_images: (2, 28, 28, 1)\n",
      "Discriminator Loss: 1.3543920516967773\n",
      "Generator Loss: 0.3446562886238098\n",
      "Discriminator Loss: 1.3544671535491943\n",
      "Generator Loss: 0.3444608151912689\n",
      "Discriminator Loss: 1.354562520980835\n",
      "Generator Loss: 0.344257116317749\n",
      "Discriminator Loss: 1.354720115661621\n",
      "Generator Loss: 0.34405747056007385\n",
      "Discriminator Loss: 1.3548160791397095\n",
      "Generator Loss: 0.34387195110321045\n",
      "Discriminator Loss: 1.354904294013977\n",
      "Generator Loss: 0.343679279088974\n",
      "Discriminator Loss: 1.3550586700439453\n",
      "Generator Loss: 0.34346380829811096\n",
      "Discriminator Loss: 1.355151891708374\n",
      "Generator Loss: 0.3432716429233551\n",
      "Discriminator Loss: 1.355216145515442\n",
      "Generator Loss: 0.3430638909339905\n",
      "Discriminator Loss: 1.3553682565689087\n",
      "Generator Loss: 0.3428846597671509\n",
      "Discriminator Loss: 1.3554134368896484\n",
      "Generator Loss: 0.3426690399646759\n",
      "Discriminator Loss: 1.3555006980895996\n",
      "Generator Loss: 0.342477023601532\n",
      "Discriminator Loss: 1.3555982112884521\n",
      "Generator Loss: 0.34227651357650757\n",
      "Discriminator Loss: 1.355701208114624\n",
      "Generator Loss: 0.34206530451774597\n",
      "Discriminator Loss: 1.3558098077774048\n",
      "Generator Loss: 0.3418733477592468\n",
      "Discriminator Loss: 1.3559110164642334\n",
      "Generator Loss: 0.3416711390018463\n",
      "Discriminator Loss: 1.3560391664505005\n",
      "Generator Loss: 0.341505229473114\n",
      "Discriminator Loss: 1.3561668395996094\n",
      "Generator Loss: 0.3413003385066986\n",
      "Discriminator Loss: 1.3562284708023071\n",
      "Generator Loss: 0.3410833179950714\n",
      "Discriminator Loss: 1.3562923669815063\n",
      "Generator Loss: 0.3408946394920349\n",
      "Discriminator Loss: 1.3563481569290161\n",
      "Generator Loss: 0.34070414304733276\n",
      "Discriminator Loss: 1.356371521949768\n",
      "Generator Loss: 0.3405001163482666\n",
      "Discriminator Loss: 1.3564304113388062\n",
      "Generator Loss: 0.34030115604400635\n",
      "Discriminator Loss: 1.3565504550933838\n",
      "Generator Loss: 0.34012487530708313\n",
      "Discriminator Loss: 1.3566124439239502\n",
      "Generator Loss: 0.33992132544517517\n",
      "Discriminator Loss: 1.3566759824752808\n",
      "Generator Loss: 0.33974000811576843\n",
      "Discriminator Loss: 1.3567423820495605\n",
      "Generator Loss: 0.33955249190330505\n",
      "Discriminator Loss: 1.3568955659866333\n",
      "Generator Loss: 0.3393554091453552\n",
      "Discriminator Loss: 1.3570055961608887\n",
      "Generator Loss: 0.3391643166542053\n",
      "Discriminator Loss: 1.3570435047149658\n",
      "Generator Loss: 0.338965505361557\n",
      "Discriminator Loss: 1.3571345806121826\n",
      "Generator Loss: 0.338798850774765\n",
      "Discriminator Loss: 1.357250690460205\n",
      "Generator Loss: 0.33862176537513733\n",
      "Discriminator Loss: 1.3573771715164185\n",
      "Generator Loss: 0.3384172022342682\n",
      "Discriminator Loss: 1.3574984073638916\n",
      "Generator Loss: 0.3382338285446167\n",
      "Discriminator Loss: 1.3575676679611206\n",
      "Generator Loss: 0.3380456864833832\n",
      "Discriminator Loss: 1.3577985763549805\n",
      "Generator Loss: 0.33783987164497375\n",
      "Discriminator Loss: 1.3578366041183472\n",
      "Generator Loss: 0.33766698837280273\n",
      "Discriminator Loss: 1.357947587966919\n",
      "Generator Loss: 0.3374677896499634\n",
      "Discriminator Loss: 1.3580408096313477\n",
      "Generator Loss: 0.3372737467288971\n",
      "Discriminator Loss: 1.3580927848815918\n",
      "Generator Loss: 0.3370860517024994\n",
      "Discriminator Loss: 1.3581743240356445\n",
      "Generator Loss: 0.33690017461776733\n",
      "Discriminator Loss: 1.3582741022109985\n",
      "Generator Loss: 0.33671778440475464\n",
      "Discriminator Loss: 1.3583309650421143\n",
      "Generator Loss: 0.33652976155281067\n",
      "Discriminator Loss: 1.3584837913513184\n",
      "Generator Loss: 0.3363606333732605\n",
      "Discriminator Loss: 1.358583688735962\n",
      "Generator Loss: 0.33617475628852844\n",
      "Discriminator Loss: 1.3587020635604858\n",
      "Generator Loss: 0.3359854817390442\n",
      "Discriminator Loss: 1.3587872982025146\n",
      "Generator Loss: 0.33581387996673584\n",
      "Discriminator Loss: 1.3588627576828003\n",
      "Generator Loss: 0.33562058210372925\n",
      "Discriminator Loss: 1.3589333295822144\n",
      "Generator Loss: 0.3354410231113434\n",
      "Discriminator Loss: 1.359043002128601\n",
      "Generator Loss: 0.3352562189102173\n",
      "Discriminator Loss: 1.359163761138916\n",
      "Generator Loss: 0.3350677192211151\n",
      "Discriminator Loss: 1.3592875003814697\n",
      "Generator Loss: 0.33487632870674133\n",
      "Discriminator Loss: 1.359368085861206\n",
      "Generator Loss: 0.3346925377845764\n",
      "Discriminator Loss: 1.3594577312469482\n",
      "Generator Loss: 0.3344931900501251\n",
      "Discriminator Loss: 1.3595170974731445\n",
      "Generator Loss: 0.3343217670917511\n",
      "Discriminator Loss: 1.359593391418457\n",
      "Generator Loss: 0.3341485857963562\n",
      "Discriminator Loss: 1.3596680164337158\n",
      "Generator Loss: 0.3339512348175049\n",
      "Discriminator Loss: 1.3597960472106934\n",
      "Generator Loss: 0.33378708362579346\n",
      "Discriminator Loss: 1.3599567413330078\n",
      "Generator Loss: 0.3336088955402374\n",
      "Discriminator Loss: 1.3600671291351318\n",
      "Generator Loss: 0.3334348499774933\n",
      "Discriminator Loss: 1.3601552248001099\n",
      "Generator Loss: 0.333250492811203\n",
      "Discriminator Loss: 1.3603839874267578\n",
      "Generator Loss: 0.3330696225166321\n",
      "Discriminator Loss: 1.3606016635894775\n",
      "Generator Loss: 0.33289042115211487\n",
      "Discriminator Loss: 1.3606715202331543\n",
      "Generator Loss: 0.3327029049396515\n",
      "Discriminator Loss: 1.3607219457626343\n",
      "Generator Loss: 0.3325382471084595\n",
      "Discriminator Loss: 1.3608229160308838\n",
      "Generator Loss: 0.33236122131347656\n",
      "Discriminator Loss: 1.360962986946106\n",
      "Generator Loss: 0.33216509222984314\n",
      "Discriminator Loss: 1.3610190153121948\n",
      "Generator Loss: 0.33200401067733765\n",
      "Discriminator Loss: 1.3611093759536743\n",
      "Generator Loss: 0.33183127641677856\n",
      "Discriminator Loss: 1.3612112998962402\n",
      "Generator Loss: 0.3316437304019928\n",
      "Discriminator Loss: 1.3612369298934937\n",
      "Generator Loss: 0.33145231008529663\n",
      "Discriminator Loss: 1.361319661140442\n",
      "Generator Loss: 0.33127710223197937\n",
      "Discriminator Loss: 1.3614223003387451\n",
      "Generator Loss: 0.3311249613761902\n",
      "Discriminator Loss: 1.3615738153457642\n",
      "Generator Loss: 0.3309400677680969\n",
      "Discriminator Loss: 1.3616310358047485\n",
      "Generator Loss: 0.33075186610221863\n",
      "Discriminator Loss: 1.3617311716079712\n",
      "Generator Loss: 0.3305639624595642\n",
      "Discriminator Loss: 1.3618247509002686\n",
      "Generator Loss: 0.3303861618041992\n",
      "Discriminator Loss: 1.361967921257019\n",
      "Generator Loss: 0.3302095830440521\n",
      "Discriminator Loss: 1.3620578050613403\n",
      "Generator Loss: 0.3300445079803467\n",
      "Discriminator Loss: 1.3621246814727783\n",
      "Generator Loss: 0.32986578345298767\n",
      "Discriminator Loss: 1.3621922731399536\n",
      "Generator Loss: 0.32969456911087036\n",
      "Discriminator Loss: 1.362325668334961\n",
      "Generator Loss: 0.32951483130455017\n",
      "Discriminator Loss: 1.3624728918075562\n",
      "Generator Loss: 0.32933589816093445\n",
      "Discriminator Loss: 1.3625162839889526\n",
      "Generator Loss: 0.32915055751800537\n",
      "Discriminator Loss: 1.3626028299331665\n",
      "Generator Loss: 0.328968346118927\n",
      "Discriminator Loss: 1.3626267910003662\n",
      "Generator Loss: 0.3287796974182129\n",
      "Discriminator Loss: 1.362723469734192\n",
      "Generator Loss: 0.32862117886543274\n",
      "Discriminator Loss: 1.3627814054489136\n",
      "Generator Loss: 0.32843345403671265\n",
      "Discriminator Loss: 1.3628382682800293\n",
      "Generator Loss: 0.3282757103443146\n",
      "Discriminator Loss: 1.3629664182662964\n",
      "Generator Loss: 0.3281012773513794\n",
      "Discriminator Loss: 1.3630322217941284\n",
      "Generator Loss: 0.3279131352901459\n",
      "Discriminator Loss: 1.363138198852539\n",
      "Generator Loss: 0.32773301005363464\n",
      "Discriminator Loss: 1.3632738590240479\n",
      "Generator Loss: 0.32755613327026367\n",
      "Discriminator Loss: 1.3633993864059448\n",
      "Generator Loss: 0.3273897171020508\n",
      "Discriminator Loss: 1.3635302782058716\n",
      "Generator Loss: 0.32723453640937805\n",
      "Discriminator Loss: 1.3636696338653564\n",
      "Generator Loss: 0.32706061005592346\n",
      "Discriminator Loss: 1.363816261291504\n",
      "Generator Loss: 0.3268895149230957\n",
      "Discriminator Loss: 1.3639355897903442\n",
      "Generator Loss: 0.32670876383781433\n",
      "Discriminator Loss: 1.364004135131836\n",
      "Generator Loss: 0.3265276849269867\n",
      "Discriminator Loss: 1.3640737533569336\n",
      "Generator Loss: 0.326355516910553\n",
      "Discriminator Loss: 1.3642221689224243\n",
      "Generator Loss: 0.3261882960796356\n",
      "Discriminator Loss: 1.3643887042999268\n",
      "Generator Loss: 0.3260236084461212\n",
      "Discriminator Loss: 1.3645360469818115\n",
      "Generator Loss: 0.32584601640701294\n",
      "Discriminator Loss: 1.3646620512008667\n",
      "Generator Loss: 0.3256854712963104\n",
      "Discriminator Loss: 1.364734172821045\n",
      "Generator Loss: 0.3255137503147125\n",
      "Discriminator Loss: 1.3648873567581177\n",
      "Generator Loss: 0.32533761858940125\n",
      "Discriminator Loss: 1.365041732788086\n",
      "Generator Loss: 0.3251625895500183\n",
      "Discriminator Loss: 1.3651126623153687\n",
      "Generator Loss: 0.32499295473098755\n",
      "Discriminator Loss: 1.3652689456939697\n",
      "Generator Loss: 0.3248249292373657\n",
      "Discriminator Loss: 1.3654077053070068\n",
      "Generator Loss: 0.324646532535553\n",
      "Discriminator Loss: 1.3655102252960205\n",
      "Generator Loss: 0.32447549700737\n",
      "Discriminator Loss: 1.3655881881713867\n",
      "Generator Loss: 0.3243083357810974\n",
      "Discriminator Loss: 1.3656824827194214\n",
      "Generator Loss: 0.32413730025291443\n",
      "Discriminator Loss: 1.3658431768417358\n",
      "Generator Loss: 0.3239682614803314\n",
      "Discriminator Loss: 1.3659472465515137\n",
      "Generator Loss: 0.3237888514995575\n",
      "Discriminator Loss: 1.3660452365875244\n",
      "Generator Loss: 0.3236198425292969\n",
      "Discriminator Loss: 1.366202712059021\n",
      "Generator Loss: 0.3234594762325287\n",
      "Discriminator Loss: 1.3664162158966064\n",
      "Generator Loss: 0.32329556345939636\n",
      "Discriminator Loss: 1.3665404319763184\n",
      "Generator Loss: 0.323102205991745\n",
      "Discriminator Loss: 1.3666317462921143\n",
      "Generator Loss: 0.3229271173477173\n",
      "Discriminator Loss: 1.3667492866516113\n",
      "Generator Loss: 0.32275816798210144\n",
      "Discriminator Loss: 1.3668655157089233\n",
      "Generator Loss: 0.32259395718574524\n",
      "Discriminator Loss: 1.3670397996902466\n",
      "Generator Loss: 0.322422593832016\n",
      "Discriminator Loss: 1.3671907186508179\n",
      "Generator Loss: 0.3222741186618805\n",
      "Discriminator Loss: 1.3674553632736206\n",
      "Generator Loss: 0.3221033811569214\n",
      "Discriminator Loss: 1.3675798177719116\n",
      "Generator Loss: 0.3219238221645355\n",
      "Discriminator Loss: 1.3678011894226074\n",
      "Generator Loss: 0.3217845857143402\n",
      "Discriminator Loss: 1.3680680990219116\n",
      "Generator Loss: 0.32160717248916626\n",
      "Discriminator Loss: 1.3682265281677246\n",
      "Generator Loss: 0.32144615054130554\n",
      "Discriminator Loss: 1.3683804273605347\n",
      "Generator Loss: 0.3212722837924957\n",
      "Discriminator Loss: 1.36859929561615\n",
      "Generator Loss: 0.32111403346061707\n",
      "Discriminator Loss: 1.36873459815979\n",
      "Generator Loss: 0.32097160816192627\n",
      "Discriminator Loss: 1.3690004348754883\n",
      "Generator Loss: 0.32080474495887756\n",
      "Discriminator Loss: 1.369194746017456\n",
      "Generator Loss: 0.3206346929073334\n",
      "Discriminator Loss: 1.3694241046905518\n",
      "Generator Loss: 0.32047730684280396\n",
      "Discriminator Loss: 1.3695387840270996\n",
      "Generator Loss: 0.320310115814209\n",
      "Discriminator Loss: 1.369673728942871\n",
      "Generator Loss: 0.3201347887516022\n",
      "Discriminator Loss: 1.3698136806488037\n",
      "Generator Loss: 0.3199658691883087\n",
      "Discriminator Loss: 1.3699411153793335\n",
      "Generator Loss: 0.31980255246162415\n",
      "Discriminator Loss: 1.3701016902923584\n",
      "Generator Loss: 0.31963154673576355\n",
      "Discriminator Loss: 1.3702633380889893\n",
      "Generator Loss: 0.31946060061454773\n",
      "Discriminator Loss: 1.3703609704971313\n",
      "Generator Loss: 0.31931161880493164\n",
      "Discriminator Loss: 1.3705079555511475\n",
      "Generator Loss: 0.3191379904747009\n",
      "Discriminator Loss: 1.3706576824188232\n",
      "Generator Loss: 0.3189718425273895\n",
      "Discriminator Loss: 1.3708480596542358\n",
      "Generator Loss: 0.31882378458976746\n",
      "Discriminator Loss: 1.37107253074646\n",
      "Generator Loss: 0.31864818930625916\n",
      "Discriminator Loss: 1.3713163137435913\n",
      "Generator Loss: 0.31848594546318054\n",
      "Discriminator Loss: 1.3715624809265137\n",
      "Generator Loss: 0.31831124424934387\n",
      "Discriminator Loss: 1.371747612953186\n",
      "Generator Loss: 0.31815358996391296\n",
      "Discriminator Loss: 1.3719151020050049\n",
      "Generator Loss: 0.3179836571216583\n",
      "Discriminator Loss: 1.3720295429229736\n",
      "Generator Loss: 0.31783056259155273\n",
      "Discriminator Loss: 1.3721925020217896\n",
      "Generator Loss: 0.31766438484191895\n",
      "Discriminator Loss: 1.3723881244659424\n",
      "Generator Loss: 0.31748494505882263\n",
      "Discriminator Loss: 1.3725453615188599\n",
      "Generator Loss: 0.31731924414634705\n",
      "Discriminator Loss: 1.3726695775985718\n",
      "Generator Loss: 0.31715908646583557\n",
      "Discriminator Loss: 1.3728221654891968\n",
      "Generator Loss: 0.31698524951934814\n",
      "Discriminator Loss: 1.3730344772338867\n",
      "Generator Loss: 0.31681132316589355\n",
      "Discriminator Loss: 1.373219609260559\n",
      "Generator Loss: 0.3166538178920746\n",
      "Discriminator Loss: 1.3734418153762817\n",
      "Generator Loss: 0.31649962067604065\n",
      "Discriminator Loss: 1.3736541271209717\n",
      "Generator Loss: 0.31633609533309937\n",
      "Discriminator Loss: 1.3738853931427002\n",
      "Generator Loss: 0.3161696791648865\n",
      "Discriminator Loss: 1.374036431312561\n",
      "Generator Loss: 0.31600385904312134\n",
      "Discriminator Loss: 1.3741579055786133\n",
      "Generator Loss: 0.3158489167690277\n",
      "Discriminator Loss: 1.3743395805358887\n",
      "Generator Loss: 0.315688818693161\n",
      "Discriminator Loss: 1.3745008707046509\n",
      "Generator Loss: 0.3155195116996765\n",
      "Discriminator Loss: 1.3747278451919556\n",
      "Generator Loss: 0.3153494894504547\n",
      "Discriminator Loss: 1.3748306035995483\n",
      "Generator Loss: 0.31518715620040894\n",
      "Discriminator Loss: 1.3749415874481201\n",
      "Generator Loss: 0.31501778960227966\n",
      "Discriminator Loss: 1.3751457929611206\n",
      "Generator Loss: 0.3148624897003174\n",
      "Discriminator Loss: 1.3753418922424316\n",
      "Generator Loss: 0.3146900236606598\n",
      "Discriminator Loss: 1.375526785850525\n",
      "Generator Loss: 0.3145373463630676\n",
      "Discriminator Loss: 1.3757197856903076\n",
      "Generator Loss: 0.31437715888023376\n",
      "Discriminator Loss: 1.375863790512085\n",
      "Generator Loss: 0.3142129182815552\n",
      "Discriminator Loss: 1.3760716915130615\n",
      "Generator Loss: 0.31404805183410645\n",
      "Discriminator Loss: 1.3762493133544922\n",
      "Generator Loss: 0.31388550996780396\n",
      "Discriminator Loss: 1.376457929611206\n",
      "Generator Loss: 0.3137107789516449\n",
      "Discriminator Loss: 1.376577615737915\n",
      "Generator Loss: 0.31355369091033936\n",
      "Discriminator Loss: 1.37671959400177\n",
      "Generator Loss: 0.31339168548583984\n",
      "Discriminator Loss: 1.3768950700759888\n",
      "Generator Loss: 0.31323716044425964\n",
      "Discriminator Loss: 1.3770631551742554\n",
      "Generator Loss: 0.31307777762413025\n",
      "Discriminator Loss: 1.377296805381775\n",
      "Generator Loss: 0.3129206597805023\n",
      "Discriminator Loss: 1.377526044845581\n",
      "Generator Loss: 0.3127506375312805\n",
      "Discriminator Loss: 1.3777551651000977\n",
      "Generator Loss: 0.31258735060691833\n",
      "Discriminator Loss: 1.3778345584869385\n",
      "Generator Loss: 0.31242164969444275\n",
      "Discriminator Loss: 1.3779854774475098\n",
      "Generator Loss: 0.3122645616531372\n",
      "Discriminator Loss: 1.3781572580337524\n",
      "Generator Loss: 0.31210091710090637\n",
      "Discriminator Loss: 1.3783327341079712\n",
      "Generator Loss: 0.3119342625141144\n",
      "About to save image...\n",
      "Shape of generated_images: (2, 28, 28, 1)\n",
      "Discriminator Loss: 1.3784316778182983\n",
      "Generator Loss: 0.311769962310791\n",
      "Discriminator Loss: 1.378612756729126\n",
      "Generator Loss: 0.3116099536418915\n",
      "Discriminator Loss: 1.3788384199142456\n",
      "Generator Loss: 0.31144067645072937\n",
      "Discriminator Loss: 1.379087209701538\n",
      "Generator Loss: 0.31128087639808655\n",
      "Discriminator Loss: 1.3793089389801025\n",
      "Generator Loss: 0.3111155033111572\n",
      "Discriminator Loss: 1.3795065879821777\n",
      "Generator Loss: 0.3109475374221802\n",
      "Discriminator Loss: 1.3796380758285522\n",
      "Generator Loss: 0.31079789996147156\n",
      "Discriminator Loss: 1.3798283338546753\n",
      "Generator Loss: 0.3106440603733063\n",
      "Discriminator Loss: 1.380021572113037\n",
      "Generator Loss: 0.31047260761260986\n",
      "Discriminator Loss: 1.3801987171173096\n",
      "Generator Loss: 0.31031137704849243\n",
      "Discriminator Loss: 1.3803386688232422\n",
      "Generator Loss: 0.3101440966129303\n",
      "Discriminator Loss: 1.3805395364761353\n",
      "Generator Loss: 0.3099857568740845\n",
      "Discriminator Loss: 1.380756139755249\n",
      "Generator Loss: 0.3098301887512207\n",
      "Discriminator Loss: 1.380890965461731\n",
      "Generator Loss: 0.30966916680336\n",
      "Discriminator Loss: 1.3810516595840454\n",
      "Generator Loss: 0.30950137972831726\n",
      "Discriminator Loss: 1.3812445402145386\n",
      "Generator Loss: 0.3093450963497162\n",
      "Discriminator Loss: 1.3814173936843872\n",
      "Generator Loss: 0.30918553471565247\n",
      "Discriminator Loss: 1.3816059827804565\n",
      "Generator Loss: 0.30902549624443054\n",
      "Discriminator Loss: 1.3818334341049194\n",
      "Generator Loss: 0.3088565170764923\n",
      "Discriminator Loss: 1.382051706314087\n",
      "Generator Loss: 0.30870673060417175\n",
      "Discriminator Loss: 1.3822050094604492\n",
      "Generator Loss: 0.30855387449264526\n",
      "Discriminator Loss: 1.382382869720459\n",
      "Generator Loss: 0.3083900809288025\n",
      "Discriminator Loss: 1.382614016532898\n",
      "Generator Loss: 0.30823224782943726\n",
      "Discriminator Loss: 1.3828097581863403\n",
      "Generator Loss: 0.3080694377422333\n",
      "Discriminator Loss: 1.3829983472824097\n",
      "Generator Loss: 0.30789944529533386\n",
      "Discriminator Loss: 1.383164644241333\n",
      "Generator Loss: 0.3077494204044342\n",
      "Discriminator Loss: 1.383345365524292\n",
      "Generator Loss: 0.3075905442237854\n",
      "Discriminator Loss: 1.38350248336792\n",
      "Generator Loss: 0.30743902921676636\n",
      "Discriminator Loss: 1.3836755752563477\n",
      "Generator Loss: 0.30727314949035645\n",
      "Discriminator Loss: 1.3838295936584473\n",
      "Generator Loss: 0.307126522064209\n",
      "Discriminator Loss: 1.3839730024337769\n",
      "Generator Loss: 0.30696332454681396\n",
      "Discriminator Loss: 1.3841238021850586\n",
      "Generator Loss: 0.3068009316921234\n",
      "Discriminator Loss: 1.3842779397964478\n",
      "Generator Loss: 0.30664780735969543\n",
      "Discriminator Loss: 1.3844802379608154\n",
      "Generator Loss: 0.30649882555007935\n",
      "Discriminator Loss: 1.3846818208694458\n",
      "Generator Loss: 0.3063463866710663\n",
      "Discriminator Loss: 1.384968638420105\n",
      "Generator Loss: 0.30620476603507996\n",
      "Discriminator Loss: 1.3852416276931763\n",
      "Generator Loss: 0.3060450851917267\n",
      "Discriminator Loss: 1.3855117559432983\n",
      "Generator Loss: 0.305902898311615\n",
      "Discriminator Loss: 1.3856871128082275\n",
      "Generator Loss: 0.305733859539032\n",
      "Discriminator Loss: 1.3858455419540405\n",
      "Generator Loss: 0.30557867884635925\n",
      "Discriminator Loss: 1.3860926628112793\n",
      "Generator Loss: 0.30542227625846863\n",
      "Discriminator Loss: 1.386297583580017\n",
      "Generator Loss: 0.3052765130996704\n",
      "Discriminator Loss: 1.3865065574645996\n",
      "Generator Loss: 0.30512022972106934\n",
      "Discriminator Loss: 1.3867038488388062\n",
      "Generator Loss: 0.30498194694519043\n",
      "Discriminator Loss: 1.3868855237960815\n",
      "Generator Loss: 0.3048287630081177\n",
      "Discriminator Loss: 1.3871479034423828\n",
      "Generator Loss: 0.3046843111515045\n",
      "Discriminator Loss: 1.3873348236083984\n",
      "Generator Loss: 0.3045309782028198\n",
      "Discriminator Loss: 1.387508749961853\n",
      "Generator Loss: 0.3043852746486664\n",
      "Discriminator Loss: 1.3877441883087158\n",
      "Generator Loss: 0.3042268753051758\n",
      "Discriminator Loss: 1.3879810571670532\n",
      "Generator Loss: 0.3040775954723358\n",
      "Discriminator Loss: 1.3881361484527588\n",
      "Generator Loss: 0.3039237856864929\n",
      "Discriminator Loss: 1.388271689414978\n",
      "Generator Loss: 0.3037774860858917\n",
      "Discriminator Loss: 1.3884639739990234\n",
      "Generator Loss: 0.30362585186958313\n",
      "Discriminator Loss: 1.3886334896087646\n",
      "Generator Loss: 0.30347558856010437\n",
      "Discriminator Loss: 1.3888115882873535\n",
      "Generator Loss: 0.3033352494239807\n",
      "Discriminator Loss: 1.3890548944473267\n",
      "Generator Loss: 0.3031857907772064\n",
      "Discriminator Loss: 1.3892513513565063\n",
      "Generator Loss: 0.303037166595459\n",
      "Discriminator Loss: 1.3894226551055908\n",
      "Generator Loss: 0.3028818964958191\n",
      "Discriminator Loss: 1.389618158340454\n",
      "Generator Loss: 0.302737295627594\n",
      "Discriminator Loss: 1.389786958694458\n",
      "Generator Loss: 0.30259454250335693\n",
      "Discriminator Loss: 1.3899457454681396\n",
      "Generator Loss: 0.3024335205554962\n",
      "Discriminator Loss: 1.3901115655899048\n",
      "Generator Loss: 0.3022928535938263\n",
      "Discriminator Loss: 1.3903144598007202\n",
      "Generator Loss: 0.3021552860736847\n",
      "Discriminator Loss: 1.3904491662979126\n",
      "Generator Loss: 0.3020044267177582\n",
      "Discriminator Loss: 1.3905919790267944\n",
      "Generator Loss: 0.3018791675567627\n",
      "Discriminator Loss: 1.39082932472229\n",
      "Generator Loss: 0.3017258942127228\n",
      "Discriminator Loss: 1.3909894227981567\n",
      "Generator Loss: 0.3015746772289276\n",
      "Discriminator Loss: 1.3910926580429077\n",
      "Generator Loss: 0.30142179131507874\n",
      "Discriminator Loss: 1.3912397623062134\n",
      "Generator Loss: 0.3012869954109192\n",
      "Discriminator Loss: 1.3913911581039429\n",
      "Generator Loss: 0.3011350929737091\n",
      "Discriminator Loss: 1.3915115594863892\n",
      "Generator Loss: 0.3009903132915497\n",
      "Discriminator Loss: 1.3917218446731567\n",
      "Generator Loss: 0.3008529245853424\n",
      "Discriminator Loss: 1.3918012380599976\n",
      "Generator Loss: 0.300715833902359\n",
      "Discriminator Loss: 1.391903042793274\n",
      "Generator Loss: 0.30057576298713684\n",
      "Discriminator Loss: 1.392067790031433\n",
      "Generator Loss: 0.30043327808380127\n",
      "Discriminator Loss: 1.39223051071167\n",
      "Generator Loss: 0.3002871572971344\n",
      "Discriminator Loss: 1.392418622970581\n",
      "Generator Loss: 0.30013948678970337\n",
      "Discriminator Loss: 1.392553448677063\n",
      "Generator Loss: 0.3000086843967438\n",
      "Discriminator Loss: 1.392649531364441\n",
      "Generator Loss: 0.2998707592487335\n",
      "Discriminator Loss: 1.392849087715149\n",
      "Generator Loss: 0.2997261881828308\n",
      "Discriminator Loss: 1.3929414749145508\n",
      "Generator Loss: 0.2996017634868622\n",
      "Discriminator Loss: 1.3931149244308472\n",
      "Generator Loss: 0.29944950342178345\n",
      "Discriminator Loss: 1.3932057619094849\n",
      "Generator Loss: 0.299318790435791\n",
      "Discriminator Loss: 1.3933424949645996\n",
      "Generator Loss: 0.2991830110549927\n",
      "Discriminator Loss: 1.3934391736984253\n",
      "Generator Loss: 0.29905828833580017\n",
      "Discriminator Loss: 1.3935716152191162\n",
      "Generator Loss: 0.2989150881767273\n",
      "Discriminator Loss: 1.3936924934387207\n",
      "Generator Loss: 0.298780620098114\n",
      "Discriminator Loss: 1.3938366174697876\n",
      "Generator Loss: 0.2986481785774231\n",
      "Discriminator Loss: 1.3939839601516724\n",
      "Generator Loss: 0.29851269721984863\n",
      "Discriminator Loss: 1.394100308418274\n",
      "Generator Loss: 0.2983763515949249\n",
      "Discriminator Loss: 1.394271731376648\n",
      "Generator Loss: 0.2982317805290222\n",
      "Discriminator Loss: 1.3943785429000854\n",
      "Generator Loss: 0.29809388518333435\n",
      "Discriminator Loss: 1.394673466682434\n",
      "Generator Loss: 0.29796335101127625\n",
      "Discriminator Loss: 1.3947712182998657\n",
      "Generator Loss: 0.2978220582008362\n",
      "Discriminator Loss: 1.3948699235916138\n",
      "Generator Loss: 0.2976968288421631\n",
      "Discriminator Loss: 1.3950563669204712\n",
      "Generator Loss: 0.2975502908229828\n",
      "Discriminator Loss: 1.3951351642608643\n",
      "Generator Loss: 0.2974218428134918\n",
      "Discriminator Loss: 1.3953166007995605\n",
      "Generator Loss: 0.2972952723503113\n",
      "Discriminator Loss: 1.3955034017562866\n",
      "Generator Loss: 0.2971665561199188\n",
      "Discriminator Loss: 1.3956329822540283\n",
      "Generator Loss: 0.29703155159950256\n",
      "Discriminator Loss: 1.3957921266555786\n",
      "Generator Loss: 0.29690414667129517\n",
      "Discriminator Loss: 1.3960295915603638\n",
      "Generator Loss: 0.29676130414009094\n",
      "Discriminator Loss: 1.3961585760116577\n",
      "Generator Loss: 0.29664406180381775\n",
      "Discriminator Loss: 1.3963587284088135\n",
      "Generator Loss: 0.2965123653411865\n",
      "Discriminator Loss: 1.3964483737945557\n",
      "Generator Loss: 0.2963864803314209\n",
      "Discriminator Loss: 1.396592378616333\n",
      "Generator Loss: 0.2962656021118164\n",
      "Discriminator Loss: 1.3967615365982056\n",
      "Generator Loss: 0.2961246967315674\n",
      "Discriminator Loss: 1.396903395652771\n",
      "Generator Loss: 0.29598307609558105\n",
      "Discriminator Loss: 1.3970235586166382\n",
      "Generator Loss: 0.29585447907447815\n",
      "Discriminator Loss: 1.3971456289291382\n",
      "Generator Loss: 0.29570844769477844\n",
      "Discriminator Loss: 1.397242546081543\n",
      "Generator Loss: 0.2955664098262787\n",
      "Discriminator Loss: 1.3973337411880493\n",
      "Generator Loss: 0.2954553961753845\n",
      "Discriminator Loss: 1.3974717855453491\n",
      "Generator Loss: 0.2953096628189087\n",
      "Discriminator Loss: 1.3975926637649536\n",
      "Generator Loss: 0.29517146944999695\n",
      "Discriminator Loss: 1.3977030515670776\n",
      "Generator Loss: 0.29503172636032104\n",
      "Discriminator Loss: 1.397800087928772\n",
      "Generator Loss: 0.2948952317237854\n",
      "Discriminator Loss: 1.3980028629302979\n",
      "Generator Loss: 0.29477593302726746\n",
      "Discriminator Loss: 1.3981258869171143\n",
      "Generator Loss: 0.2946507930755615\n",
      "Discriminator Loss: 1.39821457862854\n",
      "Generator Loss: 0.2945130169391632\n",
      "Discriminator Loss: 1.3983173370361328\n",
      "Generator Loss: 0.2943761944770813\n",
      "Discriminator Loss: 1.3984895944595337\n",
      "Generator Loss: 0.2942422330379486\n",
      "Discriminator Loss: 1.3987246751785278\n",
      "Generator Loss: 0.2941019833087921\n",
      "Discriminator Loss: 1.398888349533081\n",
      "Generator Loss: 0.2939707040786743\n",
      "Discriminator Loss: 1.3989801406860352\n",
      "Generator Loss: 0.29382795095443726\n",
      "Discriminator Loss: 1.3990728855133057\n",
      "Generator Loss: 0.2937115430831909\n",
      "Discriminator Loss: 1.3991358280181885\n",
      "Generator Loss: 0.29360586404800415\n",
      "Discriminator Loss: 1.399446725845337\n",
      "Generator Loss: 0.2934691309928894\n",
      "Discriminator Loss: 1.399666666984558\n",
      "Generator Loss: 0.29332974553108215\n",
      "Discriminator Loss: 1.399800181388855\n",
      "Generator Loss: 0.2931937575340271\n",
      "Discriminator Loss: 1.399883508682251\n",
      "Generator Loss: 0.2930695414543152\n",
      "Discriminator Loss: 1.4000442028045654\n",
      "Generator Loss: 0.2929202616214752\n",
      "Discriminator Loss: 1.4001781940460205\n",
      "Generator Loss: 0.2928030490875244\n",
      "Discriminator Loss: 1.4003043174743652\n",
      "Generator Loss: 0.2926607131958008\n",
      "Discriminator Loss: 1.4003993272781372\n",
      "Generator Loss: 0.29252150654792786\n",
      "Discriminator Loss: 1.4005194902420044\n",
      "Generator Loss: 0.2923850417137146\n",
      "Discriminator Loss: 1.4006317853927612\n",
      "Generator Loss: 0.2922404706478119\n",
      "Discriminator Loss: 1.4007527828216553\n",
      "Generator Loss: 0.29211506247520447\n",
      "Discriminator Loss: 1.400913953781128\n",
      "Generator Loss: 0.29198136925697327\n",
      "Discriminator Loss: 1.4010651111602783\n",
      "Generator Loss: 0.2918522357940674\n",
      "Discriminator Loss: 1.4011932611465454\n",
      "Generator Loss: 0.29172199964523315\n",
      "Discriminator Loss: 1.401384711265564\n",
      "Generator Loss: 0.29159417748451233\n",
      "Discriminator Loss: 1.401542067527771\n",
      "Generator Loss: 0.2914598286151886\n",
      "Discriminator Loss: 1.4016814231872559\n",
      "Generator Loss: 0.2913455069065094\n",
      "Discriminator Loss: 1.4018017053604126\n",
      "Generator Loss: 0.291205495595932\n",
      "Discriminator Loss: 1.4019564390182495\n",
      "Generator Loss: 0.29107046127319336\n",
      "Discriminator Loss: 1.4021142721176147\n",
      "Generator Loss: 0.2909388840198517\n",
      "Discriminator Loss: 1.402321696281433\n",
      "Generator Loss: 0.2908080816268921\n",
      "Discriminator Loss: 1.4024639129638672\n",
      "Generator Loss: 0.2906779944896698\n",
      "Discriminator Loss: 1.4025839567184448\n",
      "Generator Loss: 0.2905479669570923\n",
      "Discriminator Loss: 1.4027230739593506\n",
      "Generator Loss: 0.2904044985771179\n",
      "Discriminator Loss: 1.4028569459915161\n",
      "Generator Loss: 0.2902812659740448\n",
      "Discriminator Loss: 1.4029948711395264\n",
      "Generator Loss: 0.2901468873023987\n",
      "Discriminator Loss: 1.4031871557235718\n",
      "Generator Loss: 0.2900210916996002\n",
      "Discriminator Loss: 1.4033262729644775\n",
      "Generator Loss: 0.2898920476436615\n",
      "Discriminator Loss: 1.4034751653671265\n",
      "Generator Loss: 0.2897569537162781\n",
      "Discriminator Loss: 1.4036436080932617\n",
      "Generator Loss: 0.28963330388069153\n",
      "Discriminator Loss: 1.4037761688232422\n",
      "Generator Loss: 0.28951045870780945\n",
      "Discriminator Loss: 1.4039386510849\n",
      "Generator Loss: 0.2893681526184082\n",
      "Discriminator Loss: 1.4040402173995972\n",
      "Generator Loss: 0.28922760486602783\n",
      "Discriminator Loss: 1.404179334640503\n",
      "Generator Loss: 0.289104700088501\n",
      "Discriminator Loss: 1.4043080806732178\n",
      "Generator Loss: 0.2889665365219116\n",
      "Discriminator Loss: 1.4044580459594727\n",
      "Generator Loss: 0.28883641958236694\n",
      "Discriminator Loss: 1.4045790433883667\n",
      "Generator Loss: 0.2886945605278015\n",
      "Discriminator Loss: 1.4046835899353027\n",
      "Generator Loss: 0.2885783612728119\n",
      "Discriminator Loss: 1.404834270477295\n",
      "Generator Loss: 0.2884492874145508\n",
      "Discriminator Loss: 1.4049876928329468\n",
      "Generator Loss: 0.28831830620765686\n",
      "Discriminator Loss: 1.4051899909973145\n",
      "Generator Loss: 0.2881845235824585\n",
      "Discriminator Loss: 1.4054756164550781\n",
      "Generator Loss: 0.2880520224571228\n",
      "Discriminator Loss: 1.405630111694336\n",
      "Generator Loss: 0.28793463110923767\n",
      "Discriminator Loss: 1.4058858156204224\n",
      "Generator Loss: 0.28780561685562134\n",
      "Discriminator Loss: 1.4060667753219604\n",
      "Generator Loss: 0.28768110275268555\n",
      "Discriminator Loss: 1.4062403440475464\n",
      "Generator Loss: 0.28754693269729614\n",
      "Discriminator Loss: 1.4063472747802734\n",
      "Generator Loss: 0.2874222695827484\n",
      "Discriminator Loss: 1.4064786434173584\n",
      "Generator Loss: 0.2872941792011261\n",
      "Discriminator Loss: 1.4066797494888306\n",
      "Generator Loss: 0.2871737480163574\n",
      "Discriminator Loss: 1.406851053237915\n",
      "Generator Loss: 0.28704702854156494\n",
      "Discriminator Loss: 1.4070426225662231\n",
      "Generator Loss: 0.28691938519477844\n",
      "Discriminator Loss: 1.407198190689087\n",
      "Generator Loss: 0.2867870628833771\n",
      "Discriminator Loss: 1.4073100090026855\n",
      "Generator Loss: 0.2866605818271637\n",
      "Discriminator Loss: 1.4074416160583496\n",
      "Generator Loss: 0.2865408957004547\n",
      "Discriminator Loss: 1.407625436782837\n",
      "Generator Loss: 0.2864226698875427\n",
      "Discriminator Loss: 1.4078211784362793\n",
      "Generator Loss: 0.28629353642463684\n",
      "Discriminator Loss: 1.4079580307006836\n",
      "Generator Loss: 0.286167711019516\n",
      "Discriminator Loss: 1.4080966711044312\n",
      "Generator Loss: 0.2860357463359833\n",
      "Discriminator Loss: 1.4082832336425781\n",
      "Generator Loss: 0.28591400384902954\n",
      "Discriminator Loss: 1.408453345298767\n",
      "Generator Loss: 0.28579068183898926\n",
      "Discriminator Loss: 1.4085681438446045\n",
      "Generator Loss: 0.28567808866500854\n",
      "About to save image...\n",
      "Shape of generated_images: (2, 28, 28, 1)\n",
      "Discriminator Loss: 1.4087437391281128\n",
      "Generator Loss: 0.2855463922023773\n",
      "Discriminator Loss: 1.4089242219924927\n",
      "Generator Loss: 0.2854164242744446\n",
      "Discriminator Loss: 1.4090907573699951\n",
      "Generator Loss: 0.28530603647232056\n",
      "Discriminator Loss: 1.409249186515808\n",
      "Generator Loss: 0.2851715683937073\n",
      "Discriminator Loss: 1.4093626737594604\n",
      "Generator Loss: 0.2850460410118103\n",
      "Discriminator Loss: 1.4095089435577393\n",
      "Generator Loss: 0.28492051362991333\n",
      "Discriminator Loss: 1.4096572399139404\n",
      "Generator Loss: 0.284806489944458\n",
      "Discriminator Loss: 1.4098024368286133\n",
      "Generator Loss: 0.2846798598766327\n",
      "Discriminator Loss: 1.4099377393722534\n",
      "Generator Loss: 0.2845451831817627\n",
      "Discriminator Loss: 1.4100925922393799\n",
      "Generator Loss: 0.2844235897064209\n",
      "Discriminator Loss: 1.4102957248687744\n",
      "Generator Loss: 0.28430038690567017\n",
      "Discriminator Loss: 1.4104300737380981\n",
      "Generator Loss: 0.28417453169822693\n",
      "Discriminator Loss: 1.4105888605117798\n",
      "Generator Loss: 0.2840515077114105\n",
      "Discriminator Loss: 1.4107569456100464\n",
      "Generator Loss: 0.2839140295982361\n",
      "Discriminator Loss: 1.4109413623809814\n",
      "Generator Loss: 0.28379616141319275\n",
      "Discriminator Loss: 1.411020040512085\n",
      "Generator Loss: 0.2836804687976837\n",
      "Discriminator Loss: 1.411219596862793\n",
      "Generator Loss: 0.28355491161346436\n",
      "Discriminator Loss: 1.4113467931747437\n",
      "Generator Loss: 0.28343063592910767\n",
      "Discriminator Loss: 1.4114766120910645\n",
      "Generator Loss: 0.28330105543136597\n",
      "Discriminator Loss: 1.4115591049194336\n",
      "Generator Loss: 0.2831919491291046\n",
      "Discriminator Loss: 1.4117248058319092\n",
      "Generator Loss: 0.2830795347690582\n",
      "Discriminator Loss: 1.4118990898132324\n",
      "Generator Loss: 0.2829492688179016\n",
      "Discriminator Loss: 1.4120922088623047\n",
      "Generator Loss: 0.28282517194747925\n",
      "Discriminator Loss: 1.4121893644332886\n",
      "Generator Loss: 0.28270870447158813\n",
      "Discriminator Loss: 1.4123250246047974\n",
      "Generator Loss: 0.28258150815963745\n",
      "Discriminator Loss: 1.4124637842178345\n",
      "Generator Loss: 0.2824546992778778\n",
      "Discriminator Loss: 1.4126172065734863\n",
      "Generator Loss: 0.28233838081359863\n",
      "Discriminator Loss: 1.412741780281067\n",
      "Generator Loss: 0.28222528100013733\n",
      "Discriminator Loss: 1.4128961563110352\n",
      "Generator Loss: 0.2821071147918701\n",
      "Discriminator Loss: 1.4130069017410278\n",
      "Generator Loss: 0.281980961561203\n",
      "Discriminator Loss: 1.4131606817245483\n",
      "Generator Loss: 0.2818523943424225\n",
      "Discriminator Loss: 1.413307547569275\n",
      "Generator Loss: 0.2817418575286865\n",
      "Discriminator Loss: 1.4134646654129028\n",
      "Generator Loss: 0.2816106975078583\n",
      "Discriminator Loss: 1.4135662317276\n",
      "Generator Loss: 0.28148001432418823\n",
      "Discriminator Loss: 1.4136438369750977\n",
      "Generator Loss: 0.2813780605792999\n",
      "Discriminator Loss: 1.4137866497039795\n",
      "Generator Loss: 0.2812502384185791\n",
      "Discriminator Loss: 1.4139403104782104\n",
      "Generator Loss: 0.2811274230480194\n",
      "Discriminator Loss: 1.4140362739562988\n",
      "Generator Loss: 0.2810084819793701\n",
      "Discriminator Loss: 1.414125680923462\n",
      "Generator Loss: 0.28089016675949097\n",
      "Discriminator Loss: 1.4142380952835083\n",
      "Generator Loss: 0.28077220916748047\n",
      "Discriminator Loss: 1.414396047592163\n",
      "Generator Loss: 0.2806541323661804\n",
      "Discriminator Loss: 1.4145526885986328\n",
      "Generator Loss: 0.280534565448761\n",
      "Discriminator Loss: 1.4146376848220825\n",
      "Generator Loss: 0.2804185748100281\n",
      "Discriminator Loss: 1.4147838354110718\n",
      "Generator Loss: 0.2802980840206146\n",
      "Discriminator Loss: 1.4149806499481201\n",
      "Generator Loss: 0.2801811695098877\n",
      "Discriminator Loss: 1.4151004552841187\n",
      "Generator Loss: 0.28006723523139954\n",
      "Discriminator Loss: 1.4152612686157227\n",
      "Generator Loss: 0.27994826436042786\n",
      "Discriminator Loss: 1.4154090881347656\n",
      "Generator Loss: 0.2798261046409607\n",
      "Discriminator Loss: 1.415503978729248\n",
      "Generator Loss: 0.27970507740974426\n",
      "Discriminator Loss: 1.4156197309494019\n",
      "Generator Loss: 0.27958500385284424\n",
      "Discriminator Loss: 1.4157843589782715\n",
      "Generator Loss: 0.27948352694511414\n",
      "Discriminator Loss: 1.4160032272338867\n",
      "Generator Loss: 0.27935898303985596\n",
      "Discriminator Loss: 1.4161193370819092\n",
      "Generator Loss: 0.279234379529953\n",
      "Discriminator Loss: 1.4161964654922485\n",
      "Generator Loss: 0.2791265845298767\n",
      "Discriminator Loss: 1.4163143634796143\n",
      "Generator Loss: 0.2790147066116333\n",
      "Discriminator Loss: 1.416477918624878\n",
      "Generator Loss: 0.27889642119407654\n",
      "Discriminator Loss: 1.4165750741958618\n",
      "Generator Loss: 0.2787836492061615\n",
      "Discriminator Loss: 1.4167321920394897\n",
      "Generator Loss: 0.27866747975349426\n",
      "Discriminator Loss: 1.4169203042984009\n",
      "Generator Loss: 0.278542697429657\n",
      "Discriminator Loss: 1.417014479637146\n",
      "Generator Loss: 0.2784216105937958\n",
      "Discriminator Loss: 1.4171103239059448\n",
      "Generator Loss: 0.2783200144767761\n",
      "Discriminator Loss: 1.4172638654708862\n",
      "Generator Loss: 0.27819278836250305\n",
      "Discriminator Loss: 1.4173505306243896\n",
      "Generator Loss: 0.27807044982910156\n",
      "Discriminator Loss: 1.417474627494812\n",
      "Generator Loss: 0.27795159816741943\n",
      "Discriminator Loss: 1.4175909757614136\n",
      "Generator Loss: 0.2778553366661072\n",
      "Discriminator Loss: 1.4177736043930054\n",
      "Generator Loss: 0.2777423858642578\n",
      "Discriminator Loss: 1.4178800582885742\n",
      "Generator Loss: 0.27762076258659363\n",
      "Discriminator Loss: 1.4179949760437012\n",
      "Generator Loss: 0.27750715613365173\n",
      "Discriminator Loss: 1.4181387424468994\n",
      "Generator Loss: 0.27739211916923523\n",
      "Discriminator Loss: 1.4182804822921753\n",
      "Generator Loss: 0.2772723436355591\n",
      "Discriminator Loss: 1.4183586835861206\n",
      "Generator Loss: 0.277166485786438\n",
      "Discriminator Loss: 1.4184666872024536\n",
      "Generator Loss: 0.2770478129386902\n",
      "Discriminator Loss: 1.418576955795288\n",
      "Generator Loss: 0.2769261598587036\n",
      "Discriminator Loss: 1.4186677932739258\n",
      "Generator Loss: 0.27681276202201843\n",
      "Discriminator Loss: 1.4187631607055664\n",
      "Generator Loss: 0.2767055034637451\n",
      "Discriminator Loss: 1.4188752174377441\n",
      "Generator Loss: 0.27659177780151367\n",
      "Discriminator Loss: 1.4189984798431396\n",
      "Generator Loss: 0.2764813005924225\n",
      "Discriminator Loss: 1.4190731048583984\n",
      "Generator Loss: 0.27636855840682983\n",
      "Discriminator Loss: 1.4191986322402954\n",
      "Generator Loss: 0.2762732207775116\n",
      "Discriminator Loss: 1.4193403720855713\n",
      "Generator Loss: 0.2761411666870117\n",
      "Discriminator Loss: 1.4194585084915161\n",
      "Generator Loss: 0.2760225832462311\n",
      "Discriminator Loss: 1.4195506572723389\n",
      "Generator Loss: 0.275921493768692\n",
      "Discriminator Loss: 1.4196641445159912\n",
      "Generator Loss: 0.27581319212913513\n",
      "Discriminator Loss: 1.4198745489120483\n",
      "Generator Loss: 0.27570345997810364\n",
      "Discriminator Loss: 1.4200034141540527\n",
      "Generator Loss: 0.27559247612953186\n",
      "Discriminator Loss: 1.420100212097168\n",
      "Generator Loss: 0.275486558675766\n",
      "Discriminator Loss: 1.4202488660812378\n",
      "Generator Loss: 0.2753717303276062\n",
      "Discriminator Loss: 1.4203816652297974\n",
      "Generator Loss: 0.2752748429775238\n",
      "Discriminator Loss: 1.4205150604248047\n",
      "Generator Loss: 0.2751579284667969\n",
      "Discriminator Loss: 1.4206503629684448\n",
      "Generator Loss: 0.27504977583885193\n",
      "Discriminator Loss: 1.4207854270935059\n",
      "Generator Loss: 0.27493956685066223\n",
      "Discriminator Loss: 1.420902967453003\n",
      "Generator Loss: 0.2748299539089203\n",
      "Discriminator Loss: 1.4209924936294556\n",
      "Generator Loss: 0.27472007274627686\n",
      "Discriminator Loss: 1.4211351871490479\n",
      "Generator Loss: 0.2746243476867676\n",
      "Discriminator Loss: 1.4212439060211182\n",
      "Generator Loss: 0.2745145261287689\n",
      "Discriminator Loss: 1.4213556051254272\n",
      "Generator Loss: 0.2743983268737793\n",
      "Discriminator Loss: 1.4214140176773071\n",
      "Generator Loss: 0.2742878198623657\n",
      "Discriminator Loss: 1.421471118927002\n",
      "Generator Loss: 0.2741904556751251\n",
      "Discriminator Loss: 1.4216058254241943\n",
      "Generator Loss: 0.2740788161754608\n",
      "Discriminator Loss: 1.4216974973678589\n",
      "Generator Loss: 0.2739728093147278\n",
      "Discriminator Loss: 1.4217932224273682\n",
      "Generator Loss: 0.27385610342025757\n",
      "Discriminator Loss: 1.4218655824661255\n",
      "Generator Loss: 0.2737556993961334\n",
      "Discriminator Loss: 1.4219963550567627\n",
      "Generator Loss: 0.27364933490753174\n",
      "Discriminator Loss: 1.4221421480178833\n",
      "Generator Loss: 0.27353936433792114\n",
      "Discriminator Loss: 1.4223048686981201\n",
      "Generator Loss: 0.27342909574508667\n",
      "Discriminator Loss: 1.4223921298980713\n",
      "Generator Loss: 0.27333199977874756\n",
      "Discriminator Loss: 1.4225355386734009\n",
      "Generator Loss: 0.27323049306869507\n",
      "Discriminator Loss: 1.4226503372192383\n",
      "Generator Loss: 0.2731151282787323\n",
      "Discriminator Loss: 1.4227384328842163\n",
      "Generator Loss: 0.2730117738246918\n",
      "Discriminator Loss: 1.4228160381317139\n",
      "Generator Loss: 0.2729017734527588\n",
      "Discriminator Loss: 1.4229143857955933\n",
      "Generator Loss: 0.27281102538108826\n",
      "Discriminator Loss: 1.4230461120605469\n",
      "Generator Loss: 0.2726902365684509\n",
      "Discriminator Loss: 1.423163890838623\n",
      "Generator Loss: 0.27260467410087585\n",
      "Discriminator Loss: 1.4232604503631592\n",
      "Generator Loss: 0.272504061460495\n",
      "Discriminator Loss: 1.4233760833740234\n",
      "Generator Loss: 0.2723971903324127\n",
      "Discriminator Loss: 1.4234040975570679\n",
      "Generator Loss: 0.27229243516921997\n",
      "Discriminator Loss: 1.4235504865646362\n",
      "Generator Loss: 0.272200345993042\n",
      "Discriminator Loss: 1.4237836599349976\n",
      "Generator Loss: 0.2720917761325836\n",
      "Discriminator Loss: 1.4239985942840576\n",
      "Generator Loss: 0.27198106050491333\n",
      "Discriminator Loss: 1.4241416454315186\n",
      "Generator Loss: 0.2718724310398102\n",
      "Discriminator Loss: 1.424209475517273\n",
      "Generator Loss: 0.2717704772949219\n",
      "Discriminator Loss: 1.424290418624878\n",
      "Generator Loss: 0.2716613709926605\n",
      "Discriminator Loss: 1.4244270324707031\n",
      "Generator Loss: 0.27156445384025574\n",
      "Discriminator Loss: 1.4245412349700928\n",
      "Generator Loss: 0.2714560329914093\n",
      "Discriminator Loss: 1.424708604812622\n",
      "Generator Loss: 0.27135249972343445\n",
      "Discriminator Loss: 1.424847960472107\n",
      "Generator Loss: 0.271243155002594\n",
      "Discriminator Loss: 1.4249334335327148\n",
      "Generator Loss: 0.2711404263973236\n",
      "Discriminator Loss: 1.4250386953353882\n",
      "Generator Loss: 0.2710377871990204\n",
      "Discriminator Loss: 1.4251606464385986\n",
      "Generator Loss: 0.27092260122299194\n",
      "Discriminator Loss: 1.4252948760986328\n",
      "Generator Loss: 0.2708287537097931\n",
      "Discriminator Loss: 1.4253959655761719\n",
      "Generator Loss: 0.2707175016403198\n",
      "Discriminator Loss: 1.4255023002624512\n",
      "Generator Loss: 0.27062562108039856\n",
      "Discriminator Loss: 1.4256067276000977\n",
      "Generator Loss: 0.2705128490924835\n",
      "Discriminator Loss: 1.4257408380508423\n",
      "Generator Loss: 0.2704163193702698\n",
      "Discriminator Loss: 1.4258793592453003\n",
      "Generator Loss: 0.27030840516090393\n",
      "Discriminator Loss: 1.4259920120239258\n",
      "Generator Loss: 0.27019989490509033\n",
      "Discriminator Loss: 1.426146388053894\n",
      "Generator Loss: 0.27010276913642883\n",
      "Discriminator Loss: 1.4262793064117432\n",
      "Generator Loss: 0.26999762654304504\n",
      "Discriminator Loss: 1.4264191389083862\n",
      "Generator Loss: 0.26990237832069397\n",
      "Discriminator Loss: 1.4265470504760742\n",
      "Generator Loss: 0.26980459690093994\n",
      "Discriminator Loss: 1.4266799688339233\n",
      "Generator Loss: 0.26969465613365173\n",
      "Discriminator Loss: 1.4267420768737793\n",
      "Generator Loss: 0.2695857882499695\n",
      "Discriminator Loss: 1.4268193244934082\n",
      "Generator Loss: 0.2694821357727051\n",
      "Discriminator Loss: 1.4269418716430664\n",
      "Generator Loss: 0.2693774402141571\n",
      "Discriminator Loss: 1.4270546436309814\n",
      "Generator Loss: 0.26928114891052246\n",
      "Discriminator Loss: 1.4271897077560425\n",
      "Generator Loss: 0.2691795229911804\n",
      "Discriminator Loss: 1.427390694618225\n",
      "Generator Loss: 0.2690754532814026\n",
      "Discriminator Loss: 1.4274511337280273\n",
      "Generator Loss: 0.2689732015132904\n",
      "Discriminator Loss: 1.427549123764038\n",
      "Generator Loss: 0.26886919140815735\n",
      "Discriminator Loss: 1.4276397228240967\n",
      "Generator Loss: 0.26877346634864807\n",
      "Discriminator Loss: 1.427746295928955\n",
      "Generator Loss: 0.26866772770881653\n",
      "Discriminator Loss: 1.4278708696365356\n",
      "Generator Loss: 0.2685580253601074\n",
      "Discriminator Loss: 1.4279422760009766\n",
      "Generator Loss: 0.2684463858604431\n",
      "Discriminator Loss: 1.4280145168304443\n",
      "Generator Loss: 0.2683439552783966\n",
      "Discriminator Loss: 1.4281333684921265\n",
      "Generator Loss: 0.26824674010276794\n",
      "Discriminator Loss: 1.4281991720199585\n",
      "Generator Loss: 0.2681410610675812\n",
      "Discriminator Loss: 1.4282443523406982\n",
      "Generator Loss: 0.26804319024086\n",
      "Discriminator Loss: 1.4283288717269897\n",
      "Generator Loss: 0.26793739199638367\n",
      "Discriminator Loss: 1.4283968210220337\n",
      "Generator Loss: 0.26783889532089233\n",
      "Discriminator Loss: 1.4285300970077515\n",
      "Generator Loss: 0.2677391767501831\n",
      "Discriminator Loss: 1.4286547899246216\n",
      "Generator Loss: 0.26763567328453064\n",
      "Discriminator Loss: 1.4287883043289185\n",
      "Generator Loss: 0.2675374746322632\n",
      "Discriminator Loss: 1.428892731666565\n",
      "Generator Loss: 0.2674349546432495\n",
      "Discriminator Loss: 1.428982138633728\n",
      "Generator Loss: 0.2673368752002716\n",
      "Discriminator Loss: 1.4291179180145264\n",
      "Generator Loss: 0.26723095774650574\n",
      "Discriminator Loss: 1.4292367696762085\n",
      "Generator Loss: 0.2671298682689667\n",
      "Discriminator Loss: 1.4293419122695923\n",
      "Generator Loss: 0.2670331299304962\n",
      "Discriminator Loss: 1.4294633865356445\n",
      "Generator Loss: 0.2669443190097809\n",
      "Discriminator Loss: 1.4296188354492188\n",
      "Generator Loss: 0.26684045791625977\n",
      "Discriminator Loss: 1.4297444820404053\n",
      "Generator Loss: 0.2667323648929596\n",
      "Discriminator Loss: 1.429821252822876\n",
      "Generator Loss: 0.2666414976119995\n",
      "Discriminator Loss: 1.4299498796463013\n",
      "Generator Loss: 0.2665427327156067\n",
      "Discriminator Loss: 1.4300943613052368\n",
      "Generator Loss: 0.266442209482193\n",
      "Discriminator Loss: 1.4301849603652954\n",
      "Generator Loss: 0.2663400173187256\n",
      "Discriminator Loss: 1.4302992820739746\n",
      "Generator Loss: 0.26624104380607605\n",
      "Discriminator Loss: 1.4304239749908447\n",
      "Generator Loss: 0.2661378085613251\n",
      "Discriminator Loss: 1.430489182472229\n",
      "Generator Loss: 0.26605233550071716\n",
      "Discriminator Loss: 1.430600881576538\n",
      "Generator Loss: 0.2659536600112915\n",
      "Discriminator Loss: 1.4307035207748413\n",
      "Generator Loss: 0.26584407687187195\n",
      "Discriminator Loss: 1.4308892488479614\n",
      "Generator Loss: 0.2657431960105896\n",
      "Discriminator Loss: 1.4309518337249756\n",
      "Generator Loss: 0.2656533718109131\n",
      "Discriminator Loss: 1.4310137033462524\n",
      "Generator Loss: 0.2655475437641144\n",
      "Discriminator Loss: 1.4310821294784546\n",
      "Generator Loss: 0.26545336842536926\n",
      "Discriminator Loss: 1.4312002658843994\n",
      "Generator Loss: 0.2653563320636749\n",
      "Discriminator Loss: 1.431357502937317\n",
      "Generator Loss: 0.2652607858181\n",
      "Discriminator Loss: 1.431471824645996\n",
      "Generator Loss: 0.265164315700531\n",
      "Discriminator Loss: 1.4315727949142456\n",
      "Generator Loss: 0.2650805413722992\n",
      "About to save image...\n",
      "Shape of generated_images: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n"
     ]
    }
   ],
   "source": [
    "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
    "generated_images = generator.predict(codings)"
   ]
  },
  {
   "attachments": {
    "GAN image.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing:\n",
    "\n",
    "### Test #1:\n",
    "\n",
    "Attempting the to train the model, it seems to be running however I don't seem to see anyway to output training images.\n",
    "\n",
    "### Test #2:\n",
    "\n",
    "Found the issue, it seems I accidently nested the image generation inside another loop causing a repetitive loop output.\n",
    "\n",
    "### Test #3: \n",
    "\n",
    "Issues with training, however this seems to be due to the dataset. I loaded the entire library to train on rather than a limited amount meaning training time per epoch would take quite sometime. I reduced the size of the dataset to see if it improves.\n",
    "\n",
    "    Results: Adjusting size of dataset to a more managable size has vastly improved training time. From here I will work on tweaking some parameters.\n",
    "\n",
    "### Test #4:\n",
    "\n",
    "Testing generated images from baseline settings that I set.\n",
    "\n",
    "    Results: Images are outputting, however unsure if this is correctly generating the images I would like.\n",
    "\n",
    "### Test #5:\n",
    "\n",
    "The generator and discriminator are training correctly. Discriminator is curently quite good at distinguishing real and fake images although there may be some possible fixes here. The ideal loss is around 0.69 for this task, but not unreasonably high.\n",
    "A relatively low loss (0.26) indicates that your generator is somehow \"fooling\" the discriminator.  However, this doesn't necessarily mean it's generating good Fashion MNIST-like images yet.\n",
    "\n",
    "    Generator Loss: 0.2652607858181\n",
    "    Discriminator Loss: 1.431471824645996\n",
    "    Generator Loss: 0.265164315700531\n",
    "    Discriminator Loss: 1.4315727949142456\n",
    "    Generator Loss: 0.2650805413722992\n",
    "    About to save image...\n",
    "    Shape of generated_images: (2, 28, 28, 1)\n",
    "\n",
    "    Results: It's too soon to determine if there is any meaningful changes and adjustments. This is primarly due to my GPU taking some time to process the images even with a limited dataset. I find this impratical to test for anyone not running a subpar GPU in which it seems most students don't have available. In my case, I do have one and it still takes roughly 3 to 4 minutes a epoch. Suggest changing epochs to a larger number.\n",
    "\n",
    "### Test #5.1:\n",
    "\n",
    "After almost 1 hour this seems to have done some improvements, the pixelated photos resemble the basic shape of the clothing but nothing too obvious. However this is promising.\n",
    "\n",
    "### Test #6:\n",
    "\n",
    "Added code to allow the images to show during the end of each epoch. \n",
    "\n",
    "    Results: It works\n",
    "\n",
    "    ![GAN image.PNG](<attachment:GAN image.PNG>)\n",
    "\n",
    "### Test #7:\n",
    "\n",
    "I will pause here for now but from what I can tell the model appears to be working correctly and with a basic dataset this could be swapped to something more complex down the line.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

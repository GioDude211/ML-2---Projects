{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What is a Generative Adversarial Network (GAN)?**\n",
    "\n",
    "#### **The Art of Imitation:** GANs are a type of deep learning architecture where two neural networks are locked in an elaborate game of cat-and-mouse.  Think of them as a master art forger and an expert art critic:\n",
    "   **The Generator:** The 'art forger' responsible for creating realistic fakes that could pass as authentic pieces (e.g., images, music, text etc.).\n",
    "     \n",
    "   **The Discriminator:**  The 'art critic'  trained to tell the difference between a genuine masterpiece and the generator's forgeries.\n",
    "\n",
    "#### **A Competitive Game:**  Both networks get better through this competition.  The generator keeps refining its fakes to fool the discriminator, and the discriminator gets better at spotting the subtle flaws in those fakes. Over time, the generator gets so good that its creations become indistinguishable from real data.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "1. **Unsupervised Learning:** GANs don't need labeled data. They learn from a dataset of examples (e.g., a collection of real photographs), extracting the patterns and underlying structure without explicit labels like \"dog\" or \"cat.\"\n",
    "\n",
    "2. **Generator Network:**\n",
    "   * Takes random noise as input.\n",
    "   * Tries to transform that noise into data that resembles the real examples it has seen.\n",
    "   * Its goal is to make the discriminator believe its outputs are real.\n",
    "\n",
    "3. **Discriminator Network:**\n",
    "   * Takes as input both real data samples and the generator's outputs.\n",
    "   * Tries to classify whether an input is \"real\" (from the training dataset) or \"fake\" (created by the generator).\n",
    "   * Gives feedback to the generator to help it improve its fakes.\n",
    "\n",
    "4. **Adversarial Training:**\n",
    "   * The generator and discriminator are trained simultaneously.\n",
    "   * The generator improves by making the discriminator's job harder.\n",
    "   * The discriminator improves by becoming a better judge of authenticity.\n",
    "   * It's this constant push-and-pull that makes GANs so powerful.\n",
    "\n",
    "**Why use GANs?**\n",
    "\n",
    "* **Generating New Data:** Create realistic images, music, videos, or other data forms that weren't in your original dataset.\n",
    "* **Data Augmentation:** Increase the size and diversity of a dataset, useful in areas where real data is scarce.\n",
    "* **Image-to-Image Translation:** Change images from one style to another (e.g., turning sketches into photos).\n",
    "* **Super Resolution:** Enhancing image or video quality.\n",
    "\n",
    "**Ready to Build?**\n",
    "\n",
    "Building a GAN involves:\n",
    "\n",
    "* **Choosing Your Domain:**  Images, text, audio, etc.\n",
    "* **Data Preparation:**  Gathering or creating a suitable dataset.\n",
    "* **Network Architecture:**  Designing your generator and discriminator networks (often using convolutional neural networks for images).\n",
    "* **Training Setup:**  Defining the loss functions that guide the networks and selecting an optimization algorithm. \n",
    "* **Implementation:**  Coding your GAN using a deep learning framework like TensorFlow or PyTorch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "#Checks if GPU is being used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0)) \n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "#Using GPU: NVIDIA GeForce GTX 1660 SUPER - Successful\n",
    "#NOTE: This took some time to set up by installing and pathing the cuda toolkit v.12.4 and the right supplemental packages. This drastically improved\n",
    "#training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "#split dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing:\n",
    "\n",
    "# Data Conversion and Normalization\n",
    "train_images = tf.cast(train_images, tf.float32)  # Convert to float32 / this type is needed, MNIST is unit8 type\n",
    "train_images /= 255.0  # Normalize to [0, 1] range\n",
    "\n",
    "# Do the same for the test dataset if needed:\n",
    "test_images = tf.cast(test_images, tf.float32)\n",
    "test_images /= 255.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at the first image and its label\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.title(train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_size = 100\n",
    "\n",
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(7 * 7 * 128),\n",
    "    tf.keras.layers.Reshape([7, 7, 128]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"tanh\"),\n",
    "])\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
    "                           activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
    "                           activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan = tf.keras.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size=1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training loop is unusual, we cannot use the regular fit() method. Instead, we will write a custom training loop. For this, we first need to create a Dataset to iterate through the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must call `compile()` before using the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m             y2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([[\u001b[38;5;241m1.\u001b[39m]] \u001b[38;5;241m*\u001b[39m batch_size)\n\u001b[0;32m     16\u001b[0m             gan\u001b[38;5;241m.\u001b[39mtrain_on_batch(noise, y2)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodings_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(gan, dataset, batch_size, codings_size, n_epochs)\u001b[0m\n\u001b[0;32m      9\u001b[0m X_fake_and_real \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([generated_images, tf\u001b[38;5;241m.\u001b[39mexpand_dims(X_batch, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m y1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([[\u001b[38;5;241m0.\u001b[39m]] \u001b[38;5;241m*\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m [[\u001b[38;5;241m1.\u001b[39m]] \u001b[38;5;241m*\u001b[39m batch_size)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fake_and_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# phase 2 - training the generator\u001b[39;00m\n\u001b[0;32m     14\u001b[0m noise \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m[batch_size, codings_size])\n",
      "File \u001b[1;32mc:\\Users\\GioDude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:534\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_on_batch\u001b[39m(\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    528\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    532\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    533\u001b[0m ):\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert_compile_called\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_on_batch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m class_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\GioDude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:887\u001b[0m, in \u001b[0;36mTrainer._assert_compile_called\u001b[1;34m(self, method_name)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 887\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You must call `compile()` before using the model."
     ]
    }
   ],
   "source": [
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch in dataset:\n",
    "\n",
    "            # phase 1 - training the discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, tf.expand_dims(X_batch, axis=-1)], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "\n",
    "            # phase 2 - training the generator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            gan.train_on_batch(noise, y2)\n",
    "\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step\n"
     ]
    }
   ],
   "source": [
    "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
    "generated_images = generator.predict(codings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

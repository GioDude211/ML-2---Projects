{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmer: Giovanni Vecchione\n",
    "## Date: 4/8/24\n",
    "## Subject: Machine Learning 2 - Autoencoders, GANS, and Diffusion Models\n",
    "\n",
    "Chapter 17. Autoencoders, GANs, and Diffusion Models\n",
    "\n",
    "Goal: Use Generative Adversarial Networks (GAN) to build the project. Submit your project as Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from keras.regularizers import l1\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K \n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "#Checks if GPU is being used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0)) \n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "\n",
    "#Using GPU: NVIDIA GeForce GTX 1660 SUPER - Successful\n",
    "#NOTE: This took some time to set up by installing and pathing the cuda toolkit v.12.4 and the right supplemental packages. This drastically improved\n",
    "#training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoencoders:**\n",
    "1. Autoencoders can be used for dimensionality reduction. Another application is for unsupervised pretraining.\n",
    "\n",
    "2. **Autoencoder as Feature Extractor:** An autoencoder can learn a compressed representation (an \"encoding\") of the key features from your real data. You can then use this encoding as a starting point for your GAN's generator, giving it a head start in understanding the underlying structure of the data. This can lead to faster GAN training and potentially more realistic outputs.\n",
    "\n",
    "3. **Fighting Mode Collapse:** One challenge with GANs is \"mode collapse\", where the generator produces a limited variety of outputs, fooling the discriminator. An autoencoder can help here. By pre-training the discriminator on the task of reconstructing real data (like in an autoencoder setup) you can make it better at recognizing the wider variety of features present in the real dataset. This can help it push the generator to produce more diverse outputs.\n",
    "\n",
    "4. **Handling Noisy Data:** If your real data set is noisy, you can use a denoising autoencoder as a pre-processing step. The autoencoder learns to reconstruct clean images from noisy versions. This cleaned-up data can then feed into your GAN, leading to better outputs.\n",
    "\n",
    "5. **Creative Control:** You can build a hybrid model where the generator of a GAN produces the compressed representation (the \"code\") and the decoder of an autoencoder reconstructs the data from that code. This allows you to control generation with the compressed representation, leading to interesting image manipulation or style transfer effects.\n",
    "\n",
    "### **Optimization:** When compiling the stacked autoencoder, we use the MSE loss and Nadam optimization.\n",
    "### **Comparing input to output:** One way to ensure that an autoencoder is properly trained is to compare the inputs and the outputs: the differences should not be too significant.\n",
    "### **Important Note:** While autoencoders can provide benefits, they aren't strictly necessary to build a GAN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reducing Dimensions of a Dataset**\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance—say, 95% (An exception to this rule, of course, is if you are reducing dimensionality for data visualization, in which case you will want to reduce the dimensionality down to 2 or 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads and splits the MNIST dataset (introduced in Chapter 3) and performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set's variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "#X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\n",
    "#X_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n",
    "\n",
    "#pca = PCA()\n",
    "#pca.fit(X_train)\n",
    "#cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "#d = np.argmax(cumsum >= 0.95) + 1  # d equals 154\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks Dimensions\n",
    "#pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could then set n_components=d and run PCA again, but there's a better option. Instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components=0.95)\n",
    "#X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual number of components is determined during training, and it is stored in the n_components_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NOTE:** Lastly, if you are using dimensionality reduction as a preprocessing step for a supervised learning task (e.g., classification), then you can tune the number of dimensions as you would any other hyperparameter (see Chapter 2).\n",
    "\n",
    "https://learning.oreilly.com/library/view/hands-on-machine-learning/9781098125967/ch08.html#idm45720209258688:~:text=Lastly%2C%20if%20you%20are%20using%20dimensionality%20reduction%20as%20a%20preprocessing%20step%20for%20a%20supervised%20learning%20task%20(e.g.%2C%20classification)%2C%20then%20you%20can%20tune%20the%20number%20of%20dimensions%20as%20you%20would%20any%20other%20hyperparameter%20(see%20Chapter%C2%A02)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementing a Stacked Autoencoder Using Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 32 and 28 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](compile_loss/mse/Cast, sequential_17_1/sequential_16_1/reshape_5_1/Reshape)' with input shapes: [32,784], [32,28,28].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m stacked_ae \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([stacked_encoder, stacked_decoder])\n\u001b[0;32m     13\u001b[0m stacked_ae\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mstacked_ae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\GioDude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\GioDude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses\\losses.py:1154\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1152\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1153\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[1;32m-> 1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msquare(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 32 and 28 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](compile_loss/mse/Cast, sequential_17_1/sequential_16_1/reshape_5_1/Reshape)' with input shapes: [32,784], [32,28,28]."
     ]
    }
   ],
   "source": [
    "stacked_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
    "])\n",
    "stacked_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "stacked_ae = tf.keras.Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "stacked_ae.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = stacked_ae.fit(X_train, X_train, epochs=20,\n",
    "                         validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing the Reconstructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_reconstructions(model, images=X_valid, n_images=5):\n",
    "    reconstructions = np.clip(model.predict(images[:n_images]), 0, 1)\n",
    "    fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
    "    for image_index in range(n_images):\n",
    "        plt.subplot(2, n_images, 1 + image_index)\n",
    "        plt.imshow(images[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.subplot(2, n_images, 1 + n_images + image_index)\n",
    "        plt.imshow(reconstructions[image_index], cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plot_reconstructions(stacked_ae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convolutional Autoencoders:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape([28, 28, 1]),\n",
    "    tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 14 × 14 x 16\n",
    "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 7 × 7 x 32\n",
    "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=2),  # output: 3 × 3 x 64\n",
    "    tf.keras.layers.Conv2D(30, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAvgPool2D()  # output: 30\n",
    "])\n",
    "conv_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(3 * 3 * 16),\n",
    "    tf.keras.layers.Reshape((3, 3, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, strides=2, activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\",\n",
    "                                    activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding=\"same\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "conv_ae = tf.keras.Sequential([conv_encoder, conv_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "])\n",
    "dropout_decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(28 * 28),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "dropout_ae = tf.keras.Sequential([dropout_encoder, dropout_decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What is a Generative Adversarial Network (GAN)?**\n",
    "\n",
    "#### **The Art of Imitation:** GANs are a type of deep learning architecture where two neural networks are locked in an elaborate game of cat-and-mouse.  Think of them as a master art forger and an expert art critic:\n",
    "   **The Generator:** The 'art forger' responsible for creating realistic fakes that could pass as authentic pieces (e.g., images, music, text etc.).\n",
    "     \n",
    "   **The Discriminator:**  The 'art critic'  trained to tell the difference between a genuine masterpiece and the generator's forgeries.\n",
    "\n",
    "#### **A Competitive Game:**  Both networks get better through this competition.  The generator keeps refining its fakes to fool the discriminator, and the discriminator gets better at spotting the subtle flaws in those fakes. Over time, the generator gets so good that its creations become indistinguishable from real data.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "1. **Unsupervised Learning:** GANs don't need labeled data. They learn from a dataset of examples (e.g., a collection of real photographs), extracting the patterns and underlying structure without explicit labels like \"dog\" or \"cat.\"\n",
    "\n",
    "2. **Generator Network:**\n",
    "   * Takes random noise as input.\n",
    "   * Tries to transform that noise into data that resembles the real examples it has seen.\n",
    "   * Its goal is to make the discriminator believe its outputs are real.\n",
    "\n",
    "3. **Discriminator Network:**\n",
    "   * Takes as input both real data samples and the generator's outputs.\n",
    "   * Tries to classify whether an input is \"real\" (from the training dataset) or \"fake\" (created by the generator).\n",
    "   * Gives feedback to the generator to help it improve its fakes.\n",
    "\n",
    "4. **Adversarial Training:**\n",
    "   * The generator and discriminator are trained simultaneously.\n",
    "   * The generator improves by making the discriminator's job harder.\n",
    "   * The discriminator improves by becoming a better judge of authenticity.\n",
    "   * It's this constant push-and-pull that makes GANs so powerful.\n",
    "\n",
    "**Why use GANs?**\n",
    "\n",
    "* **Generating New Data:** Create realistic images, music, videos, or other data forms that weren't in your original dataset.\n",
    "* **Data Augmentation:** Increase the size and diversity of a dataset, useful in areas where real data is scarce.\n",
    "* **Image-to-Image Translation:** Change images from one style to another (e.g., turning sketches into photos).\n",
    "* **Super Resolution:** Enhancing image or video quality.\n",
    "\n",
    "**Ready to Build?**\n",
    "\n",
    "Building a GAN involves:\n",
    "\n",
    "* **Choosing Your Domain:**  Images, text, audio, etc.\n",
    "* **Data Preparation:**  Gathering or creating a suitable dataset.\n",
    "* **Network Architecture:**  Designing your generator and discriminator networks (often using convolutional neural networks for images).\n",
    "* **Training Setup:**  Defining the loss functions that guide the networks and selecting an optimization algorithm. \n",
    "* **Implementation:**  Coding your GAN using a deep learning framework like TensorFlow or PyTorch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic GAN Model:\n",
    "\n",
    "First, we need to build the generator and the discriminator. The generator is similar to an autoencoder's decoder, and the discriminator is a regular binary classifier: it takes an image as input and ends with a Dense layer containing a single unit and using the sigmoid activation function. For the second phase of each training iteration, we also need the full GAN model containing the generator followed by the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "#split dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing:\n",
    "\n",
    "# Data Conversion and Normalization\n",
    "train_images = tf.cast(train_images, tf.float32)  # Convert to float32 / this type is needed, MNIST is unit8 type\n",
    "train_images /= 255.0  # Normalize to [0, 1] range\n",
    "\n",
    "# Do the same for the test dataset if needed:\n",
    "test_images = tf.cast(test_images, tf.float32)\n",
    "test_images /= 255.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's look at the first image and its label\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.title(train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_size = 30\n",
    "\n",
    "Dense = tf.keras.layers.Dense\n",
    "generator = tf.keras.Sequential([\n",
    "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(28 * 28, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Reshape([28, 28])\n",
    "])\n",
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    Dense(150, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan = tf.keras.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Code Explination**\n",
    "\n",
    "```python\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "```\n",
    "\n",
    "* **Compiling the Discriminator:**\n",
    "    * `loss=\"binary_crossentropy\"`: This is a standard choice for a discriminator classifying real vs. fake images.\n",
    "    * `optimizer=\"rmsprop\"`:  RMSprop is a decent optimizer, though Adam is more common in GANs. Experiment to see what works best for you.\n",
    "\n",
    "* **`discriminator.trainable = False`:** \n",
    "    * **Key Concept:** You're freezing the discriminator's weights during the initial stages of training the generator. This is done because you want the generator to get better at producing plausible fakes *before* the discriminator gets too good at spotting them. \n",
    "\n",
    "* **Compiling the GAN:**\n",
    "    *  You're compiling the combined GAN model (generator + discriminator) with the same loss and optimizer. This sets the stage for adversarial training.\n",
    "\n",
    "**Important Points**\n",
    "\n",
    "1. **Training Phases:**  GAN training often involves alternating phases:\n",
    "    * **Train the Discriminator:** Set `discriminator.trainable = True`, train it on a mix of real and fake images.\n",
    "    * **Train the Generator:** Set `discriminator.trainable = False`, and train the GAN model (this indirectly trains the generator to fool the frozen discriminator).\n",
    "\n",
    "2. **Hyperparameter Tuning:** The choice of optimizer and its respective learning rates can significantly impact GAN training stability. \n",
    "\n",
    "**Suggested Training Loop Outline (Illustrative):**\n",
    "\n",
    "```python\n",
    "def train(dataset, epochs, batch_size, codings_size):\n",
    "  for epoch in range(epochs):\n",
    "    for real_images, _ in dataset.take(batch_size):  # Assuming labels are not needed\n",
    "        # Train Discriminator...\n",
    "        discriminator.trainable = True \n",
    "        # ... (Generate fake images, calculate losses, update discriminator weights)\n",
    "\n",
    "        # Train the Generator...\n",
    "        discriminator.trainable = False\n",
    "        # ... (Sample noise, generate fake images, calculate loss, update generator weights)\n",
    "\n",
    "```\n",
    "\n",
    "**Note: This is a high-level structure. You'll need to fill in the specific steps to generate fake images, calculate losses, and update weights using your chosen optimizer.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training loop is unusual, we cannot use the regular fit() method. Instead, we will write a custom training loop. For this, we first need to create a Dataset to iterate through the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size=1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Code Breakdown**\n",
    "\n",
    "```python\n",
    "batch_size = 32 \n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size=1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "```\n",
    "\n",
    "1. **Batch Size:**\n",
    "    * `batch_size = 32`: You'll be training your GAN in batches of 32 images at a time. A common choice, but you can experiment with different batch sizes.\n",
    "\n",
    "2. **`tf.data.Dataset.from_tensor_slices()`:** \n",
    "    * Creates a TensorFlow Dataset object directly from your NumPy array (`train_images`). This is an efficient way to feed data into your model.\n",
    "\n",
    "3. **`shuffle(buffer_size=1000)`:**\n",
    "    * Shuffles the data with a buffer of 1000 images. This ensures your batches are randomized during training, preventing the GAN from overfitting to a specific sequence of images.\n",
    "\n",
    "4. **`batch(batch_size, drop_remainder=True)`:**\n",
    "   * Groups your data into batches of 32 images.\n",
    "   * `drop_remainder=True` ensures all batches have the same size,  avoiding issues with smaller batches at the end of your dataset.\n",
    "\n",
    "5. **`prefetch(1)`:**\n",
    "    * Pre-fetches one batch ahead. This helps overlap data preparation and model computation, potentially speeding up your training process.\n",
    "\n",
    "**Why This Matters**\n",
    "\n",
    "* **Stochastic Gradient Descent:** GANs (and most deep learning models) use variations of stochastic gradient descent (SGD) for optimization. SGD works best when you feed it data in smaller batches.\n",
    "\n",
    "* **Generalization:** Data shuffling before forming batches helps prevent the model from getting stuck on a particular order of images, allowing for better generalization to unseen examples.\n",
    "\n",
    "* **Efficiency:**  The `tf.data` API provides efficient tools for creating the data pipeline. Prefetching helps streamline the process.\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "*  **Large Datasets:** For big datasets, you might need more advanced prefetching or adjust the `shuffle` buffer size.\n",
    "\n",
    "* **Batch Size Experimentation:** The optimal batch size can depend on your specific dataset and hardware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GioDude\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:75: UserWarning: The model does not have any trainable weights.\n",
      "  warnings.warn(\"The model does not have any trainable weights.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001BD47957B00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001BD47957B00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001BD47F78E00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001BD47F78E00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch in dataset:\n",
    "            # phase 1 - training the discriminator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            generated_images = generator(noise)\n",
    "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "            # phase 2 - training the generator\n",
    "            noise = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            y2 = tf.constant([[1.]] * batch_size)\n",
    "            gan.train_on_batch(noise, y2)\n",
    "\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"
     ]
    }
   ],
   "source": [
    "codings = tf.random.normal(shape=[batch_size, codings_size])\n",
    "generated_images = generator.predict(codings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional GAN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Replace any pooling layers with strided convolutions (in the discriminator) and transposed convolutions (in the generator).\n",
    "\n",
    "2. Use batch normalization in both the generator and the discriminator, except in the generator's output layer and the discriminator's input layer.\n",
    "\n",
    "3. Remove fully connected hidden layers for deeper architectures.\n",
    "\n",
    "4. Use ReLU activation in the generator for all layers except the output layer, which should use tanh.\n",
    "\n",
    "5. Use leaky ReLU activation in the discriminator for all layers.\n",
    "\n",
    "These guidelines will work in many cases, but not always, so you may still need to experiment with different hyperparameters. In fact, just changing the random seed and training the exact same model again will sometimes work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_size = 100\n",
    "\n",
    "generator2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(7 * 7 * 128),\n",
    "    tf.keras.layers.Reshape([7, 7, 128]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2,\n",
    "                                    padding=\"same\", activation=\"tanh\"),\n",
    "])\n",
    "discriminator2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\",\n",
    "                           activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\",\n",
    "                           activation=tf.keras.layers.LeakyReLU(0.2)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "gan2 = tf.keras.Sequential([generator2, discriminator2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator takes codings of size 100, projects them to 6,272 dimensions (7 * 7 * 128), and reshapes the result to get a 7 × 7 × 128 tensor. This tensor is batch normalized and fed to a transposed convolutional layer with a stride of 2, which upsamples it from 7 × 7 to 14 × 14 and reduces its depth from 128 to 64. The result is batch normalized again and fed to another transposed convolutional layer with a stride of 2, which upsamples it from 14 × 14 to 28 × 28 and reduces the depth from 64 to 1. This layer uses the tanh activation function, so the outputs will range from -1 to 1. For this reason, before training the GAN, we need to rescale the training set to that same range. We also need to reshape it to add the channel dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
